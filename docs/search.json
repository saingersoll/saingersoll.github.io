[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Here you’ll find a collection of projects I’ve worked on throughout my career as a Graduate Research Assistant at the 2035 Initiative and my Mater’s program for Environmental Data Science at the Bren School, UCSB."
  },
  {
    "objectID": "posts.html#welcome",
    "href": "posts.html#welcome",
    "title": "My Portfolio",
    "section": "",
    "text": "Here you’ll find a collection of projects I’ve worked on throughout my career as a Graduate Research Assistant at the 2035 Initiative and my Mater’s program for Environmental Data Science at the Bren School, UCSB."
  },
  {
    "objectID": "scratch/ideas.html",
    "href": "scratch/ideas.html",
    "title": "",
    "section": "",
    "text": "A couple of gal pals going to see Steve Lacy! Date: May 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sofia Ingersoll",
    "section": "",
    "text": "Howdy y’all! I’m Sofia (it rhymes with papaya, Sof-aya).\nI am a creative strategist who loves tackling the never-ending challenges that come with coding and communicating complex information to an audience with diverse backgrounds. In my free time, I’m a chemist in the kitchen, an amateur Spotify DJ, and a zealous zumba goer. Feel free to explore my website and learn more about me, my work at The 2035 Initiative, and my other projects.\nIf my content resonates with you, let’s connect and discuss how I can bring value to your team!"
  },
  {
    "objectID": "posts/2024-3-16-Infographic/fce-lter-infographic.html#wrangling-subsetting",
    "href": "posts/2024-3-16-Infographic/fce-lter-infographic.html#wrangling-subsetting",
    "title": "Florida Coastal Everglades LTER Infographic on Oligotrophic Sites",
    "section": "Wrangling & Subsetting",
    "text": "Wrangling & Subsetting\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                       subset site wq data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---         wq status subset          ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# subset oligo sites water nutrients\noligo_sites &lt;- clean_data %&gt;% \n  filter(eutrophic_status %in% \"Oligotrophic\") %&gt;% \n  select(site, din, srp, apa, ton, si_o2, toc, tp, eutrophic_status, latdec, londec) %&gt;% \n  na.omit()\n\n# subset meso sites water nutrients\nmeso_sites &lt;- clean_data %&gt;% \n  filter(eutrophic_status %in% \"Mesotrophic\") %&gt;% \n  select(site, din, srp, apa, ton, si_o2, toc, tp, eutrophic_status, latdec, londec) %&gt;% \n  na.omit()\n\n\n\nMap Data\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for mapping                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---      combo sites for mapping      ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# sf object of combined site wq info\nfce_surveying_sites &lt;- bind_rows(oligo_sites, meso_sites) %&gt;%\n  group_by(eutrophic_status) %&gt;% \n  st_as_sf(coords = c('londec','latdec')) %&gt;% \n  mutate('Water Quality Status' = eutrophic_status)\n\n# set CRS \nfce_sites_shp &lt;- st_crs(fce_surveying_sites, 4326)\n\n\n\n\nWaffle Data\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for waffle plot                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---     combo sites count comparison ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncombo_sites &lt;- bind_rows(oligo_sites, meso_sites) %&gt;%\n  group_by(eutrophic_status) %&gt;%\n  # Count the number of sites for each category\n  summarise(n = n())\n\n\n\n\nDumbbell Data\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                     data for dumbbell plot                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---       avg wq status nutrients     ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# summarize across all sites for plotting \noligo_sum &lt;- oligo_sites %&gt;% \n  summarize(\n    avg_tp = round(mean(tp), 3),\n    avg_din = round(mean(din), 3),\n    avg_srp = round(mean(srp), 3),\n    avg_apa = round(mean(apa), 3),\n    avg_ton = round(mean(ton), 3),\n    avg_si_o2 = round(mean(si_o2), 3),\n    avg_toc = round(mean(toc), 3)\n    )\n\n# summarize across all sites for plotting \nmeso_sum &lt;- meso_sites %&gt;% \n  summarize(\n    avg_tp = round(mean(tp), 3),\n    avg_din = round(mean(din), 3),\n    avg_srp = round(mean(srp), 3),\n    avg_apa = round(mean(apa), 3),\n    avg_ton = round(mean(ton), 3),\n    avg_si_o2 = round(mean(si_o2), 3),\n    avg_toc = round(mean(toc), 3)\n    )\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---  smallest avg wq status nutrients ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# summarize across all sites for plotting \noligo_sum_small &lt;- oligo_sites %&gt;% \n  summarize(\n    avg_tp = round(mean(tp), 3),\n    avg_srp = round(mean(srp), 3),\n    avg_apa = round(mean(apa), 3),\n    )\n\n# summarize  across all sites for plotting \nmeso_sum_small &lt;- meso_sites %&gt;% \n  summarize(\n    avg_tp = round(mean(tp), 3),\n    avg_srp = round(mean(srp), 3),\n    avg_apa = round(mean(apa), 3),\n    )\n\n# combine into a single df and assign wq status & nutrient to each observation\ncombo_avg_small &lt;- bind_rows(oligo_sum_small, meso_sum_small,\n                       .id = \"category\") %&gt;%\n  pivot_longer(-category, names_to = \"nutrient\",\n               values_to = \"Average\") %&gt;%\n  mutate(nutrient = gsub(\"avg_\", \"\", nutrient),\n         category = factor(category,\n                           levels = c(\"1\", \"2\"),\n                           labels = c(\"Oligotrophic\", \"Mesotrophic\")))\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---    medium avg wq status nutrients ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# summarize  across all sites for plotting \noligo_sum_med &lt;- oligo_sites %&gt;% \n  summarize(\n    avg_din = round(mean(din), 3)\n    )\n\n# summarize  across all sites for plotting \nmeso_sum_med &lt;- meso_sites %&gt;% \n  summarize(\n    avg_din = round(mean(din), 3)\n    )\n\n# combine into a single df and assign wq status & nutrient to each observation\ncombo_avg_med &lt;- bind_rows(oligo_sum_med, meso_sum_med,\n                       .id = \"category\") %&gt;%\n  pivot_longer(-category, names_to = \"nutrient\",\n               values_to = \"Average\") %&gt;%\n  mutate(nutrient = gsub(\"avg_\", \"\", nutrient),\n         category = factor(category,\n                           levels = c(\"1\", \"2\"),\n                           labels = c(\"Oligotrophic\", \"Mesotrophic\")))\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---      big avg wq status nutrients  ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# summarize  across all sites for plotting \noligo_sum_big &lt;- oligo_sites %&gt;% \n  summarize(\n    avg_ton = round(mean(ton), 3),\n    avg_si_o2 = round(mean(si_o2), 3)\n    )\n\n# summarize  across all sites for plotting \nmeso_sum_big &lt;- meso_sites %&gt;% \n  summarize(\n    avg_ton = round(mean(ton), 3),\n    avg_si_o2 = round(mean(si_o2), 3)\n    )\n\n# combine into a single df and assign wq status & nutrient to each observation\ncombo_avg_big &lt;- bind_rows(oligo_sum_big, meso_sum_big,\n                       .id = \"category\") %&gt;%\n  pivot_longer(-category, names_to = \"nutrient\",\n               values_to = \"Average\") %&gt;%\n  mutate(nutrient = gsub(\"avg_\", \"\", nutrient),\n         category = factor(category,\n                           levels = c(\"1\", \"2\"),\n                           labels = c(\"Oligotrophic\", \"Mesotrophic\")))\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---  biggest avg wq status nutrients  ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# summarize  across all sites for plotting \noligo_sum_bige &lt;- oligo_sites %&gt;% \n  summarize(\n    avg_toc = round(mean(toc), 3)\n    )\n\n# summarize for plotting \nmeso_sum_bige &lt;- meso_sites %&gt;% \n  summarize(\n    avg_toc = round(mean(toc), 3)\n    )\n\n# combine into a single df and assign wq status & nutrient to each observation\ncombo_avg_bige &lt;- bind_rows(oligo_sum_bige, meso_sum_bige,\n                       .id = \"category\") %&gt;%\n  pivot_longer(-category, names_to = \"nutrient\",\n               values_to = \"Average\") %&gt;%\n  mutate(nutrient = gsub(\"avg_\", \"\", nutrient),\n         category = factor(category,\n                           levels = c(\"1\", \"2\"),\n                           labels = c(\"Oligotrophic\", \"Mesotrophic\")))"
  },
  {
    "objectID": "posts/2024-3-16-Infographic/fce-lter-infographic.html#customs",
    "href": "posts/2024-3-16-Infographic/fce-lter-infographic.html#customs",
    "title": "Florida Coastal Everglades LTER Infographic on Oligotrophic Sites",
    "section": "Customs",
    "text": "Customs\n\nText\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        create plot labels                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      map labels        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nm_title &lt;- \"LTER Surveying Sites in the Florida Coastal Everglades\"\nm_subtitle &lt;- \"Identifying &lt;span style='color:#34A0A4;'&gt;**Oligotrophic**&lt;/span&gt; Estuaries\"\nm_legend &lt;- \"Water Quality Status\"\nm_alt &lt;- \"This is a map of Florida, highlighting the LTER water quality surveying sites located in the Florida Coastal Everglades. The sites are categorized into two groups: Oligotrophic (blue points) and Mesotrophic (green points). The cluster of Oligotrophic sites are observed in the western portion of the surveying area. Sites categorized as Oligotrophic contain low water nutrients, with zero to little plant growth. The total phosphorous content is &lt; 0.025 (μmol/L). Sites categorized as Mesotrophic contain greater concentrations of total phosphorous, moderate water nutrients, and have some plant growth.\"\nm_caption &lt;- \"Source: Henry Briceno. (2023). Surface Water Quality Monitoring Data (FCE LTER), \\nFlorida, USA, June 1989-ongoing\"\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      waffle labels        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nw_title &lt;- \"Proportions of FCE Estuaries Identified as &lt;span style='color:#34A0A4;'&gt;**Oligotrophic**&lt;/span&gt;\"\nw_subtitle &lt;- \"LTER Florida Coastal Everglades\"\nw_legend &lt;- \"Water Quality Status\"\nw_alt &lt;- \"This is a waffle chart of the proportion of water quality status across the LTER surveying sites located in the Florida Coastal Everglades. There is a signficantly larger portion of Mesotrophic sites. Roughly 70% total are classified as Mesotrophic. The sites are categorized into two groups: Oligotrophic (blue points) and Mesotrophic (green points). Sites categorized as Oligotrophic contain low water nutrients, with zero to little plant growth. The total phosphorous content is &lt; 0.025 (μmol/L). Sites categorized as Mesotrophic contain greater concentrations of total phosphorous, moderate water nutrients, and have some plant growth.\"\nw_caption &lt;- \"Source: Henry Briceno. (2023). Surface Water Quality Monitoring Data (FCE LTER), \\nFlorida, USA, June 1989-ongoing\"\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      dumbbell labels        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nd_title &lt;- \"&lt;span style='color:#3B1608;'&gt;**Investigating Water Nutrient Trends in**&lt;/span&gt; &lt;span style='color:#34A0A4;'&gt;**Oligotrophic**&lt;/span&gt; and &lt;span style='color:#AACC00;'&gt;**Mesotrophic**&lt;/span&gt; &lt;span style='color:#3B1608;'&gt;**Estuaries**&lt;/span&gt;\"\n  #\"Investigating Water Nutrients (umol/L) in &lt;span style='color:#34A0A4;'&gt;**Oligotrophic**&lt;/span&gt; and &lt;span style='color:#AACC00;'&gt;**Mesotrophic**&lt;/span&gt;\"\nd_subtitle &lt;- \"LTER Surveying Sites in the Florida Coastal Everglades\"\nd_alt &lt;- \"This is a dumbbell plot comparing the average water nutrient levels of Oligotrophic and Mesotrophic sites. This data is taken from the LTER water quality surveying sites located in the Florida Coastal Everglades. The sites are categorized into two groups: Oligotrophic (blue points) and Mesotrophic (green points). Sites categorized as Oligotrophic contain low water nutrients, with zero to little plant growth. The total phosphorous content is &lt; 0.025 (μmol/L).These sites demonstrated higher total nitrogen concentrations and alkaline phosphatase activity. In all other water nutrient categories, sites classified as Oligotrophic had fewer nutrients. Sites categorized as Mesotrophic contain greater concentrations of total phosphorous, moderate water nutrients, and have some plant growth.\"\nd_caption &lt;- \"Source: Henry Briceno. (2023). Surface Water Quality Monitoring Data (FCE LTER), \\nFlorida, USA, June 1989-ongoing\"\n\n\n\n\nColors & Typeface\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        plot customizations                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        wq status palette        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# water quality status palette \nstatus_pal &lt;- c(\"Oligotrophic\" = \"#34A0A4\", \"Mesotrophic\" = \"#AACC00\")\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----              fonts              ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# enable {showtext} for rendering \nshowtext_auto()\n# import fonts \nfont_add_google(name = \"Josefin Sans\", family = \"josefin\")\nfont_add_google(name = \"Sen\", family = \"sen\")"
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. A spike in air pollutants between the months of December 2017 and January 2018 is clearly observed, indicating negative effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. A spike in air pollutants between the months of December 2017 and January 2018 is clearly observed, indicating negative effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)",
    "text": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)\nBelow, we’ve overlayed the fire perimeter boundaries for the Thomas Fire over it’s respective burn scar. Our false color image displays an outline of the fire scorn area of Santa Barbara County. Here, we can observe the severity of the damage by generally visualizing the area and the types of regions impacted by the wildfire.\n\n\nCode\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Adjust the figure size and DPI\nfig, ax = plt.subplots(figsize=(6, 6)) \n\n# Plot false color bands\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# Plot Thomas Fire burn area\nthomas_fire.plot(ax=ax, color='none', edgecolor='#AA4203')\n\n# Add a legend for the burned area\nfire_scar = mpatches.Patch(color='#AA4203', label='Burned Area')\nax.legend(handles=[fire_scar], loc='upper right', fontsize=12) \n\n# Add a title to the plot\nax.set_title('False Color Image (SWIR22, NIR08, Red)', fontsize=14)  \n\n# Remove the plot axes\nax.axis('off')\n\n# Save the figure with tight bounding box and high resolution\nplt.savefig('thomas_fire_plot.png')  \nplt.show()\n\n\n\n\n\n\nCombining the Visuals\nLet’s put the most informative visuals together into a single figure to gain a full sense of the extent of damage caused by the Thomas Fire. In the figure on the right, can see a large parcel of Santa Barbara County was exposed to the fire, explaining the uptake in particulate matter observed in December of 2017 in AQI the figure on the left.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n# Plot the images in the same figure side-by-side\nfig, (ax1, ax2) = plt.subplots(1, 2, \n                               figsize = (22,8)) \n\n\n#------------------------------------------------------------------------\n# Plotting the Daily & 5-Day Rolling Window AQI for Santa Barbara County \n#------------------------------------------------------------------------\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax1.fill_between(aqi_sb.index, lower, upper, color=color, alpha=0.2, label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax1,\n            y=['aqi', 'five_day_average'],\n            color=colors,             \n            ylim= (0,400),\n            legend= True,\n            ylabel='AQI Values (ppm)'\n            )\n\n# applying customizations\nax1.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=21) \n\nax1.set_xlabel(xlabel='Date',\n               fontsize = 13)\n\nax2.set_ylabel(ylabel='AQI Values (ppm)',\n               fontsize = 10)\n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=6, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax1.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate', \n                                          'Unhealthy for Sensitive Groups'\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 18)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax1.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(1, 0.86),\n                            fontsize = 18)\n\n# Add both legends to the plot\nax1.add_artist(background_legend_art)\nax1.add_artist(line_legend_art)\n\n# Add annotation\nax1.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.28, 0.52), # position\n            xycoords='figure fraction', \n            fontsize=20, \n            color='black') \n#------------------------------------------------------------------------\n#        Plotting the Thomas Fire burned areas of Santa Barbara\n#------------------------------------------------------------------------\n # Remove plot axes\nplt.axis('off')                               \n# Plot false color bands\nfalse_color.plot.imshow(ax = ax2,              \n                        # Include colors\n                        robust = True)        \n\n# Plot thomas fire burn area\nthomas_fire.plot(ax = ax2,                     \n       # No color of burn area\n       color = 'none',                         \n       # Opacity of edgecolor\n       edgecolor = '#AA4203')     \n          \n# Add a figure legend\nfire_scar = mpatches.Patch(color = '#AA4203',\n                          label = 'Burned Area') \n\nax2.legend(handles=[fire_scar])\n\n#------------------------------------\n# ----   Plot the Fire Bound    -----\n#------------------------------------\nthomas_fire.plot(ax = ax2,\n                 color = '#AA4203',\n                 # make border around shapefile\n                edgecolor = '#AA4203', \n                 # make transparent\n                alpha = 0.5)\n\nfire_patch = mpatches.Patch(color = '#AA4203',\n                            label = \"Thomas Fire\")\n\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Plot the false landsat\nfalse_color.plot.imshow(ax = ax2,\n                          robust = True) \n\n# Edit the Legend and Caption \n# Show lables for legend\nax2.legend(handles = [fire_patch], \n        # No border around legend\n          frameon = True,\n        # Where legend is located\n          bbox_to_anchor = (0.9, 0.8),\n          fontsize = 18) \n \n# Add title\nax2.set_title('Areas Burned by Thomas Fire in Santa Barbara, CA (2017)',\n               fontsize = 21) \n\n# Plot the whole figure  \n# space well\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\nCitations:\n\nWikipedia contributors. “Thomas Fire.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 16 Apr. 2024. Web. 2 Aug. 2024.\nVCFD determines cause of The thomas fire. Ventura County Fire Department. (2019, March 13). https://vcfd.org/news/vcfd-determines-cause-of-the-thomas-fire/\nWildfire Today. “Thomas Fire Archives - Wildfire Today.” Wildfire Today, 14 Nov. 2019, https://wildfiretoday.com/tag/thomas-fire.\nCalifornia, State Of. Montecito Mudslides Anniversary, Reflections Through Images | Cal OES News. https://news.caloes.ca.gov/montecito-mudslides-anniversary-reflections-through-images.\nAirNow.gov, U.S. EPA. (n.d.). Aqi Basics. AQI Basics | AirNow.gov. https://www.airnow.gov/aqi/aqi-basics/\nMicrosoft Planetary Computer. Planetary Computer. (n.d.). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nCalifornia fire perimeters (all). California State Geoportal. (n.d.). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about"
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html",
    "title": "Affordable Housing Project",
    "section": "",
    "text": "There is not enough affordable housing available to meet the growing demand Gentrification leads to rising costs, and the displacement of people from rising housing costs has ramifications for access to public services like quality education, food, and transportation\nIncreasing density in areas to raise the median income to generator money for public services\nBarriers to development in the private sector can be addressed with city regulations that require developers to build affordable housing for more square footage instead of paying for credits to develop inland affordable housing that never happens.\nA means to streamline the production of affordable housing is the inclusion of incentives such as: increased housing density, relaxed parking requirements, or other similar concessions influence the production of affordable housing.\nData science can be used to make the identification of potential land and use by communities and private developers easier.\nCities need to make the priority developing quality affordable housing that increases stability and social mobility that can grant people autonomy\nWorking with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nGoing beyond building homes, the public sector should collaborate with local industries and nonprofit organizations to create workshops/vocational schools and local job opportunities. Increasing the overall quality of life for residents through the implementation of a ground-up (community-based) rehabilitation process for unhoused people."
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "title": "Affordable Housing Project",
    "section": "Data Science in Affordable Housing",
    "text": "Data Science in Affordable Housing\n\nData in Development\nThere are many things that can make the building of affordable housing an easy option.\n\nMaking the information available for use and implementation\n\nCompile a list / map of available plots that the city owns and the zoning options for those plots\nFor each area of land available for development:\n\nPresent 2 options:\n\nNormal development: Present the proposed zoning and floor area (FAR)\nAffordable housing development: Present the proposed up-zoning and floor area (FAR)\n\nAllow developers to compare options and make it easy for those that are interested in developing quality affordable housing.\n\n\n\n\nDeveloping Research Tools\nQuestions to consider:\n\nWhat is the goal?\n\nWhat are you trying to reveal with your analysis? Who are you trying to help?\n\nWho is defining the goal?\n\nHave you talked to members of the community that are affected by this scrutiny/observation?\nHave you made their voices central to defining the problem and listened to proposed solutions?\n\nWhat assumptions are you making about the community?\nWhat factors are you considering to be important?\n\nWhat data are you using to determine location/accessibility?\nAre you selecting the right variables in your understanding?\nHow did you choose those variables/indicators?\n\n\n\n\n\nOrganizations conducting data collection and analysis on affordable housing\n\n\n\nHousing Assessment Resources Tools (HART)\n\nLearn more here\nHousing assessment tool: Measures core housing needs and affordable housing costs by income category, household size, and vulnerable populations.\nLand assessment tool: looks at the area in Toronto that could be developed and measures its proximity to important amenities. Makes recommendations for building affordable housing in those areas because they already have access to the amenities that they would need.\n\n\nProperty acquisition tool: acquisition of existing housing to maintain access to affordable housing supply over the long term, and develop resources to aid governments at all levels to implement effective acquisition strategies.\n\n\nOther and Belonging Institute\n\n\nLearn more here\nThis institute at Berkeley works on understanding marginalization and exclusion. They have published research on the benefits of community land trusts as stewards of public land. Provide a guide for how local governments can partner with community land trusts to achieve their goals.\n\n\nHow does a Community Land Trust work:\n\nUse ground lease that ensures permanent affordability and community control to provide lower income community members\nSupport residents with services that ensure their financial stability and ability to thrive\n\n\n\nCities can donate surplus land to CLT’s to support communities.\n\n\nParkdale Neighborhood land trust\n\n\nLearn more here\nNon-profit that owns and manages land in a community ownership model, and partners with housing partners who then provide high quality affordable housing, supportive housing, and community economic development programs.\n\n\nTapestry\n\nLearn more here\nInvestment marketplace that finances community projects as a form of impact investing. Help nonprofits and cooperatives raise funds for their projects through a network of investors and buying bonds.\n\n\nCommunity Ethics Research Workshop (CREW)\n\n\nLearn more here\n“We want to develop a community review model that empowers community members to review and offer suggestions on research proposals in collaboration with researchers. Ultimately, we want to work towards reducing the harms associated with research and making it easier for researchers to collaborate in our community in a positive, respectful way.”\n\n\nBalanced Supply of Housing Research Cluster (BSHRC)\nLearn morehere\nLooking at four major Canadian city regions to understand the attitudes on neighborhood densification for affordability, choice, and diversity."
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "title": "Affordable Housing Project",
    "section": "Providing Security Beyond Housing",
    "text": "Providing Security Beyond Housing\nTo provide true housing security to individuals, we need policies to protect our people and work to create vocational schools, mentorship programs, and/or apprenticeship opportunities to help residents find career paths and gain access to high quality local jobs. This can be achieved by partnering with existing local job providers, educational institutions, and public agencies to train community members to enter the job market, obtain high-quality work, and earn a comfortable wage. In order to build a stronger economy, it is essential that the community has a wide range of opportunities and resources available. Strengthening Community Involvement\nCommunity members carry a wealth of insights to help solve local community issues like affordable housing. Working with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nFurthermore, community members need to be notified far in advance when there is talk of a significant local project. The community members should not be forced to rely on a last minute notice as a means to voice their concerns. A trend within research findings demonstrates lower rates of residential displacement when early community collaboration is practiced in the planning process (National Low Income Housing Coalition). Incorporating the community in the planning process not only helps protect low-income or existing residents from displacement and aids in the removal of developer’s ambiguity as they navigate an uncertain planning process. Overall, fostering more support for projects. Ways to Include the Community Working with local residents to research and peer-review housing policies in other areas, and most importantly. Providing an opportunity for early feedback on planning issues that are occurring within the community. Community development staff attend community meetings frequently to allow for more collaborative solutions to issues such as: affordable housing, traffic, overlay zones, density, etc. Organizing with a local educational institution to offer architectural students an opportunity to design housing projects.\nReferences\n\n“2023 State of the Nation’s Housing Report: 4 Key Takeaways.” Cost of Home, www.habitat.org/costofhome/2023-state-nations-housing-report-lack-affordable-housing#:~:text=During%20the%20pandemic%2C%20the%20number,that%20exceeded%20half%20their%20income. Accessed 11 Dec. 2023.\n“Addressing America’s Affordable Housing Crisis.” Housing Matters, 12 Apr. 2023, housingmatters.urban.org/research-summary/addressing-americas-affordable-housing-crisis.\n“Gentrification and Neighborhood Revitalization: What’s the Difference?” National Low Income Housing Coalition, 5 Apr. 2019, nlihc.org/resource/gentrification-and-neighborhood-revitalization-whats-difference.\nHelen Eloyan, www.heleneloyan.com/. Accessed 11 Dec. 2023.\nRyan Jones June 24, 2022. ” Michigan Law Journal of Law and Mobility.” 24 June 2022, futurist.law.umich.edu/potential-solutions-to-the-first-mile-last-mile-problem/#:~:text=One%20of%20these%20challenges%2C%20known,transportation%20station%20to%20their%20destination."
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Throughout the month of February in 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives. For this project, we will estimate the number of homes in Houston that lost power as a result of the first two storms and investigate if socioeconomic factors are predictors of communities recovery from a power outage. Our analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power. To determine the number of homes that lost power, we will link (spatially join) these areas with OpenStreetMap data on buildings and roads. By linking our analysis with data from the US Census Bureau we can investigate the potential socioeconomic factors that influenced recovery.\n\n\nThe following code is a walk through of how I developed the three visualizations included in the infographic created in Canva above. I would like to note that this is an early rendition of this infographic. I plan on expanding this personal project to investigate the water management district boundaries and evaluate the different types policies in place, or lack there of, that could be potential drivers of lower nutrient levels in estuaries.\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                            establish enva                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# set default chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggtext)\nlibrary(raster)\nlibrary(leaflet)\nlibrary(cowplot)\nlibrary(treemap)\nlibrary(showtext)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(sunburstR)\nlibrary(tmaptools)\n\n\n\n\n\n\n\nWe’ll be using NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reading in the night light tif files\nVIIRS_05_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\nVIIRS_06_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\nVIIRS_05_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\nVIIRS_06_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\n\n\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL query to aid in the data loading process\nquery &lt;- \"SELECT* FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# Loading in the data with the SQL query\nhighways &lt;- st_read(\"../data/houston/gis_osm_roads_free_1.gpkg\",\n                    query = query,\n                    quiet = TRUE)\n\n# Re-projecting CRS\nhighways &lt;- highways %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid(highways)\n\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL Query to aid in data loading process\nbuilding_query &lt;- \"SELECT*\nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# Loading in the building data with the query\nbuildings &lt;- st_read(\"../data/houston/gis_osm_buildings_a_free_1.gpkg\",\n                     query = building_query,\n                     quiet = TRUE)\n\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\nUsing st_read() to load the geodatabase layers and st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. Geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer, income data is stored in the X19_INCOME layer, and the median income field B19013e1.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Loading geometry data\nsocioeconomic &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\",\n                         quiet = TRUE)  \n\n# Loading income data and renaming the columns\nincome &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\",\n                  quiet = TRUE) %&gt;%\n  dplyr::select(GEOID, B19013e1) %&gt;% \n  rename(GEOID_Data = GEOID,\n         median_income = B19013e1)\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          combine tiles          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining tiles for each date as a stars object\nfeb_07 &lt;- st_mosaic(VIIRS_05_07, VIIRS_06_07)\nfeb_16 &lt;- st_mosaic(VIIRS_05_16, VIIRS_06_16)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           make a mask           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a mask to identify locations that experienced a drop greater than 200 nW cm-2sr-1\nblackout_locations &lt;- (feb_07 - feb_16) &gt; 200\n\n# Assigning non-blackout locations to be NA values\nblackout_locations[blackout_locations == FALSE] &lt;- NA\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          vectorize  mask        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Transforming the mask into a vector and fixing improper geometries\nvector_blackout_locations &lt;- st_as_sf(blackout_locations) %&gt;% \n  st_make_valid()\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Defining the Area of Interest\nhouston &lt;- st_polygon(list(rbind(c(-96.5, 29),\n                                 c(-96.5, 30.5),\n                                 c(-94.5, 30.5),\n                                 c(-96.5, 29))))\n\n# Creating a sfc and assigning the CRS to match the vector blackout data\nhouston &lt;- st_sfc(houston) %&gt;% \n  st_set_crs(4326) %&gt;% \n  st_make_valid()\n\n# cropping blackout locations to the houston dimensions\nhouston_blackout_mask &lt;- st_crop(vector_blackout_locations, houston)\n\n# Re-projecting to match the maps\nhouston_blackout_mask &lt;- houston_blackout_mask %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid()\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          create buffer          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a 200 m buffer and dissolving it as a single object\nhighway_buffer &lt;- st_buffer(highways, dist = 200) \n\n# combines the geometries within the buffer as one geometric polygon\nhighway_buffer &lt;- st_union(highway_buffer)\n\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re-projecting the dataset to match map\nbuildings &lt;- buildings %&gt;%\n  st_transform('EPSG:3083')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Filtering for homes beyond the buffer zone affected by the blackouts\nhouston_blackout_homes &lt;- st_join(buildings, houston_blackout_highways,\n                                  .predicate = st_intersects)\n\n# the number of homes impacted was displayed as 487405\n#nrow(houston_blackout_homes)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining the data for area of interest\ncensus_income &lt;- left_join(socioeconomic, income, by = \"GEOID_Data\") %&gt;%\n  st_transform('EPSG:3083') \n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re=projecting CRS\nhouston &lt;- houston %&gt;% \n  st_transform(3083)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Cropping the data \nhouston_income &lt;- st_crop(census_income, houston)\n\n# Census tracts that experienced blackout\nblackout_census &lt;- houston_blackout_homes[houston_income,] %&gt;% \n  mutate(blackout = 'yes')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining Cropped datums\nhouston_blackout_combined &lt;- st_join(houston_income, blackout_census, left = TRUE)\n\n\n\n\n\n\nCreating final combined subsets\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for mapping                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----   identifying unimpacted areas  ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Additional check to track impact of blackout\nhouston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif[is.na(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif)] &lt;- \"FALSE\"\n\n# labels unaffected areas\nhouston_blackout_combined$blackout[is.na(houston_blackout_combined$blackout)] &lt;- \"no\"\n\n# isolates unaffected areas into data subset\nno_blackout &lt;- st_as_sf(houston_blackout_combined[!grepl(\"yes\",houston_blackout_combined$blackout),])\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        categorizing areas       ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Tracking blackouts\ntrac_with_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == TRUE)\n\n# Tracking no blackouts\ntrac_without_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == FALSE)\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        create plot labels                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          violin labels          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nv_title &lt;- \"Median Income Brackets Based on Local Median\"\nv_subtitle &lt;- \"Homes in Houston, TX Affected by Blackouts\"\nv_alt &lt;- \"This is a violin plot of homes impacted by the Houston, TX blackouts (2/21), the median income of the households affected separated into quantiles. A trend of increased blackouts were experienced by individuals earning a median income of less than or equal to 100 thousand dollars.\"\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            map labels           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nm_title &lt;- \"Homes in Houston Affected by Blackouts, Median Income\"\nm_legend &lt;- \"Median Income\"\nm_alt &lt;- \"This is a map of the homes within Houston, TX that experienced a blackout. The census tracts are colored based on median income quantiles. There is a wide spread distribution of areas that were impacted. The greatest regions that experienced a blackout were in the 25th and 50th percentiles.\"\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        plot customizations                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----              fonts              ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# enable {showtext} for rendering \nshowtext_auto()\n# import fonts \nfont_add_google(name = \"Josefin Sans\", family = \"josefin\")\nfont_add_google(name = \"Sen\", family = \"sen\")\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                      wrangle ploting data                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         subset & group          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# subset data for plotting\nviolin_data &lt;- trac_with_blackout %&gt;% \n  select(median_income, Shape) %&gt;% \n  na.omit(median_income) %&gt;% \n  mutate(\n    earning_group = case_when(\n      median_income &lt;= 50000  ~ \"Low-income\",\n      median_income &gt; 50000 & median_income &lt;= 75000  ~ \"Middle-income\",\n      median_income &gt; 75000 & median_income &lt;= 100000 ~ \"Upper-middle-income\",\n      median_income &gt; 100000 & median_income &lt;= 150000 ~ \"Upper-income\",\n      median_income &gt; 150000 & median_income &lt;= 200000 ~ \"Exceptionally High-income\"\n    )\n  ) %&gt;% \n  na.omit(earning_group)\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         factor relevel          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reorder factor levels from low to exceptionally high\nviolin_data$earning_group &lt;- fct_relevel(violin_data$earning_group, \n                                         \"Low-income\", \"Middle-income\", \n                                         \"Upper-middle-income\", \"Upper-income\",\n                                         \"Exceptionally High-income\")\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          violin  plot                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nggplot(violin_data, aes(x = rev(factor(earning_group)), y = median_income)) + \n  \n  geom_violin(aes(fill = factor(rev(earning_group)))) +  \n  geom_boxplot(width = 0.2,\n               size = 0.3,\n               alpha = 0.5) +\n  scale_fill_viridis_d(option = \"magma\", name = \"Median Income Percentile Group\", direction = -1) +\n  \n  labs(\n    x = \"Percentile Group\",\n    y = \"Median Income ($)\",\n    title = v_title,\n    subtitle = v_subtitle,\n    alt = v_alt\n  ) +\n  \n  scale_y_continuous(labels = scales::dollar_format()) +\n  \n  theme_classic() +\n  \n  theme(\n    plot.title = element_markdown(family = \"josefin\",\n                              face = \"bold\",\n                              size = 20,\n                              hjust = 0.5,\n                              vjust = 65),\n    \n     plot.subtitle = element_text(family = \"sen\",\n                                 size = 15,\n                                 hjust = 0.5),\n    \n    \n    axis.text.x = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 12,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.text.y = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 13,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.title.y = element_blank(),\n    \n    axis.title.x = element_blank(),\n    \n    legend.position = \"none\",\n    \n    # legend text customs \n    legend.text = element_text(family = \"josefin\",\n                               size = 12),\n    \n    # match my website colors\n    plot.background = element_rect(color = '#FDFBF7',\n                                   fill = '#FDFBF7'),\n    \n    panel.background = element_rect(color = '#FDFBF7',\n                                    fill = '#FDFBF7'),\n    # space on the side of the plot\n    plot.margin = margin(t = 1, r = 2, b = 1, l = 1, \"cm\")\n    \n    ) +\n  \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          map of houston                              ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          impacted areas         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntmap_mode('plot')\n#census tracts with blackouts\ntm_shape(violin_data) + \n  \n  tm_basemap(leaflet::providers$OpenStreetMap) + \n  \n  tm_polygons(fill = \"earning_group\", \n              title = \"Median Income ($)\",\n              palette = \"magma\",\n              alpha = 0.5) +\n  \n  tm_scale_bar(position = c('left','bottom')) +\n  \n  tm_layout(\n    \n    title = m_title,\n    title.size = 19,\n    title.color =  \"#293F2C\",\n    \n   # alt.text = m_alt,\n    \n    # match my website colors\n    bg.color = \"#FDFBF7\",\n    outer.bg.color = \"#FDFBF7\"\n    \n  )"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#shedding-light-on-the-disproportionate-challenges-residents-faced-particularly-in-relation-to-median-income-levels",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#shedding-light-on-the-disproportionate-challenges-residents-faced-particularly-in-relation-to-median-income-levels",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Throughout the month of February in 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives. For this project, we will estimate the number of homes in Houston that lost power as a result of the first two storms and investigate if socioeconomic factors are predictors of communities recovery from a power outage. Our analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power. To determine the number of homes that lost power, we will link (spatially join) these areas with OpenStreetMap data on buildings and roads. By linking our analysis with data from the US Census Bureau we can investigate the potential socioeconomic factors that influenced recovery.\n\n\nThe following code is a walk through of how I developed the three visualizations included in the infographic created in Canva above. I would like to note that this is an early rendition of this infographic. I plan on expanding this personal project to investigate the water management district boundaries and evaluate the different types policies in place, or lack there of, that could be potential drivers of lower nutrient levels in estuaries."
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#set-up",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#set-up",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                            establish enva                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# set default chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggtext)\nlibrary(raster)\nlibrary(leaflet)\nlibrary(cowplot)\nlibrary(treemap)\nlibrary(showtext)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(sunburstR)\nlibrary(tmaptools)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#load-data",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#load-data",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "We’ll be using NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reading in the night light tif files\nVIIRS_05_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\nVIIRS_06_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\nVIIRS_05_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\nVIIRS_06_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\n\n\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL query to aid in the data loading process\nquery &lt;- \"SELECT* FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# Loading in the data with the SQL query\nhighways &lt;- st_read(\"../data/houston/gis_osm_roads_free_1.gpkg\",\n                    query = query,\n                    quiet = TRUE)\n\n# Re-projecting CRS\nhighways &lt;- highways %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid(highways)\n\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL Query to aid in data loading process\nbuilding_query &lt;- \"SELECT*\nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# Loading in the building data with the query\nbuildings &lt;- st_read(\"../data/houston/gis_osm_buildings_a_free_1.gpkg\",\n                     query = building_query,\n                     quiet = TRUE)\n\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\nUsing st_read() to load the geodatabase layers and st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. Geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer, income data is stored in the X19_INCOME layer, and the median income field B19013e1.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Loading geometry data\nsocioeconomic &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\",\n                         quiet = TRUE)  \n\n# Loading income data and renaming the columns\nincome &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\",\n                  quiet = TRUE) %&gt;%\n  dplyr::select(GEOID, B19013e1) %&gt;% \n  rename(GEOID_Data = GEOID,\n         median_income = B19013e1)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#wrangling-subsetting",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#wrangling-subsetting",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          combine tiles          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining tiles for each date as a stars object\nfeb_07 &lt;- st_mosaic(VIIRS_05_07, VIIRS_06_07)\nfeb_16 &lt;- st_mosaic(VIIRS_05_16, VIIRS_06_16)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           make a mask           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a mask to identify locations that experienced a drop greater than 200 nW cm-2sr-1\nblackout_locations &lt;- (feb_07 - feb_16) &gt; 200\n\n# Assigning non-blackout locations to be NA values\nblackout_locations[blackout_locations == FALSE] &lt;- NA\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          vectorize  mask        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Transforming the mask into a vector and fixing improper geometries\nvector_blackout_locations &lt;- st_as_sf(blackout_locations) %&gt;% \n  st_make_valid()\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Defining the Area of Interest\nhouston &lt;- st_polygon(list(rbind(c(-96.5, 29),\n                                 c(-96.5, 30.5),\n                                 c(-94.5, 30.5),\n                                 c(-96.5, 29))))\n\n# Creating a sfc and assigning the CRS to match the vector blackout data\nhouston &lt;- st_sfc(houston) %&gt;% \n  st_set_crs(4326) %&gt;% \n  st_make_valid()\n\n# cropping blackout locations to the houston dimensions\nhouston_blackout_mask &lt;- st_crop(vector_blackout_locations, houston)\n\n# Re-projecting to match the maps\nhouston_blackout_mask &lt;- houston_blackout_mask %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid()\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          create buffer          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a 200 m buffer and dissolving it as a single object\nhighway_buffer &lt;- st_buffer(highways, dist = 200) \n\n# combines the geometries within the buffer as one geometric polygon\nhighway_buffer &lt;- st_union(highway_buffer)\n\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re-projecting the dataset to match map\nbuildings &lt;- buildings %&gt;%\n  st_transform('EPSG:3083')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Filtering for homes beyond the buffer zone affected by the blackouts\nhouston_blackout_homes &lt;- st_join(buildings, houston_blackout_highways,\n                                  .predicate = st_intersects)\n\n# the number of homes impacted was displayed as 487405\n#nrow(houston_blackout_homes)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining the data for area of interest\ncensus_income &lt;- left_join(socioeconomic, income, by = \"GEOID_Data\") %&gt;%\n  st_transform('EPSG:3083') \n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re=projecting CRS\nhouston &lt;- houston %&gt;% \n  st_transform(3083)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Cropping the data \nhouston_income &lt;- st_crop(census_income, houston)\n\n# Census tracts that experienced blackout\nblackout_census &lt;- houston_blackout_homes[houston_income,] %&gt;% \n  mutate(blackout = 'yes')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining Cropped datums\nhouston_blackout_combined &lt;- st_join(houston_income, blackout_census, left = TRUE)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#affected-areas",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#affected-areas",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Creating final combined subsets\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for mapping                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----   identifying unimpacted areas  ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Additional check to track impact of blackout\nhouston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif[is.na(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif)] &lt;- \"FALSE\"\n\n# labels unaffected areas\nhouston_blackout_combined$blackout[is.na(houston_blackout_combined$blackout)] &lt;- \"no\"\n\n# isolates unaffected areas into data subset\nno_blackout &lt;- st_as_sf(houston_blackout_combined[!grepl(\"yes\",houston_blackout_combined$blackout),])\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        categorizing areas       ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Tracking blackouts\ntrac_with_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == TRUE)\n\n# Tracking no blackouts\ntrac_without_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == FALSE)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#customs",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#customs",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        create plot labels                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          violin labels          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nv_title &lt;- \"Median Income Brackets Based on Local Median\"\nv_subtitle &lt;- \"Homes in Houston, TX Affected by Blackouts\"\nv_alt &lt;- \"This is a violin plot of homes impacted by the Houston, TX blackouts (2/21), the median income of the households affected separated into quantiles. A trend of increased blackouts were experienced by individuals earning a median income of less than or equal to 100 thousand dollars.\"\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            map labels           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nm_title &lt;- \"Homes in Houston Affected by Blackouts, Median Income\"\nm_legend &lt;- \"Median Income\"\nm_alt &lt;- \"This is a map of the homes within Houston, TX that experienced a blackout. The census tracts are colored based on median income quantiles. There is a wide spread distribution of areas that were impacted. The greatest regions that experienced a blackout were in the 25th and 50th percentiles.\"\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        plot customizations                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----              fonts              ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# enable {showtext} for rendering \nshowtext_auto()\n# import fonts \nfont_add_google(name = \"Josefin Sans\", family = \"josefin\")\nfont_add_google(name = \"Sen\", family = \"sen\")"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#visuals",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#visuals",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                      wrangle ploting data                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         subset & group          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# subset data for plotting\nviolin_data &lt;- trac_with_blackout %&gt;% \n  select(median_income, Shape) %&gt;% \n  na.omit(median_income) %&gt;% \n  mutate(\n    earning_group = case_when(\n      median_income &lt;= 50000  ~ \"Low-income\",\n      median_income &gt; 50000 & median_income &lt;= 75000  ~ \"Middle-income\",\n      median_income &gt; 75000 & median_income &lt;= 100000 ~ \"Upper-middle-income\",\n      median_income &gt; 100000 & median_income &lt;= 150000 ~ \"Upper-income\",\n      median_income &gt; 150000 & median_income &lt;= 200000 ~ \"Exceptionally High-income\"\n    )\n  ) %&gt;% \n  na.omit(earning_group)\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         factor relevel          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reorder factor levels from low to exceptionally high\nviolin_data$earning_group &lt;- fct_relevel(violin_data$earning_group, \n                                         \"Low-income\", \"Middle-income\", \n                                         \"Upper-middle-income\", \"Upper-income\",\n                                         \"Exceptionally High-income\")\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          violin  plot                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nggplot(violin_data, aes(x = rev(factor(earning_group)), y = median_income)) + \n  \n  geom_violin(aes(fill = factor(rev(earning_group)))) +  \n  geom_boxplot(width = 0.2,\n               size = 0.3,\n               alpha = 0.5) +\n  scale_fill_viridis_d(option = \"magma\", name = \"Median Income Percentile Group\", direction = -1) +\n  \n  labs(\n    x = \"Percentile Group\",\n    y = \"Median Income ($)\",\n    title = v_title,\n    subtitle = v_subtitle,\n    alt = v_alt\n  ) +\n  \n  scale_y_continuous(labels = scales::dollar_format()) +\n  \n  theme_classic() +\n  \n  theme(\n    plot.title = element_markdown(family = \"josefin\",\n                              face = \"bold\",\n                              size = 20,\n                              hjust = 0.5,\n                              vjust = 65),\n    \n     plot.subtitle = element_text(family = \"sen\",\n                                 size = 15,\n                                 hjust = 0.5),\n    \n    \n    axis.text.x = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 12,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.text.y = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 13,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.title.y = element_blank(),\n    \n    axis.title.x = element_blank(),\n    \n    legend.position = \"none\",\n    \n    # legend text customs \n    legend.text = element_text(family = \"josefin\",\n                               size = 12),\n    \n    # match my website colors\n    plot.background = element_rect(color = '#FDFBF7',\n                                   fill = '#FDFBF7'),\n    \n    panel.background = element_rect(color = '#FDFBF7',\n                                    fill = '#FDFBF7'),\n    # space on the side of the plot\n    plot.margin = margin(t = 1, r = 2, b = 1, l = 1, \"cm\")\n    \n    ) +\n  \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          map of houston                              ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          impacted areas         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntmap_mode('plot')\n#census tracts with blackouts\ntm_shape(violin_data) + \n  \n  tm_basemap(leaflet::providers$OpenStreetMap) + \n  \n  tm_polygons(fill = \"earning_group\", \n              title = \"Median Income ($)\",\n              palette = \"magma\",\n              alpha = 0.5) +\n  \n  tm_scale_bar(position = c('left','bottom')) +\n  \n  tm_layout(\n    \n    title = m_title,\n    title.size = 19,\n    title.color =  \"#293F2C\",\n    \n   # alt.text = m_alt,\n    \n    # match my website colors\n    bg.color = \"#FDFBF7\",\n    outer.bg.color = \"#FDFBF7\"\n    \n  )"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#footnotes",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#footnotes",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Professionally\nI am seeking a full-time position at a forward-thinking organization where I can make meaningful contributions and further develop my skills as a data professional. I carry a hefty toolbox of analytical techniques applicable to a broad scope of tasks. I received my Master of Environmental Data Science (MEDS) degree at the Bren School of Environmental Science & Management (June 2024). I am also an ACS certified chemist, I obtained my degree in Chemistry from the University of California, San Diego (June 2023). I am passionate about continuous learning and growth. Eager to explore new technologies and methodologies to stay ahead of the curve in this rapidly evolving field.\nAs a Data Engineer Graduate Research Assistant at The 2035 Initiative, I’ve had the opportunity to work on a variety of projects geared towards the sustainable transition to decarbonization. The majority of my projects are people-focused. I aid in gathering the public’s perspective on energy related policies and leverage collected data to help drive informed action. In my role, I employ various geo-spatial techniques to develop workflows for surveying projects. I analyze and interpret Qualtrics data by generating Tableau reports, as well as maps, and graphs using R and Python. I’ve debugged workflows and provided descriptive feedback on developmental changes for data mining projects. I foster collaboration among interdisciplinary teams to create unique solutions and deliverables.\n\n\n\n\n\n\n\n\nMEDS Class of 2024 the day of our Capstone Public Presentation! Date: May 2024.\n\nI aspire to work with an organization that shares my dedication to environmental stewardship and social equity. My goal is to amplify the voices of vulnerable communities, ensuring they are heard and considered in climate change decisions. By adopting a grassroots approach, I believe we can create a more just and sustainable world for future generations. Together, we can turn our collective vision into reality, forging a path toward a brighter, more equitable future for all.\n\n\nBehind the Screen\nIn case you were curious about my name pronunciation, (“Sof-aya” like papaya), it’s actually a piece of my family’s history. See, my mom’s ancestors immigrated from Trondheim, Norway and carried a tradition of namesakes. Unlike the typical Jr., Sr., The III, our family skips every other generation. For example, I’m named after my beloved grandmother, Sofia Anne. This tradition’s been in our family for eight generations! Growing up, I didn’t always love having a unique pronunciation, but it’s given me the confidence to speak up and politely correct someone. I love my name, the strength it gives me, and my lineage tied to it.\n\n\n\n\n\n\n\n\nThis is my lovely grandmother, whom I was named after, Sofia Anne. In this photo, we were out enjoying brunch at this tasty place called Madison on Park in San Diego. Date: June 2023.\n\nNow a little bit about me! My favorite way to spend my free time is dancing, trying out new recipes, yoga, and organizing get-togethers. I thoroughly enjoy every aspect of planning a gathering. From visualizing the event, creating task lists, and sending out invitations, to coordinating all the necessary elements to create meaningful memories. It brings me delight seeing it all come together in full fruition.\n\n\n\n\n\n\n\n\n\n\n\n\nOur zumba crew takes on ecstatic dance in Santa Barbara! Date: April 2024.\n\n\n\n\n\n\n\n\n\n\n\n\nBeach day meets craft time with the council (aka our nickname for our friend group lol) in Malibu. Date: July 2022.\n\n\nAs of recently, I’m in a post graduation limbo and only working part-time. Therefore, I am seizing the opportunity to experiment with containerization and delve deeper into continuous integration and continuous delivery. I often find myself thinking about the cost of carbon for code queries. Over the next few years, I am committed to refining my software development skills to achieve the highest efficiency and lowest computational cost."
  }
]