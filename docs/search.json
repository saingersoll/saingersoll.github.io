[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Here you’ll find a collection of selected projects from my Maters program for Environmental Data Science at the Bren School of Environmental Science & Management, University of California Santa Barbara."
  },
  {
    "objectID": "posts.html#welcome",
    "href": "posts.html#welcome",
    "title": "My Portfolio",
    "section": "",
    "text": "Here you’ll find a collection of selected projects from my Maters program for Environmental Data Science at the Bren School of Environmental Science & Management, University of California Santa Barbara."
  },
  {
    "objectID": "scratch/ideas.html",
    "href": "scratch/ideas.html",
    "title": "",
    "section": "",
    "text": "A couple of gal pals going to see Steve Lacy! Date: May 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sofia Ingersoll",
    "section": "",
    "text": "People know me as a carbon crusader, an environmental data scientist, and a certified chemist (inside and outside of the kitchen). I utilize data science to amplify the voices of vulnerable communities to drive sustainable climate-solutions paired with actionable policy suggestions.\nCurrently, I’m at The 2035 Initiative researching global climate adaptation and resiliency in disadvantaged communities and aiding in the development of an equitable clean energy grid model for CA. I employ techniques like geospatial analysis, and statistics to quantify and visualize diverse sets of empirical research and environmental data to support transformational policy change.\nI am a continuous learner. I actively seek to satiate my curiosity for emerging energy technologies, as well as, strategies to tackle GHG emissions and the carbon economy.\nFeel free to explore my website and learn more about me and a selection of my projects. If my content resonates with you, let’s connect and discuss how I can bring value to your team!"
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html",
    "title": "Affordable Housing Project",
    "section": "",
    "text": "There is not enough affordable housing available to meet the growing demand Gentrification leads to rising costs, and the displacement of people from rising housing costs has ramifications for access to public services like quality education, food, and transportation\nIncreasing density in areas to raise the median income to generator money for public services\nBarriers to development in the private sector can be addressed with city regulations that require developers to build affordable housing for more square footage instead of paying for credits to develop inland affordable housing that never happens.\nA means to streamline the production of affordable housing is the inclusion of incentives such as: increased housing density, relaxed parking requirements, or other similar concessions influence the production of affordable housing.\nData science can be used to make the identification of potential land and use by communities and private developers easier.\nCities need to make the priority developing quality affordable housing that increases stability and social mobility that can grant people autonomy\nWorking with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nGoing beyond building homes, the public sector should collaborate with local industries and nonprofit organizations to create workshops/vocational schools and local job opportunities. Increasing the overall quality of life for residents through the implementation of a ground-up (community-based) rehabilitation process for unhoused people."
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "title": "Affordable Housing Project",
    "section": "Data Science in Affordable Housing",
    "text": "Data Science in Affordable Housing\n\nData in Development\nThere are many things that can make the building of affordable housing an easy option.\n\nMaking the information available for use and implementation\n\nCompile a list / map of available plots that the city owns and the zoning options for those plots\nFor each area of land available for development:\n\nPresent 2 options:\n\nNormal development: Present the proposed zoning and floor area (FAR)\nAffordable housing development: Present the proposed up-zoning and floor area (FAR)\n\nAllow developers to compare options and make it easy for those that are interested in developing quality affordable housing.\n\n\n\n\nDeveloping Research Tools\nQuestions to consider:\n\nWhat is the goal?\n\nWhat are you trying to reveal with your analysis? Who are you trying to help?\n\nWho is defining the goal?\n\nHave you talked to members of the community that are affected by this scrutiny/observation?\nHave you made their voices central to defining the problem and listened to proposed solutions?\n\nWhat assumptions are you making about the community?\nWhat factors are you considering to be important?\n\nWhat data are you using to determine location/accessibility?\nAre you selecting the right variables in your understanding?\nHow did you choose those variables/indicators?\n\n\n\n\n\nOrganizations conducting data collection and analysis on affordable housing\n\n\n\nHousing Assessment Resources Tools (HART)\n\nLearn more here\nHousing assessment tool: Measures core housing needs and affordable housing costs by income category, household size, and vulnerable populations.\nLand assessment tool: looks at the area in Toronto that could be developed and measures its proximity to important amenities. Makes recommendations for building affordable housing in those areas because they already have access to the amenities that they would need.\n\n\nProperty acquisition tool: acquisition of existing housing to maintain access to affordable housing supply over the long term, and develop resources to aid governments at all levels to implement effective acquisition strategies.\n\n\nOther and Belonging Institute\n\n\nLearn more here\nThis institute at Berkeley works on understanding marginalization and exclusion. They have published research on the benefits of community land trusts as stewards of public land. Provide a guide for how local governments can partner with community land trusts to achieve their goals.\n\n\nHow does a Community Land Trust work:\n\nUse ground lease that ensures permanent affordability and community control to provide lower income community members\nSupport residents with services that ensure their financial stability and ability to thrive\n\n\n\nCities can donate surplus land to CLT’s to support communities.\n\n\nParkdale Neighborhood land trust\n\n\nLearn more here\nNon-profit that owns and manages land in a community ownership model, and partners with housing partners who then provide high quality affordable housing, supportive housing, and community economic development programs.\n\n\nTapestry\n\nLearn more here\nInvestment marketplace that finances community projects as a form of impact investing. Help nonprofits and cooperatives raise funds for their projects through a network of investors and buying bonds.\n\n\nCommunity Ethics Research Workshop (CREW)\n\n\nLearn more here\n“We want to develop a community review model that empowers community members to review and offer suggestions on research proposals in collaboration with researchers. Ultimately, we want to work towards reducing the harms associated with research and making it easier for researchers to collaborate in our community in a positive, respectful way.”\n\n\nBalanced Supply of Housing Research Cluster (BSHRC)\nLearn morehere\nLooking at four major Canadian city regions to understand the attitudes on neighborhood densification for affordability, choice, and diversity."
  },
  {
    "objectID": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "href": "posts/2023-12-13-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "title": "Affordable Housing Project",
    "section": "Providing Security Beyond Housing",
    "text": "Providing Security Beyond Housing\nTo provide true housing security to individuals, we need policies to protect our people and work to create vocational schools, mentorship programs, and/or apprenticeship opportunities to help residents find career paths and gain access to high quality local jobs. This can be achieved by partnering with existing local job providers, educational institutions, and public agencies to train community members to enter the job market, obtain high-quality work, and earn a comfortable wage. In order to build a stronger economy, it is essential that the community has a wide range of opportunities and resources available. Strengthening Community Involvement\nCommunity members carry a wealth of insights to help solve local community issues like affordable housing. Working with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nFurthermore, community members need to be notified far in advance when there is talk of a significant local project. The community members should not be forced to rely on a last minute notice as a means to voice their concerns. A trend within research findings demonstrates lower rates of residential displacement when early community collaboration is practiced in the planning process (National Low Income Housing Coalition). Incorporating the community in the planning process not only helps protect low-income or existing residents from displacement and aids in the removal of developer’s ambiguity as they navigate an uncertain planning process. Overall, fostering more support for projects. Ways to Include the Community Working with local residents to research and peer-review housing policies in other areas, and most importantly. Providing an opportunity for early feedback on planning issues that are occurring within the community. Community development staff attend community meetings frequently to allow for more collaborative solutions to issues such as: affordable housing, traffic, overlay zones, density, etc. Organizing with a local educational institution to offer architectural students an opportunity to design housing projects.\nReferences\n\n“2023 State of the Nation’s Housing Report: 4 Key Takeaways.” Cost of Home, www.habitat.org/costofhome/2023-state-nations-housing-report-lack-affordable-housing#:~:text=During%20the%20pandemic%2C%20the%20number,that%20exceeded%20half%20their%20income. Accessed 11 Dec. 2023.\n“Addressing America’s Affordable Housing Crisis.” Housing Matters, 12 Apr. 2023, housingmatters.urban.org/research-summary/addressing-americas-affordable-housing-crisis.\n“Gentrification and Neighborhood Revitalization: What’s the Difference?” National Low Income Housing Coalition, 5 Apr. 2019, nlihc.org/resource/gentrification-and-neighborhood-revitalization-whats-difference.\nHelen Eloyan, www.heleneloyan.com/. Accessed 11 Dec. 2023.\nRyan Jones June 24, 2022. ” Michigan Law Journal of Law and Mobility.” 24 June 2022, futurist.law.umich.edu/potential-solutions-to-the-first-mile-last-mile-problem/#:~:text=One%20of%20these%20challenges%2C%20known,transportation%20station%20to%20their%20destination."
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nIn AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. The background of the plot is color categorized according to the AQI’s six categories of concern for ppm levels. A relative trend of moderate is seen throughout the year. However,a spike in air pollutants between the months of December 2017 and January 2018 is clearly observed. The sharp initial peak observed the day the Thomas Fire began (Dec. 4, 2017) caused an inital AQI response within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\nLet’s get a full understanding of exactly how much local area was affected by the Thomas Fire by importing the fire perimeter boundary information & some super cool Landsat 2\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nIn AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. The background of the plot is color categorized according to the AQI’s six categories of concern for ppm levels. A relative trend of moderate is seen throughout the year. However,a spike in air pollutants between the months of December 2017 and January 2018 is clearly observed. The sharp initial peak observed the day the Thomas Fire began (Dec. 4, 2017) caused an inital AQI response within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\nLet’s get a full understanding of exactly how much local area was affected by the Thomas Fire by importing the fire perimeter boundary information & some super cool Landsat 2\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "href": "posts/2023-12-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)",
    "text": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)\nBelow, we’ve overlayed the fire perimeter boundaries for the Thomas Fire over it’s respective burn scar. Our false color image displays an outline of the fire scorn area of Santa Barbara County. Here, we can observe the severity of the damage by generally visualizing the area and the types of regions impacted by the wildfire.\n\n\nCode\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Adjust the figure size and DPI\nfig, ax = plt.subplots(figsize=(6, 6)) \n\n# Plot false color bands\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# Plot Thomas Fire burn area\nthomas_fire.plot(ax=ax, color='none', edgecolor='#AA4203')\n\n# Add a legend for the burned area\nfire_scar = mpatches.Patch(color='#AA4203', label='Burned Area')\nax.legend(handles=[fire_scar], loc='upper right', fontsize=12) \n\n# Add a title to the plot\nax.set_title('False Color Image (SWIR22, NIR08, Red)', fontsize=14)  \n\n# Remove the plot axes\nax.axis('off')\n\n# Save the figure with tight bounding box and high resolution\nplt.savefig('thomas_fire_plot.png')  \nplt.show()\n\n\n\n\n\n\nCombining the Visuals\nLet’s put the most informative visuals together into a single figure to gain a full sense of the extent of damage caused by the Thomas Fire. In AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n# Plot the images in the same figure side-by-side\nfig, (ax1, ax2) = plt.subplots(1, 2, \n                               figsize = (22,8)) \n\n\n#------------------------------------------------------------------------\n# Plotting the Daily & 5-Day Rolling Window AQI for Santa Barbara County \n#------------------------------------------------------------------------\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax1.fill_between(aqi_sb.index, lower, upper, color=color, alpha=0.2, label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax1,\n            y=['aqi', 'five_day_average'],\n            color=colors,             \n            ylim= (0,400),\n            legend= True,\n            ylabel='AQI Values (ppm)'\n            )\n\n# applying customizations\nax1.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=21) \n\nax1.set_xlabel(xlabel='Date',\n               fontsize = 13)\n\nax2.set_ylabel(ylabel='AQI Values (ppm)',\n               fontsize = 10)\n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=6, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax1.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate', \n                                          'Unhealthy for Sensitive Groups'\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 18)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax1.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(1, 0.86),\n                            fontsize = 18)\n\n# Add both legends to the plot\nax1.add_artist(background_legend_art)\nax1.add_artist(line_legend_art)\n\n# Add annotation\nax1.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.28, 0.52), # position\n            xycoords='figure fraction', \n            fontsize=20, \n            color='black') \n#------------------------------------------------------------------------\n#        Plotting the Thomas Fire burned areas of Santa Barbara\n#------------------------------------------------------------------------\n # Remove plot axes\nplt.axis('off')                               \n# Plot false color bands\nfalse_color.plot.imshow(ax = ax2,              \n                        # Include colors\n                        robust = True)        \n\n# Plot thomas fire burn area\nthomas_fire.plot(ax = ax2,                     \n       # No color of burn area\n       color = 'none',                         \n       # Opacity of edgecolor\n       edgecolor = '#AA4203')     \n          \n# Add a figure legend\nfire_scar = mpatches.Patch(color = '#AA4203',\n                          label = 'Burned Area') \n\nax2.legend(handles=[fire_scar])\n\n#------------------------------------\n# ----   Plot the Fire Bound    -----\n#------------------------------------\nthomas_fire.plot(ax = ax2,\n                 color = '#AA4203',\n                 # make border around shapefile\n                edgecolor = '#AA4203', \n                 # make transparent\n                alpha = 0.5)\n\nfire_patch = mpatches.Patch(color = '#AA4203',\n                            label = \"Thomas Fire\")\n\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Plot the false landsat\nfalse_color.plot.imshow(ax = ax2,\n                          robust = True) \n\n# Edit the Legend and Caption \n# Show lables for legend\nax2.legend(handles = [fire_patch], \n        # No border around legend\n          frameon = True,\n        # Where legend is located\n          bbox_to_anchor = (0.9, 0.8),\n          fontsize = 18) \n \n# Add title\nax2.set_title('Areas Burned by Thomas Fire in Santa Barbara, CA (2017)',\n               fontsize = 21) \n\n# Plot the whole figure  \n# space well\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\nCitations:\n\nWikipedia contributors. “Thomas Fire.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 16 Apr. 2024. Web. 2 Aug. 2024.\nVCFD determines cause of The thomas fire. Ventura County Fire Department. (2019, March 13). https://vcfd.org/news/vcfd-determines-cause-of-the-thomas-fire/\nWildfire Today. “Thomas Fire Archives - Wildfire Today.” Wildfire Today, 14 Nov. 2019, https://wildfiretoday.com/tag/thomas-fire.\nCalifornia, State Of. Montecito Mudslides Anniversary, Reflections Through Images | Cal OES News. https://news.caloes.ca.gov/montecito-mudslides-anniversary-reflections-through-images.\nAirNow.gov, U.S. EPA. (n.d.). Aqi Basics. AQI Basics | AirNow.gov. https://www.airnow.gov/aqi/aqi-basics/\nMicrosoft Planetary Computer. Planetary Computer. (n.d.). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nCalifornia fire perimeters (all). California State Geoportal. (n.d.). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about"
  },
  {
    "objectID": "posts/2023-12-08_SDS_Wind_Infrastructure/SDS_Wind_Infrastructure.html",
    "href": "posts/2023-12-08_SDS_Wind_Infrastructure/SDS_Wind_Infrastructure.html",
    "title": "Spatially Distorted Signalling: How Opinions Against Wind Infrastructure Delay Our Transition to Renewable Energy",
    "section": "",
    "text": "Spatially Distorted Signaling: How Opinions Against Wind Infrastructure Delay Our Transition to Renewable Energy\n\n🗃️GitHub Repo: https://github.com/saingersoll/Spatially-Distorted-Signaling-US-Wind-Infrastructure\n\n\nMain Takeaways\n\nPopulation Density: Effect: A unit increase in population density is associated with a slight increase in the odds of having an operational wind plant. This suggests that areas with higher population densities are marginally more likely to host wind plants and less likely to experience minority holder opinions taking the majority.\nMedian Income: Effect: An increase in median income is linked to a decrease in the odds of having an operational wind plant. Higher income areas show a lower likelihood of wind plant activity, potentially due to different local priorities or economic factors. Uneven socio-economic power-dynamics could lead to minority opinion holders preventing the development of wind power infrastructure, alongside other renewable energy solutions.\nAnti-Wind Infrastructure Opinion: Effect: Areas with higher opposition to wind infrastructure are less likely to have operational wind plants. This aligns with expectations that local opposition impacts the establishment of wind plants.\n\n\n\n\nOverview\nThe phenomenon of Spatial Distorted Signalling (SDS) describes the mobilization of minority opinion holders pushing back electorally and promote legislation that aligns with their beliefs. Leah Stokes (et.al) has explored the SDS phenomenon as a natural experiment in her piece, Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy (2016). The findings in this paper describe that rural Canadian communities had a greater ability to mobilize and organize political push back against majority chair holders in parliament after the passing of legislation which invited the development of wind infrastructure through incentives.\nSince then, Leah has navigated the nuances of varying percentiles in races, political affiliation (particularly the % precinct gop voting share), the scale and size of the project, as well as, the volume of local mobilization in her research, Replication Data for: Prevalence and predictors of wind energy opposition in North America (2023). \nI am hoping to reproduce these naturally observed outcomes with the US Wind Data and assess the relationship of population density and the project status of wind plants. Analysis of these relationships could provide insight into understanding the scaling effect that local resistance has on spatially distorted signalling in relation to wind infrastructure projects and and sustainable climate policy.\n\n\nTechniques Applied\n\nSingle & Multivariate Logit Regression Models\nLogit & Log Odds\nPredictive Probability\nEthical Critiques: Addressing Limitations\n\n\nLimitations\n\nConsidering Omitted Variable Bias (OVB)\nNeglecting additional variables without testing is improper practice. Exogeneity is a very difficult OLS assumption to uphold – alongside a normal distribution of the error mean. A means to determine relationships between variables is running various linear regression models and comparing the \\(R^2\\) value. In this project, we focus our attention on the why OLS was not the analysis method of choice for our relationships of interest. In the instance of logistic regression, a log odds ratio must be taken to interpret each individual variable. There is a strong possibility that the models utilized in this project are not exogenous and require deeper analysis to determine the impact of underlining influences.\n\n\nInsufficient Data\nThe data set may not fully capture all relevant factors affecting wind plant activity, such as specific local policies or environmental conditions.\n\n\n\n\nU.S. Wind Power Plant Data\n\nReplication Data for: Prevalence and predictors of wind energy opposition in North America\n\nThe data source that was utilized in this project, US Wind Data, focuses on the public stance on wind infrastructure for census tract regions within a 3 km buffer zone of a wind infrastructure project. It contains categorical variables, binary variables, continuous socioeconomic factors such as % of races, % precinct political gop affiliated voting share, mobilization tactics, and more. This data is associated with the Replication Data for: Prevalence and predictors of wind energy opposition in North America, doi Harvard Dataverse, V1, 2023. The collaborators on that project include: Stokes, Leah; Franzblau, Emma; Lovering, Jessica R.; Miljanich, Chris. Leah Stokes and her awesome team gathered all of this information from American Wind Association (“awea”), operational, Columbia Sabin Center (“Columbia”).\n\nVariables of Interest:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstatus\nDescribes the project operating status. In this study, we have converted it into a binary variable: 1 is operating, 0 is not_operating.\n\n\npop_den\nTract-level 2010 census data for population density (per mi^2)\n\n\nmed_inc\nTract-level 2010 census data for median income ($)\n\n\nis_anti_wind\nBinary measure of wind opposition: 1 is against wind power developments, 0 is pro wind power developments.\n\n\n\n\n\nData Citation\n{1. Stokes, Leah; Franzblau, Emma; Lovering, Jessica R.; Miljanich, Chris. \"Replication Data for: Prevalence and predictors of wind energy opposition in North America\", https://doi.org/10.7910/DVN/LE2V0R, Harvard Dataverse, V1, 2023.}\n\n\n\nLoading Libraries\nThe following libraries were selected based on their functionality and ability to optimize our data for mapping.\n\n\nCode\n# Loading Libraries\nlibrary(tidyverse)        # essential r package \nlibrary(sf)               # package simplifies spatial dataframes\nlibrary(tmap)\nlibrary(terra)\nlibrary(broom)\nlibrary(stars)\nlibrary(sjPlot)\nlibrary(naniar)\nlibrary(cowplot)\nlibrary(maptiles)       \nlibrary(ggthemes)\nlibrary(ggspatial)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nset.seed(99)\nknitr::opts_chunk$set(echo = T, warning = F, message = F)\n\n\n\n\nRead & Wrangle Data\nConverting Vector into Raster Data for Mapping\nBelow we will use the package sf to convert the lat/long vector data into a raster geometry column. In this single line, we will also be assigning the CRS EPSG:4326 to the sf data frame. Coordinate Reference Systems, CRS, are required in order for the data to be projected onto a map. The CRS was selected because it provides a relatively proportionate display of the United States. We are open to suggestions regarding our CRS if a different project better fits our data.\n\nU.S. Wind Data\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Read & Raster      ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# reading in & storing data\nwind_data &lt;- read.csv(\"../data/wind_data/wind_data_usa.csv\") \n\n# Confirm the Data Loaded Properly\n#head(wind_data)                  # displays the first 6 rows of the data\n\n# Let's read in our data\nwind_sf &lt;- wind_data %&gt;%       \n  # creates geometry column with desired crs \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) \n                                     \n# quick CRS check\n#glimpse(crs(wind_sf))                  # output should reveal WGS84, EPSG:4326\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Check Point!       ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Let's stop and see if our outputs are what we expect.\n# Were the lat/long columns correctly converted into a geometry column?\n# setdiff() is a way to quickly determine the differences between two data sets.\n\n# Sweet! we are looking good\n#setdiff(colnames(wind_sf), colnames(wind_data))\n\n\n\n\nMapping Wind Power Infrastructure Plants in the U.S.\nBefore diving in, let’s get a sense of where we’ll be investigating. Using `ggplot()`, we can visualize the locations of wind infrastructure power plants throughout the United States. To achieve a more granular map, we’ll need to utilize another data set to create a base layer for our map in order to observe these wind plants with respect to state and county jurisdictions.\n\n\nCode\n# First visual of the U.S. wind data provided by the geometry points\nwind_plants &lt;- ggplot(wind_sf) +\n  annotation_map_tile(type = \"osm\") +\n  geom_sf(col = 'darkgreen',\n          alpha = 0.5,\n          size = 3)\n\nwind_plants\n\n\n\n\n\n\n\n\nDetermining the Process\nWe will employ a series of models to describe the effect of census tract level population density on the operating status of wind power infrastructure. A combination of binary and interaction logit regression will be considered.\nThe initial model will apply OLS regression, this is really a formality to demonstrate why OLS is not the correct approach for interpreting our relationships of interest. The following will be a model with two continuous variables\n\nRegression Model Components\nBinary Indicator Variable will be status column: opertating is 1, and not_operating will be 0.\nThese variables focus more on regionally dependent factors that intuitively seem to have an impact on mobilization variables that we don’t have time to cover in this project. We’ll be working with a mix of discrete and continuous data, so there some wrangling will be necessary to run the regressions we’re interested in.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---- Inspect & Standarize Data ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Determining Variable Assignments for OLS\n#unique(wind_sf$status)     # displays unique values in this\n\n# Need to rename status output variables\n# creating two categories: operating & not_operating\n# We are removing 'Operating | Decommissioned' because it skews the data\nunwanted_status &lt;- \"Operating | Decommissioned\"\nreplacement_status &lt;- \"Uncertain Status\"\nwind_sf$status[wind_sf$status== unwanted_status]&lt;-\"Uncertain Status\"  \n\n# were we successful ?\n#unique(wind_sf$status)     # displays unique values in this\n\n# cleaning out NAs for OLS\nwind_sf &lt;- wind_sf %&gt;%\n  filter(is.na(status) == 'FALSE') %&gt;% \n  filter(is.na(is_anti_wind) == 'FALSE') %&gt;% \n  filter(is.na(pop_den) == 'FALSE') %&gt;% \n  filter(is.na(med_inc) == 'FALSE') %&gt;% \n  filter(is.na(median_age) == 'FALSE') %&gt;% \n  filter(is.na(n_turbs) == 'FALSE')\n\n# were we successful ?\n#unique(wind_sf$status)     # displays unique values in this\n\n# if_else preserves the data type but replaces unwanted values\nwind_us &lt;- wind_sf %&gt;% \n  mutate(status = if_else(\n    status %in% c('Cancelled', 'Out of service (temporarily)', 'Standby', 'Decommissioned', 'Uncertain Status'), 'not_operating',\n    'operating') \n  )\n\n# are our only outputs \"operating\" and \"not_operating\"?\n#print(unique(wind_us$status))\n\n# status as factor and reassigned values\nwind_us &lt;- wind_us %&gt;% \n  mutate(status = case_when(status == \"operating\" ~ 1,\n            status == \"not_operating\" ~ 0))\n\n\n\n\nCheck Point!\nDoes the binary variable contain only 0s or 1s?\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Check point!       ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# are our only outputs 0 or 1?\nprint(unique(wind_us$status))\n\n\n[1] 1 0\n\n\n\n\nVisualizing the Categorical Response Variable Across a Distribution\nBefore jumping into any analysis, it’s important to get a sense of how the data is distributed and if there are any underlying trends or biases. The two visual aids we’re going to create are a violin plot with jitter points (left) and a comparative regression plot using OLS and GLM (right). Combining two figures provides us fuller insights into both the general trend and changes in probability of the binary outcome for the population density predictor.\nThe visualizations display the majority of the distribution lies within the actively operating wind infrastructure plants. A trend of inactive plants and lower population density is notable in both figures. Collectively they demonstrate smaller population densities contain more inactive wind infrastructure plants. This could be attributed to with weight of a singular vote in regions with smaller demographics.\nLocal mobilization of minority opinion holders in these regions have a greater availability to push back against policymakers. However, this visual does not encapsulate all of the necessary information required to determine this with full certainty. Our data set has low availability for non-operating infrastructure and as such, in the regression figure on the right these are being treated as outliers.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           Violin Distribution          ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create the violin plot with log scale\ndensity_plot &lt;- ggplot(data = wind_us, \n                       aes(x = factor(status), \n                           y = pop_den, \n                           fill = factor(status))) + \n  \n  geom_violin(alpha = 0.6, color = \"darkblue\") + \n  \n  geom_jitter(col = \"#F84C0B\",\n              width = 0,\n              height = 0.05,\n              alpha = 0.35,\n              size = 4) +\n  \n  labs(title = \"Population Density vs Wind Power Plant Operating Status\",\n       subtitle= \"Logorithimic Distribution\",\n       x = \"Activation Status\",\n       y = expression(\"Population Density (Log Scale, \" ~ mi^-2 ~ \")\")) + \n  \n  \n  # rename x-axis labels for clarity\n  scale_x_discrete(labels = c(\"0\" = \"Inactive\", \"1\" = \"Active\")) +  \n  # Apply logarithmic scale to y-axis\n  scale_y_log10() + \n  \n  theme_538() + \n  \n  scale_fill_manual(values = c(\"skyblue\", \"darkblue\")) +\n  \n  # Adjust title font and alignment\n  theme(plot.title = element_text(size = 40,\n                                  family = \"Georgia\", \n                                  face = \"bold\",\n                                  hjust = .99,\n                                  color =\"#293F2C\"),  \n        \n        # Adjust subtitle font and alignment\n        plot.subtitle = element_text(size = 38,\n                                     family = \"Georgia\",\n                                     color =\"#293F2C\",\n                                     hjust = 0.5), \n        \n        axis.title = element_text(size = 36,\n                                  family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n        axis.text = element_text(size = 34,\n                                 family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n         # Move legend to the bottom\n        legend.position = \"top\", \n        \n        # Remove legend title if not needed\n        legend.title = element_blank(),  \n        \n        # Adjust legend text size\n        legend.text = element_text(size = 34,\n                                   family = \"Georgia\",\n                                   color =\"#293F2C\"), \n        \n         # Background color for legend\n        legend.key = element_rect(fill = \"grey94\", color = \"grey94\"), \n        \n        plot.background = element_rect(fill = \"#FDFBF7\")\n        ) +  \n  \n  coord_flip()\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Jitter OLS + GLM                ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Optimized jitter plot with smooth lines\njitter_plot_optimized &lt;- ggplot(data = wind_us, \n                                aes(x = pop_den, \n                                    y = status)) + \n  \n  # assign color to legend \n geom_jitter(aes(color = \"Data Points\"),\n              width = 0,\n              height = 0.05,\n              alpha = 0.6,\n              size = 4) +  # Adjusted size for better visibility\n  \n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              aes(color = \"OLS Line\"),\n              size = 1.2,  # Slightly thicker line for visibility\n              linetype = \"solid\") + \n  \n  geom_smooth(method = \"glm\",\n              se = T,\n              aes(color = \"GLM Line\"),\n              size = 1.2,\n              linetype = \"dashed\",\n              method.args = list(family = \"binomial\")) +\n  \n  labs(title = \"Population Density vs Wind Power Plant Operating Status\",\n      subtitle= \"Logorithimic Distribution Regression Comparison\",\n       y = \"Activation Status\",\n       x = expression(\"Population Density (Log Scale, \" ~ mi^-2 ~ \")\")) + \n  \n  # rename yaxis labels for clarity\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"0\" = \"Inactive\", \"1\" = \"Active\")) + \n  \n  scale_x_log10() + \n  \n  theme_538() +\n  \n  # Adjust title font and alignment\n  theme(plot.title = element_text(size = 40,\n                                  family = \"Georgia\", \n                                  face = \"bold\",\n                                  hjust = .99,\n                                  color =\"#293F2C\"),  \n        \n        # Adjust subtitle font and alignment\n        plot.subtitle = element_text(size = 38,\n                                     family = \"Georgia\",\n                                     hjust = 0.5,\n                                     color =\"#293F2C\"), \n        \n        axis.title = element_text(size = 36,\n                                  family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n        axis.text = element_text(size = 34,\n                                 family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n         # Move legend to the bottom\n        legend.position = \"top\", \n        \n        # Remove legend title if not needed\n        legend.title = element_blank(),  \n        \n        # Adjust legend text size\n        legend.text = element_text(size = 34,\n                                   family = \"Georgia\",\n                                  color =\"#293F2C\"), \n        \n         # Background color for legend\n        legend.key = element_rect(fill = \"grey94\", color = \"grey94\"), \n        \n        plot.background = element_rect(fill = \"#FDFBF7\")\n        ) +   \n  \n   scale_color_manual(name = \"Legend\",  # Title for the legend\n                     values = c(\"Data Points\" = \"#F84C0B\", \n                                \"OLS Line\" = \"blue\", \n                                \"GLM Line\" = \"skyblue\"),\n                     labels = c(\"Data Points\" = \"Data Points\", \n                                \"OLS Line\" = \"OLS Line\", \n                                \"GLM Line\" = \"GLM Line\"))\n\n\n# Combine plots horizontally\ncombined_plot &lt;- density_plot + \n                  jitter_plot_optimized + \n  # Arrange plots side-by-side\n                  plot_layout(ncol = 2)  \n\ncombined_plot\n\n\n\n\n\n\n\nPredictions for Binary Indicator Variable\n\nLogistic Model with One Continuous Variable\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density)  +\\varepsilon \\]\nIn Table 2, the intercept has a log-odds of 3.272 is noted a statistically significant value (p-value &lt; 0.01). This value provides insights into the baseline probability of the outcome when all predictors are set to zero. It suggests that the baseline probability of having an active wind plant\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           1 Continuous Variable        ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Initial regression 1 betas for null\n# function\nstatus &lt;- glm(status ~ pop_den,\n                       wind_us,\n                       family = 'binomial')\n      \n# model table\nsjPlot::tab_model(status,\n          transform = NULL,\n           # predictor labels\n          pred.labels = c(\"Intercept\", \"Population Density (mi^-2)\"),\n          # include p-val\n          show.p = TRUE, \n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          dv.labels = c(\"log Probability of Active Wind Power Plant Operating Status\"),\n          string.p = \"p-value\",\n          show.r2 = FALSE,\n          title = \"Table 2: Logit Regression Model Results for Population Density\",\n          digits = 3)\n\n\n\nTable 2: Logit Regression Model Results for Population Density\n\n\n \nlog Probability of Active Wind Power Plant Operating Status\n\n\nPredictors\nLog-Odds\nCI\np-value\n\n\nIntercept\n3.272 ***\n2.968 – 3.605\n&lt;0.001\n\n\nPopulation Density (mi^-2)\n0.000 \n-0.000 – 0.001\n0.593\n\n\nObservations\n1179\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\n\n\nLogistic Model with Two Continuous Variables\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\nIn Table 3, the intercept has a log-odds of 4.028 is noted a statistically significant value (p-value &lt; 0.01). This value provides insights into the baseline probability of the outcome when all predictors are set to zero.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           2 Continuous Variables        ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nstatus_2 &lt;- glm(status ~ pop_den + med_inc,\n               wind_us,\n               family = 'binomial')\n# model table\nsjPlot::tab_model(status_2,\n          transform = NULL,\n           # predictor labels\n          pred.labels = c(\"Intercept\", \"Population Density (mi^-2)\", \"Median Income ($)\"),\n          # include p-val\n          show.p = TRUE, \n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          dv.labels = c(\"log Probability of Active Wind Power Plant Operating Status\"),\n          string.p = \"p-value\",\n          show.r2 = FALSE,\n          title = \"Table 3: Logit Regression Model Results for Population Density & Median Income\",\n          digits = 3)\n\n\n\nTable 3: Logit Regression Model Results for Population Density & Median Income\n\n\n \nlog Probability of Active Wind Power Plant Operating Status\n\n\nPredictors\nLog-Odds\nCI\np-value\n\n\nIntercept\n4.028 ***\n3.007 – 4.974\n&lt;0.001\n\n\nPopulation Density (mi^-2)\n0.000 \n-0.000 – 0.001\n0.512\n\n\nMedian Income ($)\n-0.000 *\n-0.000 – 0.000\n0.096\n\n\nObservations\n1179\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\n\n\n\nAnalysis of Log Odds Ratio\nIn order to calculate the baseline probability of having an active wind plant, the following equation will be applied:\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1 + \\beta_2+\\varepsilon \\]\nit is manipulated to solve for the probability, p: \\[p̂ = e^(β0+β1x1+eβ0+β1x)\\]\nTo simplify our workflow, we’re going to solve for p using the uniroot function to find the probability for different values of population density. It allows us to solve for a range of p values using \\(R^2\\).\nWe’re going to inspect the operational probability according to the population density quantile.\n\n\nCode\n# Define a function to calculate probability from logistic regression coefficients\nfind_probability &lt;- function(pop_den, coefficients) {\n  fun &lt;- function(p) {\n    (1 - p) * exp(coefficients[1] + coefficients[2] * pop_den) - p\n  }\n  uniroot(fun, interval = c(0, 1))$root\n}\n\n# Coefficients from the logistic regression model\ncoefficients &lt;- status$coefficients\n\n# What are the pop den quantiles?\n#quantile(wind_data$pop_den)\n\n# Population density quantiles from the data\nquantiles_pop_den &lt;- c(quantile(wind_sf$pop_den))\n\n# Calculate probabilities for each quantile value\nprobabilities &lt;- sapply(quantiles_pop_den, function(d) find_probability(d, coefficients))\n\n# Create a dataframe with the quantile population densities and their corresponding probabilities\npop_den_prob &lt;- data.frame(\n  pop_density = quantiles_pop_den,\n  probability = probabilities\n)\n\n# Print the table using kable\nkable(pop_den_prob, \n      format = \"pipe\", \n      col.names = c(\"Population Density (mi^-2)\", \"Probability\"),\n      caption = \"Table 4: Probability of Active Wind Power Plant Operating Status at Different Population Densities\")\n\n\n\nTable 4: Probability of Active Wind Power Plant Operating Status at Different Population Densities\n\n\n\nPopulation Density (mi^-2)\nProbability\n\n\n\n\n0%\n1.431998e-01\n0.9634634\n\n\n25%\n4.308644e+00\n0.9634896\n\n\n50%\n1.249329e+01\n0.9635412\n\n\n75%\n4.039379e+01\n0.9637166\n\n\n100%\n1.394051e+04\n0.9968966\n\n\n\n\n\n\n\nCode\n# Define the function to calculate probability\nfind_probability_2 &lt;- function(pop_den, med_inc, coefficients) {\n  # Logistic regression function\n  logit &lt;- coefficients_2[\"(Intercept)\"] + coefficients_2[\"pop_den\"] * pop_den + coefficients_2[\"med_inc\"] * med_inc\n  p &lt;- exp(logit) / (1 + exp(logit))\n  return(p)\n}\n\n# Define quantiles for population density and income\nquantiles_pop_den &lt;- c(0, 4.31505, 12.53426, 40.79216, 13940.51491)\nquantiles_med_inc &lt;- c(14962.0, 41424.0, 47750.0, 55405.5, 185769.0)\n\n\n# Coefficients from the logistic regression model\ncoefficients_2 &lt;- status_2$coefficients\n\n# Create a data frame for the plot\nplot_data &lt;- expand.grid(\n  income_group = quantiles_med_inc,\n  pop_den_group = quantiles_pop_den) %&gt;%\n \n   mutate(\n    probability = mapply(\n      \n      function(inc, den) find_probability_2(den, inc, coefficients_2),\n      income_group,\n      pop_den_group\n    ),\n   \n     income_group_label = factor(income_group,\n                                 levels = quantiles_med_inc,\n                                 labels = paste0(\"Income Quantile \", 1:5)),\n    \n    # name change for plotting ease\n    pop_den_group_label = factor(pop_den_group, \n                                 levels = quantiles_pop_den,\n                                 labels = paste0(\"Quantile \",\n                                                 1:5))\n )\n\n\ncolor_palette &lt;- c(\n  \"Quantile 1\" = \"grey80\",  # Very light blue\n  \"Quantile 2\" = \"#D0EAFB\",  # Light blue\n  \"Quantile 3\" = \"#A6C7E1\",  # Medium-light blue\n  \"Quantile 4\" = \"#4A90D3\",  # Medium blue\n  \"Quantile 5\" = \"#003C71\"   # Dark blue\n)\n\n\n# Create the plot\nggplot(plot_data, aes(x = income_group_label,\n                      y = probability,\n                      fill = pop_den_group_label)) +\n  \n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  \n  scale_fill_manual(values = color_palette) +\n  \n  \n  labs(\n    title = \"Probability of Active Wind Power Plant Operating Status\" ,\n    subtitle = paste0(\"Assessing Median Income ($) and Population Density (\", expression(\"mi^-2 \"), \") Quantiles\"),\n    x = \"Income Quantile ($)\",\n    y = \"Probability\",\n    fill = \"Population Density Quantile\"\n  ) +\n  \n  theme_minimal() +\n  \n  theme(\n    \n    text = element_text(family = \"Georgia\",\n                        size = 18),\n    \n    axis.text.x = element_text(size = 10),\n    \n    axis.text.y = element_text(size = 10),\n    \n    axis.title.x = element_text(size = 12),\n    \n    axis.title.y = element_text(size = 12),\n    \n    \n    plot.title = element_text(size = 17,\n                              face = \"bold\",\n                              hjust = .99),\n    \n    plot.subtitle = element_text(size = 15,\n                                 hjust = 1.25),\n    \n    plot.background = element_rect(fill = \"#FDFBF7\"),\n\n    # Adjust legend\n    legend.position = \"top\",\n    # Smaller legend title\n    legend.title = element_text(size = 12),   \n    # Smaller legend text\n    legend.text = element_text(size = 10),    \n    # Smaller legend key (box size)\n    legend.key.size = unit(0.5, \"cm\"),        \n    # Space between legend items\n    legend.spacing.x = unit(0.2, \"cm\"),     \n    # Space between legend items\n    legend.spacing.y = unit(0.2, \"cm\"),      \n    legend.background = element_rect(fill = \"white\",\n                                     color = \"grey100\",\n                                      # Background box\n                                     size = 0.5)\n\n  ) +\n  \n  guides(fill = guide_legend(title = \"Population Density Quantile\", \n                             title.position = \"top\",\n                             title.hjust = 0.5))\n\n\n\n\n\nThe probability distribution of having an active local wind plant across different population density and median income quantiles reveals some notable trends:\n\nPopulation Density: Consistent with expectations, areas with the highest population density show the highest probability of hosting wind plants in this example. This trend aligns with the assumption that regions with more people may have a higher demand for renewable energy sources like wind power. Studies have shown, “urban areas with higher population densities are often more likely to invest in environmental infrastructure, including wind energy projects” (Kahn, 2007, p. 58).\nMedian Income: Interestingly, high-income areas with lower population densities tend to have the lowest likelihood of having an active local wind plant. This observation is intriguing and suggests that higher income alone may decrease the probability of wind power plant activity. A study on socio-economic dynamics conducted by McCright and Dunlap found “socioeconomic status significantly affects individuals’ environmental attitudes and policy preferences” (McCright & Dunlap, 2011, p. 402). Their research suggests that higher-income individuals might have different priorities or less immediate need for such infrastructure compared to lower-income communities.\n\n\nQuestion:\nCould an assumption be made in which areas at risk of spatially distorted signalling have a greater propensity for higher median income and lower population density?\nLet’s touch a bit on the social psychology at potentially at play.\n\nResources and Political Mobilization:\n\nHigher Income Brackets: Addressing this question involves considering the impact of political and economic influence on energy policy. Verba, Schlozman, and Brady argue that “higher-income individuals typically have more resources and time to engage in political activities, which can influence local energy policies” (Verba, Schlozman, & Brady, 1995, p. 234). This might suggest that wealthier areas could exert more influence to prioritize different energy investments or limit the visibility of such projects.\nLower Income Brackets: Conversely, those in lower income brackets may have fewer resources and less time for such activities, potentially resulting in lower levels of wind plant advocacy and adoption.\n\nDonations and Lobbying:\n\nHigh-income individuals or entities may make substantial donations to lobbying groups or political campaigns to promote specific agendas, including energy policies that align with their interests. Hertel and Tsigas highlight that “financial contributions and lobbying play a significant role in shaping policy decisions” (Hertel & Tsigas, 2002, p. 78). This could mean that higher-income areas, with their greater financial resources, might be able to affect decisions on wind plant locations or energy policy. This financial influence can interfere the development and implementation of local energy projects.\n\nInfluence of Socioeconomic Status:\n\nThe presence of wind plants might be influenced by socioeconomic status in ways that reflect broader patterns of power and influence. These insights align with the notion that “economic power and market conditions influence decisions related to energy infrastructure” (Kirschen & Strbac, 2004, p. 112). Therefore, it is plausible that higher-income areas might both contribute to and benefit from a different set of energy policies compared to lower-income areas. Areas with higher incomes might prioritize other forms of energy or infrastructure development based on their resources and political connections.\n\n\nNow that we’ve explored probabilities for our two models in a systematic way, it’s important to have an understanding of these relationships on a continuous scale. To gain a deeper understanding of the logistic regression model, we will leverage the Odds Ratio. We will create a table displaying the Odds Ratio, which quantifies how frequently a binary event occurs relative to its baseline. This approach provides a comprehensive and more efficient way to interpret the relationships between variables in the model.\n\n\n\n\nInterpreting Coefficients Using Odds Ratio\nTo better interpret the relationship in our logistic regression model, we will focus on odds rather than probabilities. Although odds and probabilities are often confused, they are distinct concepts that are related through the following formula:\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\nThe odds of a binary event are the ratio of how often it happens, to how often it doesn’t happen. Here, \\[\\hat{p}​\\]represents the predicted probability, and \\[exp⁡(β^0+β^1⋅x)\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x)exp(β^​0​+β^​1​⋅x) \\] denotes the odds based on our model.\nWe will create an odds_hat variable to represent the predicted odds. The odds ratio for a one-unit increase in the predictor variable, x, is given by:\n\\[\ne^{\\hat{\\beta}_1}\n\\]\nThis ratio indicates how the odds of the outcome change with a one-unit increase in x. Importantly:\n\nInterpretation of Odds Ratio: eβ1e{_1}eβ^​1​ is the factor by which the odds are multiplied for a one-unit increase in the predictor variable x\nIndependence from Predictor Value: This ratio does not depend on the specific value of x; it represents a constant multiplicative effect on the odds regardless of x’s value.\n\nThus, the odds ratio provides a useful and consistent measure of the effect of a predictor variable on the odds of the outcome in logistic regression.\n\nLogistic Model with One Continuous Variable\n\n\nCode\n# Generate predictions and calculate odds\nstatus_popden_predicted_odds &lt;- status %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  mutate(y_hat = .fitted,\n         odds_hat = y_hat / (1 - y_hat)) %&gt;%\n  # Order by odds in descending order\n  arrange(desc(odds_hat)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\n# Create and style the table\nstatus_popden_predicted_odds %&gt;%\n  select(y_hat, odds_hat) %&gt;%\n  mutate(\n    y_hat = scales::percent(y_hat, accuracy = 1),\n    odds_hat = scales::number(odds_hat, accuracy = 1)\n  ) %&gt;%\n  kable(\n    caption = \"Table 5: Top 10 Predicted Odds and Probabilities from Logistic Regression\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nTable 5: Top 10 Predicted Odds and Probabilities from Logistic Regression\n\n\ny_hat\nodds_hat\n\n\n\n\n100%\n321\n\n\n99%\n159\n\n\n99%\n79\n\n\n99%\n77\n\n\n99%\n69\n\n\n99%\n69\n\n\n98%\n63\n\n\n98%\n58\n\n\n98%\n58\n\n\n98%\n57\n\n\n\n\n\n\n\n\nOur model estimates that one unit increase in population density is associated with a change in the odds ratio of \\(e^(0.0002) =1.0002\\), or a 2.0e-04% increase in the odds of wind plant having an operating status.\n\n\nCode\n# Ensure you have the correct model object\n# Summary of the model to check coefficients\nmodel_summary &lt;- summary(status)\n\n# Extract the odds ratio for population density\n# Assuming population density is the second coefficient\nodds_ratio_pop_den &lt;- exp(model_summary$coefficients[\"pop_den\", \"Estimate\"])\n\n# Generate a descriptive text with the rounded odds ratio\npaste0(\"Odds Ratio for Population Density is \", round(odds_ratio_pop_den, 4))\n\n\n[1] \"Odds Ratio for Population Density is 1.0002\"\n\n\nLogistic Model with Two Continuous Variables\n\n\nCode\n# Generate predictions and calculate odds\nstatus_popden_predicted_odds_2 &lt;- status_2 %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  mutate(y_hat = .fitted,\n         odds_hat = y_hat / (1 - y_hat)) %&gt;%\n  # Order by odds in descending order\n  arrange(desc(odds_hat)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\n# Create and style the table\nstatus_popden_predicted_odds_2 %&gt;%\n  select(y_hat, odds_hat) %&gt;%\n  mutate(\n    y_hat = scales::percent(y_hat, accuracy = 1),\n    odds_hat = scales::number(odds_hat, accuracy = 1)\n  ) %&gt;%\n  kable(\n    caption = \"Table 6: Top 10 Predicted Odds and Probabilities from Logistic Regression\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nTable 6: Top 10 Predicted Odds and Probabilities from Logistic Regression\n\n\ny_hat\nodds_hat\n\n\n\n\n100%\n1 103\n\n\n100%\n415\n\n\n99%\n146\n\n\n99%\n119\n\n\n99%\n109\n\n\n99%\n109\n\n\n99%\n102\n\n\n99%\n87\n\n\n99%\n84\n\n\n99%\n81\n\n\n\n\n\n\n\n\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\n\n\nCode\n# Ensure you have the correct model object\n# Summary of the model to check coefficients\nmodel_summary_2 &lt;- summary(status_2)\n\n# Extract and calculate the odds ratio for population density\nodds_ratio_pop_den &lt;- exp(model_summary_2$coefficients[\"pop_den\", \"Estimate\"])\n\n# Extract and calculate the odds ratio for median income\n# Note: Since the odds ratio for median income is typically 1 - exp(Estimate), this should be done correctly\nodds_ratio_med_inc &lt;- 1 - exp(model_summary_2$coefficients[\"med_inc\", \"Estimate\"])\n\n# Generate descriptive texts with the rounded odds ratios\npaste0(\"Odds Ratio for Population Density: \", round(odds_ratio_pop_den, 4))\n\n\n[1] \"Odds Ratio for Population Density: 1.0002\"\n\n\nCode\npaste0(\"Odds Ratio for Median Income: \", round(odds_ratio_med_inc, 4))\n\n\n[1] \"Odds Ratio for Median Income: 0\"\n\n\n\n\n\nProbabilistic Predictions\nTo evaluate the probability of having an active wind plant, we use out-of-sample predictions with the type.predict argument set to “response” to obtain fitted values on the probability scale.\nLogistic Model with One Continuous Variable\n\n\nCode\n# probability scale\nprobability_predictions &lt;- augment(status, type.predict = \"response\") \n\n\nprobability_predictions &lt;- probability_predictions %&gt;%\n  # Order by odds in descending order\n  arrange(desc(.fitted)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\nprobability_predictions_table &lt;- probability_predictions %&gt;%\n  mutate(Predicted_Probability = .fitted,\n         Std_Dev = .sigma) %&gt;%\n  select(Predicted_Probability, Std_Dev) %&gt;%\n  mutate(Predicted_Probability = scales::percent(Predicted_Probability, accuracy = 1)) %&gt;%\n  kable(caption = \"Table 7: Probability Predictions from Logistic Regression Model\",\n        format = \"html\",\n        digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"center\",\n                font_size = 12)\n\nprobability_predictions_table\n\n\n\n\nTable 7: Probability Predictions from Logistic Regression Model\n\n\nPredicted_Probability\nStd_Dev\n\n\n\n\n100%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n\n\n\n\n\n\nLogistic Model with Two Continuous Variables\n\n\nCode\n# probability scale\nprobability_predictions_2 &lt;- augment(status_2, type.predict = \"response\") \n\nprobability_predictions_2 &lt;- probability_predictions_2 %&gt;%\n  # Order by odds in descending order\n  arrange(desc(.fitted)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\nprobability_predictions_table_2 &lt;- probability_predictions_2 %&gt;%\n  mutate(Predicted_Probability = .fitted,\n         Std_Dev = .sigma) %&gt;%\n  select(Predicted_Probability, Std_Dev) %&gt;%\n  mutate(Predicted_Probability = scales::percent(Predicted_Probability, accuracy = 1)) %&gt;%\n  kable(caption = \"Table 8: Probability Predictions from Logistic Regression Model\",\n        format = \"html\",\n        digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"center\",\n                font_size = 12)\n\nprobability_predictions_table_2\n\n\n\n\nTable 8: Probability Predictions from Logistic Regression Model\n\n\nPredicted_Probability\nStd_Dev\n\n\n\n\n100%\n0.55\n\n\n100%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n\n\n\n\n\n\nFor example, our bivariate model predicts the odds that an area with a population density of 124 (\\(mi^-2\\)) and a median income of $46,094 and would have an operating wind plant is ~97%.\n\n\nCode\n# New data for prediction\nexploring_model &lt;- data.frame(pop_den = 124, med_inc = 46094)\n\n# Get predictions\npredicted_probability &lt;- augment(status_2, newdata = exploring_model, type.predict = \"response\") %&gt;%\n  select(.fitted) %&gt;%\n  mutate(Predicted_Probability = scales::percent(.fitted, accuracy = 1)) %&gt;%\n  rename(\"Predicted Probability\" = Predicted_Probability) %&gt;%\n  add_column(\n    \"Population Density\" = exploring_model$pop_den,\n    \"Median Income\" = exploring_model$med_inc,\n    .before = 1\n  )\n\n# Display the table\npredicted_probability %&gt;%\n  kable(\n    caption = \"Predicted Probability for Population Density and Median Income Under Specific Conditions\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nPredicted Probability for Population Density and Median Income Under Specific Conditions\n\n\nPopulation Density\nMedian Income\n.fitted\nPredicted Probability\n\n\n\n\n124\n46094\n0.97\n97%\n\n\n\n\n\n\n\n\n\n\nComprehensive Interaction Model Containing Binary Predictor Variable\n\nCoefficient and Odds Ratio Table\nTo interpret the model, we compute the odds ratios for each coefficient, providing insight into how each variable and its interactions affect wind plant activity.\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) * \\beta_2  (Median Income) * \\beta_3  (AntiWindOpinion) +\\varepsilon \\]\n\n\nCode\n# Fit the comprehensive model\ncomprehensive_model &lt;- glm(status ~ pop_den * med_inc * is_anti_wind,\n                           data = wind_us,\n                           family = 'binomial')\n\n#summary(comprehensive_model)\n\n# Compute odds ratios for the model coefficients\n#exp(coef(comprehensive_model))\n\n# Extract coefficients and compute odds ratios\ncoef_summary &lt;- data.frame(\n  Term = names(coef(comprehensive_model)),\n  Estimate = coef(comprehensive_model),\n  Odds_Ratio = exp(coef(comprehensive_model))\n)\n\n# Create and style the table\nkable(coef_summary, format = \"html\", digits = 4, caption = \"Table 10: Summary of Coefficients and Odds Ratios\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), position = \"center\", font_size = 12)\n\n\n\n\nTable 10: Summary of Coefficients and Odds Ratios\n\n\n\nTerm\nEstimate\nOdds_Ratio\n\n\n\n\n(Intercept)\n(Intercept)\n4.1017\n60.4441\n\n\npop_den\npop_den\n0.0011\n1.0011\n\n\nmed_inc\nmed_inc\n0.0000\n1.0000\n\n\nis_anti_wind\nis_anti_wind\n-0.7008\n0.4962\n\n\npop_den:med_inc\npop_den:med_inc\n0.0000\n1.0000\n\n\npop_den:is_anti_wind\npop_den:is_anti_wind\n-0.0016\n0.9984\n\n\nmed_inc:is_anti_wind\nmed_inc:is_anti_wind\n0.0000\n1.0000\n\n\npop_den:med_inc:is_anti_wind\npop_den:med_inc:is_anti_wind\n0.0000\n1.0000\n\n\n\n\n\n\n\n\nThis table provides a comprehensive view of how each variable and their interactions contribute to the likelihood of wind plant activity, facilitating a better understanding of the model’s results.\n\nMain Effects: The odds ratios for pop_den, med_inc, and is_anti_wind indicate the individual effects of each variable on the likelihood of having an active wind plant.\nInteraction Effects: The interaction terms show how the relationship between each pair of variables influences the odds of wind plant activity. For instance, the interaction between pop_den and med_inc captures how the effect of population density on wind plant status changes with median income.\n\n\n\n\nOur Findings\nOur analysis of wind plant activity using logistic regression models has provided valuable insights into the factors influencing the presence of operational wind plants.\n\nMain Takeaways\nPopulation Density: Effect: A unit increase in population density is associated with a slight increase in the odds of having an operational wind plant. This suggests that areas with higher population densities are marginally more likely to host wind plants.\nMedian Income: Effect: An increase in median income is linked to a decrease in the odds of having an operational wind plant. Higher income areas show a lower likelihood of wind plant activity, potentially due to different local priorities or economic factors.\nAnti-Wind Infrastructure Opinion: Effect: Areas with higher opposition to wind infrastructure are less likely to have operational wind plants. This aligns with expectations that local opposition impacts the establishment of wind plants.\nModel Performance:\nP-Values: The p-values for the majority of the coefficients in our models were above the significance level of 0.05. This indicates that while our models show some trends, the results are not statistically significant enough to make strong conclusions. The lack of significance suggests that there may be insufficient evidence to definitively assess the impact of these variables on wind plant activity.However, the intercept provided a baseline log-odds of having an operational wind plant fell within the range of approximately 60% for observations in this data set. The gaps within the data for areas without operating wind plants result in heavy bias towards operating wind plants.\n\n\nFuture Works Considerations\nIf given the opportunity, I would expand the dataset to include any more possible non-operational wind plants and explore in greater detail how exogenous our variables are and determine which values are likely interacting, to produce the best model fit. By expanding the dataset and refining the model, we can better understand the dynamics influencing the operational status of wind plants and make more informed conclusions. Additionally, I would bolster this analysis by identifying and including relevant variables to address the omitted variables bias present in this statistical exploration.\n\n\n\nCitations\n\nStokes, Leah C., et al. “Prevalence and Predictors of Wind Energy Opposition in North America.” Proceedings of the National Academy of Sciences, vol. 120, no. 40, Sept. 2023, doi:10.1073/pnas.2302313120.\nStoke, L. C. “Front Matter.” American Journal of Political Science, vol. 60, no. 4, 2016. JSTOR, http://www.jstor.org/stable/24877456. Accessed 26 October. 2023\nKahn, M. E. (2007). Green Cities: Urban Growth and the Environment. Brookings Institution Press.\nMcCright, A. M., & Dunlap, R. E. (2011). The Politicization of Climate Change and Polarization in the American Public’s Views of Global Warming. Society & Natural Resources, 24(5), 398-413.\nVerba, S., Schlozman, K. L., & Brady, H. E. (1995). Voice and Equality: Civic Voluntarism in American Politics. Harvard University Press.\nHertel, T. W., & Tsigas, M. E. (2002). The Role of Political Lobbying and Financial Contributions in Policy Making. In Public Policy Analysis (pp. 67-90). Routledge.\nKirschen, D. S., & Strbac, G. (2004). Fundamentals of Power System Economics. Wiley.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ingersoll2023,\n  author = {Ingersoll, Sofia},\n  title = {Spatially {Distorted} {Signalling:} {How} {Opinions}\n    {Against} {Wind} {Infrastructure} {Delay} {Our} {Transition} to\n    {Renewable} {Energy}},\n  date = {2023-12-14},\n  url = {https://saingersoll.github.io/posts/SDS_Wind_Infrastructure},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIngersoll, Sofia. 2023. “Spatially Distorted Signalling: How\nOpinions Against Wind Infrastructure Delay Our Transition to Renewable\nEnergy.” December 14, 2023. https://saingersoll.github.io/posts/SDS_Wind_Infrastructure."
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Throughout the month of February in 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives. For this project, we will estimate the number of homes in Houston that lost power as a result of the first two storms and investigate if socioeconomic factors are predictors of communities recovery from a power outage. Our analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power. To determine the number of homes that lost power, we will link (spatially join) these areas with OpenStreetMap data on buildings and roads. By linking our analysis with data from the US Census Bureau we can investigate the potential socioeconomic factors that influenced recovery.\n\n\nThe following code is a walk through of how I developed the three visualizations included in the infographic created in Canva above. I would like to note that this is an early rendition of this infographic. I plan on expanding this personal project to investigate the water management district boundaries and evaluate the different types policies in place, or lack there of, that could be potential drivers of lower nutrient levels in estuaries.\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                            establish enva                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# set default chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggtext)\nlibrary(raster)\nlibrary(leaflet)\nlibrary(cowplot)\nlibrary(treemap)\nlibrary(showtext)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(sunburstR)\nlibrary(tmaptools)\n\n\n\n\n\n\n\nWe’ll be using NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reading in the night light tif files\nVIIRS_05_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\nVIIRS_06_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\nVIIRS_05_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\nVIIRS_06_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\n\n\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL query to aid in the data loading process\nquery &lt;- \"SELECT* FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# Loading in the data with the SQL query\nhighways &lt;- st_read(\"../data/houston/gis_osm_roads_free_1.gpkg\",\n                    query = query,\n                    quiet = TRUE)\n\n# Re-projecting CRS\nhighways &lt;- highways %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid(highways)\n\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL Query to aid in data loading process\nbuilding_query &lt;- \"SELECT*\nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# Loading in the building data with the query\nbuildings &lt;- st_read(\"../data/houston/gis_osm_buildings_a_free_1.gpkg\",\n                     query = building_query,\n                     quiet = TRUE)\n\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\nUsing st_read() to load the geodatabase layers and st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. Geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer, income data is stored in the X19_INCOME layer, and the median income field B19013e1.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Loading geometry data\nsocioeconomic &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\",\n                         quiet = TRUE)  \n\n# Loading income data and renaming the columns\nincome &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\",\n                  quiet = TRUE) %&gt;%\n  dplyr::select(GEOID, B19013e1) %&gt;% \n  rename(GEOID_Data = GEOID,\n         median_income = B19013e1)\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          combine tiles          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining tiles for each date as a stars object\nfeb_07 &lt;- st_mosaic(VIIRS_05_07, VIIRS_06_07)\nfeb_16 &lt;- st_mosaic(VIIRS_05_16, VIIRS_06_16)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           make a mask           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a mask to identify locations that experienced a drop greater than 200 nW cm-2sr-1\nblackout_locations &lt;- (feb_07 - feb_16) &gt; 200\n\n# Assigning non-blackout locations to be NA values\nblackout_locations[blackout_locations == FALSE] &lt;- NA\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          vectorize  mask        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Transforming the mask into a vector and fixing improper geometries\nvector_blackout_locations &lt;- st_as_sf(blackout_locations) %&gt;% \n  st_make_valid()\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Defining the Area of Interest\nhouston &lt;- st_polygon(list(rbind(c(-96.5, 29),\n                                 c(-96.5, 30.5),\n                                 c(-94.5, 30.5),\n                                 c(-96.5, 29))))\n\n# Creating a sfc and assigning the CRS to match the vector blackout data\nhouston &lt;- st_sfc(houston) %&gt;% \n  st_set_crs(4326) %&gt;% \n  st_make_valid()\n\n# cropping blackout locations to the houston dimensions\nhouston_blackout_mask &lt;- st_crop(vector_blackout_locations, houston)\n\n# Re-projecting to match the maps\nhouston_blackout_mask &lt;- houston_blackout_mask %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid()\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          create buffer          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a 200 m buffer and dissolving it as a single object\nhighway_buffer &lt;- st_buffer(highways, dist = 200) \n\n# combines the geometries within the buffer as one geometric polygon\nhighway_buffer &lt;- st_union(highway_buffer)\n\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re-projecting the dataset to match map\nbuildings &lt;- buildings %&gt;%\n  st_transform('EPSG:3083')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Filtering for homes beyond the buffer zone affected by the blackouts\nhouston_blackout_homes &lt;- st_join(buildings, houston_blackout_highways,\n                                  .predicate = st_intersects)\n\n# the number of homes impacted was displayed as 487405\n#nrow(houston_blackout_homes)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining the data for area of interest\ncensus_income &lt;- left_join(socioeconomic, income, by = \"GEOID_Data\") %&gt;%\n  st_transform('EPSG:3083') \n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re=projecting CRS\nhouston &lt;- houston %&gt;% \n  st_transform(3083)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Cropping the data \nhouston_income &lt;- st_crop(census_income, houston)\n\n# Census tracts that experienced blackout\nblackout_census &lt;- houston_blackout_homes[houston_income,] %&gt;% \n  mutate(blackout = 'yes')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining Cropped datums\nhouston_blackout_combined &lt;- st_join(houston_income, blackout_census, left = TRUE)\n\n\n\n\n\n\nCreating final combined subsets\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for mapping                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----   identifying unimpacted areas  ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Additional check to track impact of blackout\nhouston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif[is.na(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif)] &lt;- \"FALSE\"\n\n# labels unaffected areas\nhouston_blackout_combined$blackout[is.na(houston_blackout_combined$blackout)] &lt;- \"no\"\n\n# isolates unaffected areas into data subset\nno_blackout &lt;- st_as_sf(houston_blackout_combined[!grepl(\"yes\",houston_blackout_combined$blackout),])\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        categorizing areas       ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Tracking blackouts\ntrac_with_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == TRUE)\n\n# Tracking no blackouts\ntrac_without_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == FALSE)\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        create plot labels                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          violin labels          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nv_title &lt;- \"Median Income Brackets Based on Local Median\"\nv_subtitle &lt;- \"Homes in Houston, TX Affected by Blackouts\"\nv_alt &lt;- \"This is a violin plot of homes impacted by the Houston, TX blackouts (2/21), the median income of the households affected separated into quantiles. A trend of increased blackouts were experienced by individuals earning a median income of less than or equal to 100 thousand dollars.\"\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            map labels           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nm_title &lt;- \"Homes in Houston Affected by Blackouts, Median Income\"\nm_legend &lt;- \"Median Income\"\nm_alt &lt;- \"This is a map of the homes within Houston, TX that experienced a blackout. The census tracts are colored based on median income quantiles. There is a wide spread distribution of areas that were impacted. The greatest regions that experienced a blackout were in the 25th and 50th percentiles.\"\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        plot customizations                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----              fonts              ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# enable {showtext} for rendering \nshowtext_auto()\n# import fonts \nfont_add_google(name = \"Josefin Sans\", family = \"josefin\")\nfont_add_google(name = \"Sen\", family = \"sen\")\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                      wrangle ploting data                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         subset & group          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# subset data for plotting\nviolin_data &lt;- trac_with_blackout %&gt;% \n  select(median_income, Shape) %&gt;% \n  na.omit(median_income) %&gt;% \n  mutate(\n    earning_group = case_when(\n      median_income &lt;= 50000  ~ \"Low-income\",\n      median_income &gt; 50000 & median_income &lt;= 75000  ~ \"Middle-income\",\n      median_income &gt; 75000 & median_income &lt;= 100000 ~ \"Upper-middle-income\",\n      median_income &gt; 100000 & median_income &lt;= 150000 ~ \"Upper-income\",\n      median_income &gt; 150000 & median_income &lt;= 200000 ~ \"Exceptionally High-income\"\n    )\n  ) %&gt;% \n  na.omit(earning_group)\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         factor relevel          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reorder factor levels from low to exceptionally high\nviolin_data$earning_group &lt;- fct_relevel(violin_data$earning_group, \n                                         \"Low-income\", \"Middle-income\", \n                                         \"Upper-middle-income\", \"Upper-income\",\n                                         \"Exceptionally High-income\")\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          violin  plot                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nggplot(violin_data, aes(x = rev(factor(earning_group)), y = median_income)) + \n  \n  geom_violin(aes(fill = factor(rev(earning_group)))) +  \n  geom_boxplot(width = 0.2,\n               size = 0.3,\n               alpha = 0.5) +\n  scale_fill_viridis_d(option = \"magma\", name = \"Median Income Percentile Group\", direction = -1) +\n  \n  labs(\n    x = \"Percentile Group\",\n    y = \"Median Income ($)\",\n    title = v_title,\n    subtitle = v_subtitle,\n    alt = v_alt\n  ) +\n  \n  scale_y_continuous(labels = scales::dollar_format()) +\n  \n  theme_classic() +\n  \n  theme(\n    plot.title = element_markdown(family = \"josefin\",\n                              face = \"bold\",\n                              size = 20,\n                              hjust = 0.5,\n                              vjust = 65),\n    \n     plot.subtitle = element_text(family = \"sen\",\n                                 size = 15,\n                                 hjust = 0.5),\n    \n    \n    axis.text.x = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 12,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.text.y = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 13,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.title.y = element_blank(),\n    \n    axis.title.x = element_blank(),\n    \n    legend.position = \"none\",\n    \n    # legend text customs \n    legend.text = element_text(family = \"josefin\",\n                               size = 12),\n    \n    # match my website colors\n    plot.background = element_rect(color = '#FDFBF7',\n                                   fill = '#FDFBF7'),\n    \n    panel.background = element_rect(color = '#FDFBF7',\n                                    fill = '#FDFBF7'),\n    # space on the side of the plot\n    plot.margin = margin(t = 1, r = 2, b = 1, l = 1, \"cm\")\n    \n    ) +\n  \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          map of houston                              ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          impacted areas         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntmap_mode('plot')\n#census tracts with blackouts\ntm_shape(violin_data) + \n  \n  tm_basemap(leaflet::providers$OpenStreetMap) + \n  \n  tm_polygons(fill = \"earning_group\", \n              title = \"Median Income ($)\",\n              palette = \"magma\",\n              alpha = 0.5) +\n  \n  tm_scale_bar(position = c('left','bottom')) +\n  \n  tm_layout(\n    \n    title = m_title,\n    title.size = 19,\n    title.color =  \"#293F2C\",\n    \n   # alt.text = m_alt,\n    \n    # match my website colors\n    bg.color = \"#FDFBF7\",\n    outer.bg.color = \"#FDFBF7\"\n    \n  )"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#shedding-light-on-the-disproportionate-challenges-residents-faced-particularly-in-relation-to-median-income-levels",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#shedding-light-on-the-disproportionate-challenges-residents-faced-particularly-in-relation-to-median-income-levels",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Throughout the month of February in 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives. For this project, we will estimate the number of homes in Houston that lost power as a result of the first two storms and investigate if socioeconomic factors are predictors of communities recovery from a power outage. Our analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power. To determine the number of homes that lost power, we will link (spatially join) these areas with OpenStreetMap data on buildings and roads. By linking our analysis with data from the US Census Bureau we can investigate the potential socioeconomic factors that influenced recovery.\n\n\nThe following code is a walk through of how I developed the three visualizations included in the infographic created in Canva above. I would like to note that this is an early rendition of this infographic. I plan on expanding this personal project to investigate the water management district boundaries and evaluate the different types policies in place, or lack there of, that could be potential drivers of lower nutrient levels in estuaries."
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#set-up",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#set-up",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                            establish enva                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# set default chunk options\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggtext)\nlibrary(raster)\nlibrary(leaflet)\nlibrary(cowplot)\nlibrary(treemap)\nlibrary(showtext)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(sunburstR)\nlibrary(tmaptools)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#load-data",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#load-data",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "We’ll be using NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reading in the night light tif files\nVIIRS_05_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\nVIIRS_06_07 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\nVIIRS_05_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\nVIIRS_06_16 &lt;- read_stars(\"../data/houston/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\n\n\n\n\n\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL query to aid in the data loading process\nquery &lt;- \"SELECT* FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# Loading in the data with the SQL query\nhighways &lt;- st_read(\"../data/houston/gis_osm_roads_free_1.gpkg\",\n                    query = query,\n                    quiet = TRUE)\n\n# Re-projecting CRS\nhighways &lt;- highways %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid(highways)\n\n\n\n\n\nWe can also obtain building data from OpenStreetMap. We downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SQL Query to aid in data loading process\nbuilding_query &lt;- \"SELECT*\nFROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# Loading in the building data with the query\nbuildings &lt;- st_read(\"../data/houston/gis_osm_buildings_a_free_1.gpkg\",\n                     query = building_query,\n                     quiet = TRUE)\n\n\n\n\n\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\nUsing st_read() to load the geodatabase layers and st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata. Geometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer, income data is stored in the X19_INCOME layer, and the median income field B19013e1.\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Loading geometry data\nsocioeconomic &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\",\n                         quiet = TRUE)  \n\n# Loading income data and renaming the columns\nincome &lt;- st_read(\"../data/houston/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\",\n                  quiet = TRUE) %&gt;%\n  dplyr::select(GEOID, B19013e1) %&gt;% \n  rename(GEOID_Data = GEOID,\n         median_income = B19013e1)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#wrangling-subsetting",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#wrangling-subsetting",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          combine tiles          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining tiles for each date as a stars object\nfeb_07 &lt;- st_mosaic(VIIRS_05_07, VIIRS_06_07)\nfeb_16 &lt;- st_mosaic(VIIRS_05_16, VIIRS_06_16)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           make a mask           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a mask to identify locations that experienced a drop greater than 200 nW cm-2sr-1\nblackout_locations &lt;- (feb_07 - feb_16) &gt; 200\n\n# Assigning non-blackout locations to be NA values\nblackout_locations[blackout_locations == FALSE] &lt;- NA\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          vectorize  mask        ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Transforming the mask into a vector and fixing improper geometries\nvector_blackout_locations &lt;- st_as_sf(blackout_locations) %&gt;% \n  st_make_valid()\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Defining the Area of Interest\nhouston &lt;- st_polygon(list(rbind(c(-96.5, 29),\n                                 c(-96.5, 30.5),\n                                 c(-94.5, 30.5),\n                                 c(-96.5, 29))))\n\n# Creating a sfc and assigning the CRS to match the vector blackout data\nhouston &lt;- st_sfc(houston) %&gt;% \n  st_set_crs(4326) %&gt;% \n  st_make_valid()\n\n# cropping blackout locations to the houston dimensions\nhouston_blackout_mask &lt;- st_crop(vector_blackout_locations, houston)\n\n# Re-projecting to match the maps\nhouston_blackout_mask &lt;- houston_blackout_mask %&gt;% \n  st_transform(3083) %&gt;% \n  st_make_valid()\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          create buffer          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Creating a 200 m buffer and dissolving it as a single object\nhighway_buffer &lt;- st_buffer(highways, dist = 200) \n\n# combines the geometries within the buffer as one geometric polygon\nhighway_buffer &lt;- st_union(highway_buffer)\n\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re-projecting the dataset to match map\nbuildings &lt;- buildings %&gt;%\n  st_transform('EPSG:3083')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Filtering for homes beyond the buffer zone affected by the blackouts\nhouston_blackout_homes &lt;- st_join(buildings, houston_blackout_highways,\n                                  .predicate = st_intersects)\n\n# the number of homes impacted was displayed as 487405\n#nrow(houston_blackout_homes)\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                              wrangle data                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining the data for area of interest\ncensus_income &lt;- left_join(socioeconomic, income, by = \"GEOID_Data\") %&gt;%\n  st_transform('EPSG:3083') \n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          CRS Correction         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Re=projecting CRS\nhouston &lt;- houston %&gt;% \n  st_transform(3083)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            crop for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Cropping the data \nhouston_income &lt;- st_crop(census_income, houston)\n\n# Census tracts that experienced blackout\nblackout_census &lt;- houston_blackout_homes[houston_income,] %&gt;% \n  mutate(blackout = 'yes')\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           combo for AOI         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining Cropped datums\nhouston_blackout_combined &lt;- st_join(houston_income, blackout_census, left = TRUE)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#affected-areas",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#affected-areas",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Creating final combined subsets\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                           data for mapping                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----      isolate affected areas     ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# identifying homes boardering the buffer and beyond the buffer\nhouston_blackout_highways &lt;- st_difference(houston_blackout_mask, highway_buffer)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----   identifying unimpacted areas  ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Additional check to track impact of blackout\nhouston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif[is.na(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif)] &lt;- \"FALSE\"\n\n# labels unaffected areas\nhouston_blackout_combined$blackout[is.na(houston_blackout_combined$blackout)] &lt;- \"no\"\n\n# isolates unaffected areas into data subset\nno_blackout &lt;- st_as_sf(houston_blackout_combined[!grepl(\"yes\",houston_blackout_combined$blackout),])\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        categorizing areas       ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Tracking blackouts\ntrac_with_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == TRUE)\n\n# Tracking no blackouts\ntrac_without_blackout &lt;- houston_blackout_combined%&gt;%\n  filter(houston_blackout_combined$VNP46A1.A2021038.h08v05.001.2021039064328.tif == FALSE)"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#customs",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#customs",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        create plot labels                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          violin labels          ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nv_title &lt;- \"Median Income Brackets Based on Local Median\"\nv_subtitle &lt;- \"Homes in Houston, TX Affected by Blackouts\"\nv_alt &lt;- \"This is a violin plot of homes impacted by the Houston, TX blackouts (2/21), the median income of the households affected separated into quantiles. A trend of increased blackouts were experienced by individuals earning a median income of less than or equal to 100 thousand dollars.\"\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----            map labels           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nm_title &lt;- \"Homes in Houston Affected by Blackouts, Median Income\"\nm_legend &lt;- \"Median Income\"\nm_alt &lt;- \"This is a map of the homes within Houston, TX that experienced a blackout. The census tracts are colored based on median income quantiles. There is a wide spread distribution of areas that were impacted. The greatest regions that experienced a blackout were in the 25th and 50th percentiles.\"\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                        plot customizations                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----              fonts              ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# enable {showtext} for rendering \nshowtext_auto()\n# import fonts \nfont_add_google(name = \"Josefin Sans\", family = \"josefin\")\nfont_add_google(name = \"Sen\", family = \"sen\")"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#visuals",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#visuals",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "",
    "text": "Code\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                      wrangle ploting data                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         subset & group          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# subset data for plotting\nviolin_data &lt;- trac_with_blackout %&gt;% \n  select(median_income, Shape) %&gt;% \n  na.omit(median_income) %&gt;% \n  mutate(\n    earning_group = case_when(\n      median_income &lt;= 50000  ~ \"Low-income\",\n      median_income &gt; 50000 & median_income &lt;= 75000  ~ \"Middle-income\",\n      median_income &gt; 75000 & median_income &lt;= 100000 ~ \"Upper-middle-income\",\n      median_income &gt; 100000 & median_income &lt;= 150000 ~ \"Upper-income\",\n      median_income &gt; 150000 & median_income &lt;= 200000 ~ \"Exceptionally High-income\"\n    )\n  ) %&gt;% \n  na.omit(earning_group)\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----         factor relevel          ----  \n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reorder factor levels from low to exceptionally high\nviolin_data$earning_group &lt;- fct_relevel(violin_data$earning_group, \n                                         \"Low-income\", \"Middle-income\", \n                                         \"Upper-middle-income\", \"Upper-income\",\n                                         \"Exceptionally High-income\")\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          violin  plot                                ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nggplot(violin_data, aes(x = rev(factor(earning_group)), y = median_income)) + \n  \n  geom_violin(aes(fill = factor(rev(earning_group)))) +  \n  geom_boxplot(width = 0.2,\n               size = 0.3,\n               alpha = 0.5) +\n  scale_fill_viridis_d(option = \"magma\", name = \"Median Income Percentile Group\", direction = -1) +\n  \n  labs(\n    x = \"Percentile Group\",\n    y = \"Median Income ($)\",\n    title = v_title,\n    subtitle = v_subtitle,\n    alt = v_alt\n  ) +\n  \n  scale_y_continuous(labels = scales::dollar_format()) +\n  \n  theme_classic() +\n  \n  theme(\n    plot.title = element_markdown(family = \"josefin\",\n                              face = \"bold\",\n                              size = 20,\n                              hjust = 0.5,\n                              vjust = 65),\n    \n     plot.subtitle = element_text(family = \"sen\",\n                                 size = 15,\n                                 hjust = 0.5),\n    \n    \n    axis.text.x = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 12,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.text.y = element_text(family = \"josefin\",\n                               face = \"bold\",\n                               size = 13,\n                               angle = 0,\n                               vjust = 0.8),\n    \n    axis.title.y = element_blank(),\n    \n    axis.title.x = element_blank(),\n    \n    legend.position = \"none\",\n    \n    # legend text customs \n    legend.text = element_text(family = \"josefin\",\n                               size = 12),\n    \n    # match my website colors\n    plot.background = element_rect(color = '#FDFBF7',\n                                   fill = '#FDFBF7'),\n    \n    panel.background = element_rect(color = '#FDFBF7',\n                                    fill = '#FDFBF7'),\n    # space on the side of the plot\n    plot.margin = margin(t = 1, r = 2, b = 1, l = 1, \"cm\")\n    \n    ) +\n  \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----                          map of houston                              ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----          impacted areas         ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntmap_mode('plot')\n#census tracts with blackouts\ntm_shape(violin_data) + \n  \n  tm_basemap(leaflet::providers$OpenStreetMap) + \n  \n  tm_polygons(fill = \"earning_group\", \n              title = \"Median Income ($)\",\n              palette = \"magma\",\n              alpha = 0.5) +\n  \n  tm_scale_bar(position = c('left','bottom')) +\n  \n  tm_layout(\n    \n    title = m_title,\n    title.size = 19,\n    title.color =  \"#293F2C\",\n    \n   # alt.text = m_alt,\n    \n    # match my website colors\n    bg.color = \"#FDFBF7\",\n    outer.bg.color = \"#FDFBF7\"\n    \n  )"
  },
  {
    "objectID": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#footnotes",
    "href": "posts/2023-12-14-HoustonBlackout/Houston_Blackout.html#footnotes",
    "title": "Assessing the Residential Impact of February 2021 Blackouts in Houston, Texas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Professionally\nI am seeking a full-time position at a forward-thinking organization where I can make meaningful contributions and further develop my skills as a data professional. I carry a hefty toolbox of analytical techniques applicable to a broad scope of tasks. I received my Master of Environmental Data Science (MEDS) degree at the Bren School of Environmental Science & Management (June 2024). I am also an ACS certified chemist, I obtained my degree in Chemistry from the University of California, San Diego (June 2023). I am passionate about continuous learning and growth. Eager to explore new technologies and methodologies to stay ahead of the curve in this rapidly evolving field.\nAs a Data Scientist – Graduate Research Assistant at The 2035 Initiative, I’ve had the opportunity to work on a variety of projects geared towards the sustainable transition to decarbonization. The majority of my projects are people-focused. I aid in gathering the public’s perspective on climate related policies and leverage collected data to help drive informed action. In my role, I employ various geospatial techniques to develop workflows for surveying projects. I analyze and interpret Qualtrics data by generating statistical reports, as well as maps, and graphs using R and Python. I’ve debugged workflows and provided descriptive feedback on developmental changes for data mining projects. I foster collaboration among interdisciplinary teams to create unique solutions and deliverables.\n\n\n\n\n\n\n\n\nMEDS Class of 2024 the day of our Capstone Public Presentation! Date: May 2024.\n\nI aspire to work with an organization that shares my dedication to environmental stewardship and social equity. My goal is to amplify the voices of vulnerable communities, ensuring they are heard and considered when writing climate change policies and making actionable decisions. By adopting a grassroots approach, I believe we can create a more just and sustainable world for future generations. Together, we can turn our collective vision into reality, forging a path toward a brighter, more equitable future for all.\n\n\nBehind the Screen\nIn case you were curious about my name pronunciation, (“Sof-aya” like papaya), it’s actually a piece of my family’s history. See, my mom’s ancestors immigrated from Trondheim, Norway and carried a tradition of namesakes. Unlike the typical Jr., Sr., The III, our family skips every other generation. For example, I’m named after my beloved grandmother, Sofia Anne. This tradition’s been in our family for eight generations! Growing up, I didn’t always love having a unique pronunciation, but it’s given me the confidence to speak up and politely correct someone. I love my name, the strength it gives me, and my lineage tied to it.\n\n\n\n\n\n\n\n\nThis is my lovely grandmother, whom I was named after, Sofia Anne. In this photo, we were out enjoying brunch at this tasty place called Madison on Park in San Diego. Date: June 2023.\n\nNow a little bit about me! My favorite way to spend my free time is dancing, trying out new recipes, yoga, and organizing get-togethers. I thoroughly enjoy every aspect of planning a gathering. From visualizing the event, creating task lists, and sending out invitations, to coordinating all the necessary elements to create meaningful memories. It brings me delight seeing it all come together in full fruition.\n\n\n\n\n\n\n\n\n\n\n\n\nOur zumba crew takes on ecstatic dance in Santa Barbara! Date: April 2024.\n\n\n\n\n\n\n\n\n\n\n\n\nBeach day meets craft time with the council (aka our nickname for our friend group lol) in Malibu. Date: July 2022.\n\n\nAs of recently, I’m in a post graduation limbo and only working part-time. Therefore, I am seizing the opportunity to explore emerging sustainable energy technologies and brain storming a pipeline to a solar-punk future."
  },
  {
    "objectID": "index.html#howdy-there-im-sofia-it-rhymes-with-papaya-sof-aya",
    "href": "index.html#howdy-there-im-sofia-it-rhymes-with-papaya-sof-aya",
    "title": "Sofia Ingersoll",
    "section": "",
    "text": "People know me as a carbon crusader, an environmental data scientist, and a certified chemist (inside and outside of the kitchen). I utilize data science to amplify the voices of vulnerable communities to drive sustainable climate-solutions paired with actionable policy suggestions.\nCurrently, I’m at The 2035 Initiative researching global climate adaptation and resiliency in disadvantaged communities and aiding in the development of an equitable clean energy grid model for CA. I employ techniques like geospatial analysis, and statistics to quantify and visualize diverse sets of empirical research and environmental data to support transformational policy change.\nI am a continuous learner. I actively seek to satiate my curiosity for emerging energy technologies, as well as, strategies to tackle GHG emissions and the carbon economy.\nFeel free to explore my website and learn more about me and a selection of my projects. If my content resonates with you, let’s connect and discuss how I can bring value to your team!"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Sofia Ingersoll",
    "section": "Experience",
    "text": "Experience\n\nGIS & Remote Sensing Data Analyst | The 2035 Initiative, Santa Barbara, CA | Oct 2023 - Present\n\n\nCommunications Manager & Lead Software Developer | Climate and Global Dynamics Lab National Center for Atmospheric Research (CGDL NCAR), Santa Barbara, CA | Jan 2024 - Jun 2024\n\n\nData Analyst | UCSD Department of Analytical and Atmospheric Chemistry | Dec 2021 - Jun 2023\n\n\nInvestment Specialist | Keller Williams Realty, Inc. Partnered with Big H Homes | Jul 2019 - Aug 2020"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sofia Ingersoll",
    "section": "Education",
    "text": "Education\n\nBren School of Environmental Science & Management, University of California Santa Barbara | Master of Environmental Data Science | (June 2024)\n\n\nUniversity of California San Diego – Earl Warren College | La Jolla, CA | BS in Chemistry | (June 2023)\n\n\nMoorpark College | Moorpark, CA | AS in Mathematics, Physics, Natural Sciences, and Chemistry | (May 2021)"
  },
  {
    "objectID": "posts/2023-12-14_SDS_Wind_Infrastructure/SDS_Wind_Infrastructure.html",
    "href": "posts/2023-12-14_SDS_Wind_Infrastructure/SDS_Wind_Infrastructure.html",
    "title": "Spatially Distorted Signalling: How Opinions Against Wind Infrastructure Delay Our Transition to Renewable Energy",
    "section": "",
    "text": "Spatially Distorted Signaling: How Opinions Against Wind Infrastructure Delay Our Transition to Renewable Energy\n\n🗃️GitHub Repo: https://github.com/saingersoll/Spatially-Distorted-Signaling-US-Wind-Infrastructure\n\n\nMain Takeaways\n\nPopulation Density\nEffect: A unit increase in population density is associated with a slight increase in the odds of having an operational wind plant. This suggests that areas with higher population densities are marginally more likely to host wind plants and less likely to experience minority holder opinions taking the majority.\nMedian Income\nEffect: An increase in median income is linked to a decrease in the odds of having an operational wind plant. Higher income areas show a lower likelihood of wind plant activity, potentially due to different local priorities or economic factors. Uneven socio-economic power-dynamics could lead to minority opinion holders preventing the development of wind power infrastructure, alongside other renewable energy solutions.\nAnti-Wind Infrastructure Opinion\nEffect: Areas with higher opposition to wind infrastructure are less likely to have operational wind plants. This aligns with expectations that local opposition impacts the establishment of wind plants.\n\n\n\n\nOverview\nThe phenomenon of Spatial Distorted Signalling (SDS) describes the mobilization of minority opinion holders pushing back electorally to promote legislation that aligns with their beliefs. Leah Stokes (et.al) has explored the SDS phenomenon as a natural experiment in her piece, Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy (2016). The findings in this paper describe that rural Canadian communities had a greater ability to mobilize and organize political push back against majority chair holders in parliament after the passing of legislation which invited the development of wind infrastructure through incentives.\nSince then, Leah has navigated the nuances of varying percentiles in races, political affiliation (particularly the % precinct gop voting share), the scale and size of the project, as well as, the volume of local mobilization in her research, Replication Data for: Prevalence and predictors of wind energy opposition in North America (2023). \nI am hoping to reproduce these naturally observed outcomes with US wind plant data and assess how the relationship of population density, income, and opinions against wind power effect the activitity of wind plants. Analysis of these relationships could provide insight into understanding the scaling effect that local resistance has on spatially distorted signalling in relation to wind infrastructure projects and and sustainable climate policy.\n\n\nTechniques Applied\n\nSingle & Multivariate Logit Regression Models\nLogit & Log Odds\nPredictive Probability\nEthical Critiques: Addressing Limitations\n\n\nLimitations\n\nConsidering Omitted Variable Bias (OVB)\nNeglecting additional variables without testing is improper practice. Exogeneity is a very difficult OLS assumption to uphold – alongside a normal distribution of the error mean. A means to determine relationships between variables is running various linear regression models and comparing the \\(R^2\\) value. In this project, we focus our attention on the why OLS was not the analysis method of choice for our relationships of interest. In the instance of logistic regression, a log odds ratio must be taken to interpret each individual variable. There is a strong possibility that the models utilized in this project are not exogenous and require deeper analysis to determine the impact of underlining influences.\n\n\nInsufficient Data\nThe data set may not fully capture all relevant factors affecting wind plant activity, such as specific local policies or environmental conditions.\n\n\n\n\nU.S. Wind Power Plant Data\n\nReplication Data for: Prevalence and predictors of wind energy opposition in North America\n\nThe data source that was utilized in this project, US Wind Data, focuses on the public stance on wind infrastructure for census tract regions within a 3 km buffer zone of a wind infrastructure project. It contains categorical variables, binary variables, continuous socioeconomic factors such as % of races, % precinct political gop affiliated voting share, mobilization tactics, and more. This data is associated with the Replication Data for: Prevalence and predictors of wind energy opposition in North America, doi Harvard Dataverse, V1, 2023. The collaborators on that project include: Stokes, Leah; Franzblau, Emma; Lovering, Jessica R.; Miljanich, Chris. Leah Stokes and her awesome team gathered all of this information from American Wind Association (“awea”), operational, Columbia Sabin Center (“Columbia”).\n\nVariables of Interest:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nstatus\nDescribes the project operating status. In this study, we have converted it into a binary variable: 1 is operating, 0 is not_operating.\n\n\npop_den\nTract-level 2010 census data for population density (per mi^2)\n\n\nmed_inc\nTract-level 2010 census data for median income ($)\n\n\nis_anti_wind\nBinary measure of wind opposition: 1 is against wind power developments, 0 is pro wind power developments.\n\n\n\n\n\nData Citation\n{1. Stokes, Leah; Franzblau, Emma; Lovering, Jessica R.; Miljanich, Chris. \"Replication Data for: Prevalence and predictors of wind energy opposition in North America\", https://doi.org/10.7910/DVN/LE2V0R, Harvard Dataverse, V1, 2023.}\n\n\n\nLoading Libraries\nThe following libraries were selected based on their functionality and ability to optimize our data for mapping.\n\n\nCode\n# Loading Libraries\nlibrary(tidyverse)        # essential r package \nlibrary(sf)               # package simplifies spatial dataframes\nlibrary(tmap)\nlibrary(terra)\nlibrary(broom)\nlibrary(stars)\nlibrary(sjPlot)\nlibrary(naniar)\nlibrary(cowplot)\nlibrary(maptiles)       \nlibrary(ggthemes)\nlibrary(ggspatial)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nset.seed(99)\nknitr::opts_chunk$set(echo = T, warning = F, message = F)\n\n\n\n\nRead & Wrangle Data\nConverting Vector into Raster Data for Mapping\nBelow we will use the package sf to convert the lat/long vector data into a raster geometry column. In this single line, we will also be assigning the CRS EPSG:4326 to the sf data frame. Coordinate Reference Systems, CRS, are required in order for the data to be projected onto a map. The CRS was selected because it provides a relatively proportionate display of the United States. We are open to suggestions regarding our CRS if a different project better fits our data.\n\nU.S. Wind Data\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Read & Raster      ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# reading in & storing data\nwind_data &lt;- read.csv(\"../data/wind_data/wind_data_usa.csv\") \n\n# Confirm the Data Loaded Properly\n#head(wind_data)                  # displays the first 6 rows of the data\n\n# Let's read in our data\nwind_sf &lt;- wind_data %&gt;%       \n  # creates geometry column with desired crs \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) \n                                     \n# quick CRS check\n#glimpse(crs(wind_sf))                  # output should reveal WGS84, EPSG:4326\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Check Point!       ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Let's stop and see if our outputs are what we expect.\n# Were the lat/long columns correctly converted into a geometry column?\n# setdiff() is a way to quickly determine the differences between two data sets.\n\n# Sweet! we are looking good\n#setdiff(colnames(wind_sf), colnames(wind_data))\n\n\n\n\nMapping Wind Power Infrastructure Plants in the U.S.\nBefore diving in, let’s get a sense of where we’ll be investigating. Using `ggplot()`, we can visualize the locations of wind infrastructure power plants throughout the United States. To achieve a more granular map, we’ll need to utilize another data set to create a base layer for our map in order to observe these wind plants with respect to state and county jurisdictions.\n\n\nCode\n# First visual of the U.S. wind data provided by the geometry points\nwind_plants &lt;- ggplot(wind_sf) +\n  annotation_map_tile(type = \"osm\") +\n  geom_sf(col = 'darkgreen',\n          alpha = 0.5,\n          size = 3)\n\nwind_plants\n\n\n\n\n\n\n\n\nDetermining the Process\nWe will employ a series of models to describe the effect of census tract level population density on the operating status of wind power infrastructure. A combination of binary and interaction logit regression will be considered.\nThe initial model will apply OLS regression, this is really a formality to demonstrate why OLS is not the correct approach for interpreting our relationships of interest. The following will be a model with two continuous variables\n\nRegression Model Components\nBinary Indicator Variable will be status column: opertating is 1, and not_operating will be 0.\nThese variables focus more on regionally dependent factors that intuitively seem to have an impact on mobilization variables that we don’t have time to cover in this project. We’ll be working with a mix of discrete and continuous data, so there some wrangling will be necessary to run the regressions we’re interested in.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ---- Inspect & Standarize Data ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Determining Variable Assignments for OLS\n#unique(wind_sf$status)     # displays unique values in this\n\n# Need to rename status output variables\n# creating two categories: operating & not_operating\n# We are removing 'Operating | Decommissioned' because it skews the data\nunwanted_status &lt;- \"Operating | Decommissioned\"\nreplacement_status &lt;- \"Uncertain Status\"\nwind_sf$status[wind_sf$status== unwanted_status]&lt;-\"Uncertain Status\"  \n\n# were we successful ?\n#unique(wind_sf$status)     # displays unique values in this\n\n# cleaning out NAs for OLS\nwind_sf &lt;- wind_sf %&gt;%\n  filter(is.na(status) == 'FALSE') %&gt;% \n  filter(is.na(is_anti_wind) == 'FALSE') %&gt;% \n  filter(is.na(pop_den) == 'FALSE') %&gt;% \n  filter(is.na(med_inc) == 'FALSE') %&gt;% \n  filter(is.na(median_age) == 'FALSE') %&gt;% \n  filter(is.na(n_turbs) == 'FALSE')\n\n# were we successful ?\n#unique(wind_sf$status)     # displays unique values in this\n\n# if_else preserves the data type but replaces unwanted values\nwind_us &lt;- wind_sf %&gt;% \n  mutate(status = if_else(\n    status %in% c('Cancelled', 'Out of service (temporarily)', 'Standby', 'Decommissioned', 'Uncertain Status'), 'not_operating',\n    'operating') \n  )\n\n# are our only outputs \"operating\" and \"not_operating\"?\n#print(unique(wind_us$status))\n\n# status as factor and reassigned values\nwind_us &lt;- wind_us %&gt;% \n  mutate(status = case_when(status == \"operating\" ~ 1,\n            status == \"not_operating\" ~ 0))\n\n\n\n\nCheck Point!\nDoes the binary variable contain only 0s or 1s?\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Check point!       ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# are our only outputs 0 or 1?\nprint(unique(wind_us$status))\n\n\n[1] 1 0\n\n\n\n\nVisualizing the Categorical Response Variable Across a Distribution\nBefore jumping into any analysis, it’s important to get a sense of how the data is distributed and if there are any underlying trends or biases. The two visual aids we’re going to create are a violin plot with jitter points (left) and a comparative regression plot using OLS and GLM (right). Combining two figures provides us fuller insights into both the general trend and changes in probability of the binary outcome for the population density predictor.\nThe visualizations display the majority of the distribution lies within the actively operating wind infrastructure plants. A trend of inactive plants and lower population density is notable in both figures. Collectively they demonstrate smaller population densities contain more inactive wind infrastructure plants. This could be attributed to with weight of a singular vote in regions with smaller demographics.\nLocal mobilization of minority opinion holders in these regions have a greater availability to push back against policymakers. However, this visual does not encapsulate all of the necessary information required to determine this with full certainty. Our data set has low availability for non-operating infrastructure and as such, in the regression figure on the right these are being treated as outliers.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           Violin Distribution          ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create the violin plot with log scale\ndensity_plot &lt;- ggplot(data = wind_us, \n                       aes(x = factor(status), \n                           y = pop_den, \n                           fill = factor(status))) + \n  \n  geom_violin(alpha = 0.6, color = \"darkblue\") + \n  \n  geom_jitter(col = \"#F84C0B\",\n              width = 0,\n              height = 0.05,\n              alpha = 0.35,\n              size = 4) +\n  \n  labs(title = \"Population Density vs Wind Power Plant Operating Status\",\n       subtitle= \"Logorithimic Distribution\",\n       x = \"Activation Status\",\n       y = expression(\"Population Density (Log Scale, \" ~ mi^-2 ~ \")\")) + \n  \n  \n  # rename x-axis labels for clarity\n  scale_x_discrete(labels = c(\"0\" = \"Inactive\", \"1\" = \"Active\")) +  \n  # Apply logarithmic scale to y-axis\n  scale_y_log10() + \n  \n  theme_538() + \n  \n  scale_fill_manual(values = c(\"skyblue\", \"darkblue\")) +\n  \n  # Adjust title font and alignment\n  theme(plot.title = element_text(size = 40,\n                                  family = \"Georgia\", \n                                  face = \"bold\",\n                                  hjust = .99,\n                                  color =\"#293F2C\"),  \n        \n        # Adjust subtitle font and alignment\n        plot.subtitle = element_text(size = 38,\n                                     family = \"Georgia\",\n                                     color =\"#293F2C\",\n                                     hjust = 0.5), \n        \n        axis.title = element_text(size = 36,\n                                  family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n        axis.text = element_text(size = 34,\n                                 family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n         # Move legend to the bottom\n        legend.position = \"top\", \n        \n        # Remove legend title if not needed\n        legend.title = element_blank(),  \n        \n        # Adjust legend text size\n        legend.text = element_text(size = 34,\n                                   family = \"Georgia\",\n                                   color =\"#293F2C\"), \n        \n         # Background color for legend\n        legend.key = element_rect(fill = \"grey94\", color = \"grey94\"), \n        \n        plot.background = element_rect(fill = \"#FDFBF7\")\n        ) +  \n  \n  coord_flip()\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----        Jitter OLS + GLM                ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Optimized jitter plot with smooth lines\njitter_plot_optimized &lt;- ggplot(data = wind_us, \n                                aes(x = pop_den, \n                                    y = status)) + \n  \n  # assign color to legend \n geom_jitter(aes(color = \"Data Points\"),\n              width = 0,\n              height = 0.05,\n              alpha = 0.6,\n              size = 4) +  # Adjusted size for better visibility\n  \n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              aes(color = \"OLS Line\"),\n              size = 1.2,  # Slightly thicker line for visibility\n              linetype = \"solid\") + \n  \n  geom_smooth(method = \"glm\",\n              se = T,\n              aes(color = \"GLM Line\"),\n              size = 1.2,\n              linetype = \"dashed\",\n              method.args = list(family = \"binomial\")) +\n  \n  labs(title = \"Population Density vs Wind Power Plant Operating Status\",\n      subtitle= \"Logorithimic Distribution Regression Comparison\",\n       y = \"Activation Status\",\n       x = expression(\"Population Density (Log Scale, \" ~ mi^-2 ~ \")\")) + \n  \n  # rename yaxis labels for clarity\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"0\" = \"Inactive\", \"1\" = \"Active\")) + \n  \n  scale_x_log10() + \n  \n  theme_538() +\n  \n  # Adjust title font and alignment\n  theme(plot.title = element_text(size = 40,\n                                  family = \"Georgia\", \n                                  face = \"bold\",\n                                  hjust = .99,\n                                  color =\"#293F2C\"),  \n        \n        # Adjust subtitle font and alignment\n        plot.subtitle = element_text(size = 38,\n                                     family = \"Georgia\",\n                                     hjust = 0.5,\n                                     color =\"#293F2C\"), \n        \n        axis.title = element_text(size = 36,\n                                  family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n        axis.text = element_text(size = 34,\n                                 family = \"Georgia\",\n                                  color =\"#293F2C\"),\n        \n         # Move legend to the bottom\n        legend.position = \"top\", \n        \n        # Remove legend title if not needed\n        legend.title = element_blank(),  \n        \n        # Adjust legend text size\n        legend.text = element_text(size = 34,\n                                   family = \"Georgia\",\n                                  color =\"#293F2C\"), \n        \n         # Background color for legend\n        legend.key = element_rect(fill = \"grey94\", color = \"grey94\"), \n        \n        plot.background = element_rect(fill = \"#FDFBF7\")\n        ) +   \n  \n   scale_color_manual(name = \"Legend\",  # Title for the legend\n                     values = c(\"Data Points\" = \"#F84C0B\", \n                                \"OLS Line\" = \"blue\", \n                                \"GLM Line\" = \"skyblue\"),\n                     labels = c(\"Data Points\" = \"Data Points\", \n                                \"OLS Line\" = \"OLS Line\", \n                                \"GLM Line\" = \"GLM Line\"))\n\n\n# Combine plots horizontally\ncombined_plot &lt;- density_plot + \n                  jitter_plot_optimized + \n  # Arrange plots side-by-side\n                  plot_layout(ncol = 2)  \n\ncombined_plot\n\n\n\n\n\n\n\nPredictions for Binary Indicator Variable\n\nLogistic Model with One Continuous Variable\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density)  +\\varepsilon \\]\nIn Table 2, the intercept has a log-odds of 3.272 is noted a statistically significant value (p-value &lt; 0.01). This value provides insights into the baseline probability of the outcome when all predictors are set to zero. It suggests that the baseline probability of having an active wind plant\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           1 Continuous Variable        ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Initial regression 1 betas for null\n# function\nstatus &lt;- glm(status ~ pop_den,\n                       wind_us,\n                       family = 'binomial')\n      \n# model table\nsjPlot::tab_model(status,\n          transform = NULL,\n           # predictor labels\n          pred.labels = c(\"Intercept\", \"Population Density (mi^-2)\"),\n          # include p-val\n          show.p = TRUE, \n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          dv.labels = c(\"log Probability of Active Wind Power Plant Operating Status\"),\n          string.p = \"p-value\",\n          show.r2 = FALSE,\n          title = \"Table 2: Logit Regression Model Results for Population Density\",\n          digits = 3)\n\n\n\nTable 2: Logit Regression Model Results for Population Density\n\n\n \nlog Probability of Active Wind Power Plant Operating Status\n\n\nPredictors\nLog-Odds\nCI\np-value\n\n\nIntercept\n3.272 ***\n2.968 – 3.605\n&lt;0.001\n\n\nPopulation Density (mi^-2)\n0.000 \n-0.000 – 0.001\n0.593\n\n\nObservations\n1179\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\n\n\nLogistic Model with Two Continuous Variables\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\nIn Table 3, the intercept has a log-odds of 4.028 is noted a statistically significant value (p-value &lt; 0.01). This value provides insights into the baseline probability of the outcome when all predictors are set to zero.\n\n\nCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ----           2 Continuous Variables        ----\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nstatus_2 &lt;- glm(status ~ pop_den + med_inc,\n               wind_us,\n               family = 'binomial')\n# model table\nsjPlot::tab_model(status_2,\n          transform = NULL,\n           # predictor labels\n          pred.labels = c(\"Intercept\", \"Population Density (mi^-2)\", \"Median Income ($)\"),\n          # include p-val\n          show.p = TRUE, \n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          dv.labels = c(\"log Probability of Active Wind Power Plant Operating Status\"),\n          string.p = \"p-value\",\n          show.r2 = FALSE,\n          title = \"Table 3: Logit Regression Model Results for Population Density & Median Income\",\n          digits = 3)\n\n\n\nTable 3: Logit Regression Model Results for Population Density & Median Income\n\n\n \nlog Probability of Active Wind Power Plant Operating Status\n\n\nPredictors\nLog-Odds\nCI\np-value\n\n\nIntercept\n4.028 ***\n3.007 – 4.974\n&lt;0.001\n\n\nPopulation Density (mi^-2)\n0.000 \n-0.000 – 0.001\n0.512\n\n\nMedian Income ($)\n-0.000 *\n-0.000 – 0.000\n0.096\n\n\nObservations\n1179\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\n\n\n\nAnalysis of Log Odds Ratio\nIn order to calculate the baseline probability of having an active wind plant, the following equation will be applied:\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1 + \\beta_2+\\varepsilon \\]\nit is manipulated to solve for the probability, p: \\[p̂ = e^(β0+β1x1+eβ0+β1x)\\]\nTo simplify our workflow, we’re going to solve for p using the uniroot function to find the probability for different values of population density. It allows us to solve for a range of p values using \\(R^2\\).\nWe’re going to inspect the operational probability according to the population density quantile.\n\n\nCode\n# Define a function to calculate probability from logistic regression coefficients\nfind_probability &lt;- function(pop_den, coefficients) {\n  fun &lt;- function(p) {\n    (1 - p) * exp(coefficients[1] + coefficients[2] * pop_den) - p\n  }\n  uniroot(fun, interval = c(0, 1))$root\n}\n\n# Coefficients from the logistic regression model\ncoefficients &lt;- status$coefficients\n\n# What are the pop den quantiles?\n#quantile(wind_data$pop_den)\n\n# Population density quantiles from the data\nquantiles_pop_den &lt;- c(quantile(wind_sf$pop_den))\n\n# Calculate probabilities for each quantile value\nprobabilities &lt;- sapply(quantiles_pop_den, function(d) find_probability(d, coefficients))\n\n# Create a dataframe with the quantile population densities and their corresponding probabilities\npop_den_prob &lt;- data.frame(\n  pop_density = quantiles_pop_den,\n  probability = probabilities\n)\n\n# Print the table using kable\nkable(pop_den_prob, \n      format = \"pipe\", \n      col.names = c(\"Population Density (mi^-2)\", \"Probability\"),\n      caption = \"Table 4: Probability of Active Wind Power Plant Operating Status at Different Population Densities\")\n\n\n\nTable 4: Probability of Active Wind Power Plant Operating Status at Different Population Densities\n\n\n\nPopulation Density (mi^-2)\nProbability\n\n\n\n\n0%\n1.431998e-01\n0.9634634\n\n\n25%\n4.308644e+00\n0.9634896\n\n\n50%\n1.249329e+01\n0.9635412\n\n\n75%\n4.039379e+01\n0.9637166\n\n\n100%\n1.394051e+04\n0.9968966\n\n\n\n\n\n\n\nCode\n# Define the function to calculate probability\nfind_probability_2 &lt;- function(pop_den, med_inc, coefficients) {\n  # Logistic regression function\n  logit &lt;- coefficients_2[\"(Intercept)\"] + coefficients_2[\"pop_den\"] * pop_den + coefficients_2[\"med_inc\"] * med_inc\n  p &lt;- exp(logit) / (1 + exp(logit))\n  return(p)\n}\n\n# Define quantiles for population density and income\nquantiles_pop_den &lt;- c(0, 4.31505, 12.53426, 40.79216, 13940.51491)\nquantiles_med_inc &lt;- c(14962.0, 41424.0, 47750.0, 55405.5, 185769.0)\n\n\n# Coefficients from the logistic regression model\ncoefficients_2 &lt;- status_2$coefficients\n\n# Create a data frame for the plot\nplot_data &lt;- expand.grid(\n  income_group = quantiles_med_inc,\n  pop_den_group = quantiles_pop_den) %&gt;%\n \n   mutate(\n    probability = mapply(\n      \n      function(inc, den) find_probability_2(den, inc, coefficients_2),\n      income_group,\n      pop_den_group\n    ),\n   \n     income_group_label = factor(income_group,\n                                 levels = quantiles_med_inc,\n                                 labels = paste0(\"Income Quantile \", 1:5)),\n    \n    # name change for plotting ease\n    pop_den_group_label = factor(pop_den_group, \n                                 levels = quantiles_pop_den,\n                                 labels = paste0(\"Quantile \",\n                                                 1:5))\n )\n\n\ncolor_palette &lt;- c(\n  \"Quantile 1\" = \"grey80\",  # Very light blue\n  \"Quantile 2\" = \"#D0EAFB\",  # Light blue\n  \"Quantile 3\" = \"#A6C7E1\",  # Medium-light blue\n  \"Quantile 4\" = \"#4A90D3\",  # Medium blue\n  \"Quantile 5\" = \"#003C71\"   # Dark blue\n)\n\n\n# Create the plot\nggplot(plot_data, aes(x = income_group_label,\n                      y = probability,\n                      fill = pop_den_group_label)) +\n  \n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  \n  scale_fill_manual(values = color_palette) +\n  \n  \n  labs(\n    title = \"Probability of Active Wind Power Plant Operating Status\" ,\n    subtitle = paste0(\"Assessing Median Income ($) and Population Density (\", expression(\"mi^-2 \"), \") Quantiles\"),\n    x = \"Income Quantile ($)\",\n    y = \"Probability\",\n    fill = \"Population Density Quantile\"\n  ) +\n  \n  theme_minimal() +\n  \n  theme(\n    \n    text = element_text(family = \"Georgia\",\n                        size = 18),\n    \n    axis.text.x = element_text(size = 10),\n    \n    axis.text.y = element_text(size = 10),\n    \n    axis.title.x = element_text(size = 12),\n    \n    axis.title.y = element_text(size = 12),\n    \n    \n    plot.title = element_text(size = 17,\n                              face = \"bold\",\n                              hjust = .99),\n    \n    plot.subtitle = element_text(size = 15,\n                                 hjust = 1.25),\n    \n    plot.background = element_rect(fill = \"#FDFBF7\"),\n\n    # Adjust legend\n    legend.position = \"top\",\n    # Smaller legend title\n    legend.title = element_text(size = 12),   \n    # Smaller legend text\n    legend.text = element_text(size = 10),    \n    # Smaller legend key (box size)\n    legend.key.size = unit(0.5, \"cm\"),        \n    # Space between legend items\n    legend.spacing.x = unit(0.2, \"cm\"),     \n    # Space between legend items\n    legend.spacing.y = unit(0.2, \"cm\"),      \n    legend.background = element_rect(fill = \"white\",\n                                     color = \"grey100\",\n                                      # Background box\n                                     size = 0.5)\n\n  ) +\n  \n  guides(fill = guide_legend(title = \"Population Density Quantile\", \n                             title.position = \"top\",\n                             title.hjust = 0.5))\n\n\n\n\n\nThe probability distribution of having an active local wind plant across different population density and median income quantiles reveals some notable trends:\n\nPopulation Density: Consistent with expectations, areas with the highest population density show the highest probability of hosting wind plants in this example. This trend aligns with the assumption that regions with more people may have a higher demand for renewable energy sources like wind power. Studies have shown, “urban areas with higher population densities are often more likely to invest in environmental infrastructure, including wind energy projects” (Kahn, 2007, p. 58).\nMedian Income: Interestingly, high-income areas with lower population densities tend to have the lowest likelihood of having an active local wind plant. This observation is intriguing and suggests that higher income alone may decrease the probability of wind power plant activity. A study on socio-economic dynamics conducted by McCright and Dunlap found “socioeconomic status significantly affects individuals’ environmental attitudes and policy preferences” (McCright & Dunlap, 2011, p. 402). Their research suggests that higher-income individuals might have different priorities or less immediate need for such infrastructure compared to lower-income communities.\n\n\nQuestion:\nCould an assumption be made in which areas at risk of spatially distorted signalling have a greater propensity for higher median income and lower population density?\nLet’s touch a bit on the social psychology at potentially at play.\n\nResources and Political Mobilization:\n\nHigher Income Brackets: Addressing this question involves considering the impact of political and economic influence on energy policy. Verba, Schlozman, and Brady argue that “higher-income individuals typically have more resources and time to engage in political activities, which can influence local energy policies” (Verba, Schlozman, & Brady, 1995, p. 234). This might suggest that wealthier areas could exert more influence to prioritize different energy investments or limit the visibility of such projects.\nLower Income Brackets: Conversely, those in lower income brackets may have fewer resources and less time for such activities, potentially resulting in lower levels of wind plant advocacy and adoption.\n\nDonations and Lobbying:\n\nHigh-income individuals or entities may make substantial donations to lobbying groups or political campaigns to promote specific agendas, including energy policies that align with their interests. Hertel and Tsigas highlight that “financial contributions and lobbying play a significant role in shaping policy decisions” (Hertel & Tsigas, 2002, p. 78). This could mean that higher-income areas, with their greater financial resources, might be able to affect decisions on wind plant locations or energy policy. This financial influence can interfere the development and implementation of local energy projects.\n\nInfluence of Socioeconomic Status:\n\nThe presence of wind plants might be influenced by socioeconomic status in ways that reflect broader patterns of power and influence. These insights align with the notion that “economic power and market conditions influence decisions related to energy infrastructure” (Kirschen & Strbac, 2004, p. 112). Therefore, it is plausible that higher-income areas might both contribute to and benefit from a different set of energy policies compared to lower-income areas. Areas with higher incomes might prioritize other forms of energy or infrastructure development based on their resources and political connections.\n\n\nNow that we’ve explored probabilities for our two models in a systematic way, it’s important to have an understanding of these relationships on a continuous scale. To gain a deeper understanding of the logistic regression model, we will leverage the Odds Ratio. We will create a table displaying the Odds Ratio, which quantifies how frequently a binary event occurs relative to its baseline. This approach provides a comprehensive and more efficient way to interpret the relationships between variables in the model.\n\n\n\n\nInterpreting Coefficients Using Odds Ratio\nTo better interpret the relationship in our logistic regression model, we will focus on odds rather than probabilities. Although odds and probabilities are often confused, they are distinct concepts that are related through the following formula:\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\nThe odds of a binary event are the ratio of how often it happens, to how often it doesn’t happen. Here, \\[\\hat{p}​\\]represents the predicted probability, and \\[exp⁡(β^0+β^1⋅x)\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x)exp(β^​0​+β^​1​⋅x) \\] denotes the odds based on our model.\nWe will create an odds_hat variable to represent the predicted odds. The odds ratio for a one-unit increase in the predictor variable, x, is given by:\n\\[\ne^{\\hat{\\beta}_1}\n\\]\nThis ratio indicates how the odds of the outcome change with a one-unit increase in x. Importantly:\n\nInterpretation of Odds Ratio: eβ1e{_1}eβ^​1​ is the factor by which the odds are multiplied for a one-unit increase in the predictor variable x\nIndependence from Predictor Value: This ratio does not depend on the specific value of x; it represents a constant multiplicative effect on the odds regardless of x’s value.\n\nThus, the odds ratio provides a useful and consistent measure of the effect of a predictor variable on the odds of the outcome in logistic regression.\n\nLogistic Model with One Continuous Variable\n\n\nCode\n# Generate predictions and calculate odds\nstatus_popden_predicted_odds &lt;- status %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  mutate(y_hat = .fitted,\n         odds_hat = y_hat / (1 - y_hat)) %&gt;%\n  # Order by odds in descending order\n  arrange(desc(odds_hat)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\n# Create and style the table\nstatus_popden_predicted_odds %&gt;%\n  select(y_hat, odds_hat) %&gt;%\n  mutate(\n    y_hat = scales::percent(y_hat, accuracy = 1),\n    odds_hat = scales::number(odds_hat, accuracy = 1)\n  ) %&gt;%\n  kable(\n    caption = \"Table 5: Top 10 Predicted Odds and Probabilities from Logistic Regression\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nTable 5: Top 10 Predicted Odds and Probabilities from Logistic Regression\n\n\ny_hat\nodds_hat\n\n\n\n\n100%\n321\n\n\n99%\n159\n\n\n99%\n79\n\n\n99%\n77\n\n\n99%\n69\n\n\n99%\n69\n\n\n98%\n63\n\n\n98%\n58\n\n\n98%\n58\n\n\n98%\n57\n\n\n\n\n\n\n\n\nOur model estimates that one unit increase in population density is associated with a change in the odds ratio of \\(e^(0.0002) =1.0002\\), or a 2.0e-04% increase in the odds of wind plant having an operating status.\n\n\nCode\n# Ensure you have the correct model object\n# Summary of the model to check coefficients\nmodel_summary &lt;- summary(status)\n\n# Extract the odds ratio for population density\n# Assuming population density is the second coefficient\nodds_ratio_pop_den &lt;- exp(model_summary$coefficients[\"pop_den\", \"Estimate\"])\n\n# Generate a descriptive text with the rounded odds ratio\npaste0(\"Odds Ratio for Population Density is \", round(odds_ratio_pop_den, 4))\n\n\n[1] \"Odds Ratio for Population Density is 1.0002\"\n\n\nLogistic Model with Two Continuous Variables\n\n\nCode\n# Generate predictions and calculate odds\nstatus_popden_predicted_odds_2 &lt;- status_2 %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  mutate(y_hat = .fitted,\n         odds_hat = y_hat / (1 - y_hat)) %&gt;%\n  # Order by odds in descending order\n  arrange(desc(odds_hat)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\n# Create and style the table\nstatus_popden_predicted_odds_2 %&gt;%\n  select(y_hat, odds_hat) %&gt;%\n  mutate(\n    y_hat = scales::percent(y_hat, accuracy = 1),\n    odds_hat = scales::number(odds_hat, accuracy = 1)\n  ) %&gt;%\n  kable(\n    caption = \"Table 6: Top 10 Predicted Odds and Probabilities from Logistic Regression\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nTable 6: Top 10 Predicted Odds and Probabilities from Logistic Regression\n\n\ny_hat\nodds_hat\n\n\n\n\n100%\n1 103\n\n\n100%\n415\n\n\n99%\n146\n\n\n99%\n119\n\n\n99%\n109\n\n\n99%\n109\n\n\n99%\n102\n\n\n99%\n87\n\n\n99%\n84\n\n\n99%\n81\n\n\n\n\n\n\n\n\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) + \\beta_2  (Median Income) +\\varepsilon \\]\n\n\nCode\n# Ensure you have the correct model object\n# Summary of the model to check coefficients\nmodel_summary_2 &lt;- summary(status_2)\n\n# Extract and calculate the odds ratio for population density\nodds_ratio_pop_den &lt;- exp(model_summary_2$coefficients[\"pop_den\", \"Estimate\"])\n\n# Extract and calculate the odds ratio for median income\n# Note: Since the odds ratio for median income is typically 1 - exp(Estimate), this should be done correctly\nodds_ratio_med_inc &lt;- 1 - exp(model_summary_2$coefficients[\"med_inc\", \"Estimate\"])\n\n# Generate descriptive texts with the rounded odds ratios\npaste0(\"Odds Ratio for Population Density: \", round(odds_ratio_pop_den, 4))\n\n\n[1] \"Odds Ratio for Population Density: 1.0002\"\n\n\nCode\npaste0(\"Odds Ratio for Median Income: \", round(odds_ratio_med_inc, 4))\n\n\n[1] \"Odds Ratio for Median Income: 0\"\n\n\n\n\n\nProbabilistic Predictions\nTo evaluate the probability of having an active wind plant, we use out-of-sample predictions with the type.predict argument set to “response” to obtain fitted values on the probability scale.\nLogistic Model with One Continuous Variable\n\n\nCode\n# probability scale\nprobability_predictions &lt;- augment(status, type.predict = \"response\") \n\n\nprobability_predictions &lt;- probability_predictions %&gt;%\n  # Order by odds in descending order\n  arrange(desc(.fitted)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\nprobability_predictions_table &lt;- probability_predictions %&gt;%\n  mutate(Predicted_Probability = .fitted,\n         Std_Dev = .sigma) %&gt;%\n  select(Predicted_Probability, Std_Dev) %&gt;%\n  mutate(Predicted_Probability = scales::percent(Predicted_Probability, accuracy = 1)) %&gt;%\n  kable(caption = \"Table 7: Probability Predictions from Logistic Regression Model\",\n        format = \"html\",\n        digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"center\",\n                font_size = 12)\n\nprobability_predictions_table\n\n\n\n\nTable 7: Probability Predictions from Logistic Regression Model\n\n\nPredicted_Probability\nStd_Dev\n\n\n\n\n100%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n98%\n0.55\n\n\n\n\n\n\n\n\nLogistic Model with Two Continuous Variables\n\n\nCode\n# probability scale\nprobability_predictions_2 &lt;- augment(status_2, type.predict = \"response\") \n\nprobability_predictions_2 &lt;- probability_predictions_2 %&gt;%\n  # Order by odds in descending order\n  arrange(desc(.fitted)) %&gt;% \n  # Select top 5 rows\n  slice_head(n = 10)            \n\nprobability_predictions_table_2 &lt;- probability_predictions_2 %&gt;%\n  mutate(Predicted_Probability = .fitted,\n         Std_Dev = .sigma) %&gt;%\n  select(Predicted_Probability, Std_Dev) %&gt;%\n  mutate(Predicted_Probability = scales::percent(Predicted_Probability, accuracy = 1)) %&gt;%\n  kable(caption = \"Table 8: Probability Predictions from Logistic Regression Model\",\n        format = \"html\",\n        digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"center\",\n                font_size = 12)\n\nprobability_predictions_table_2\n\n\n\n\nTable 8: Probability Predictions from Logistic Regression Model\n\n\nPredicted_Probability\nStd_Dev\n\n\n\n\n100%\n0.55\n\n\n100%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n99%\n0.55\n\n\n\n\n\n\n\n\nFor example, our bivariate model predicts the odds that an area with a population density of 124 (\\(mi^-2\\)) and a median income of $46,094 and would have an operating wind plant is ~97%.\n\n\nCode\n# New data for prediction\nexploring_model &lt;- data.frame(pop_den = 124, med_inc = 46094)\n\n# Get predictions\npredicted_probability &lt;- augment(status_2, newdata = exploring_model, type.predict = \"response\") %&gt;%\n  select(.fitted) %&gt;%\n  mutate(Predicted_Probability = scales::percent(.fitted, accuracy = 1)) %&gt;%\n  rename(\"Predicted Probability\" = Predicted_Probability) %&gt;%\n  add_column(\n    \"Population Density\" = exploring_model$pop_den,\n    \"Median Income\" = exploring_model$med_inc,\n    .before = 1\n  )\n\n# Display the table\npredicted_probability %&gt;%\n  kable(\n    caption = \"Predicted Probability for Population Density and Median Income Under Specific Conditions\",\n    format = \"html\",\n    digits = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    position = \"center\",\n    font_size = 12\n  )\n\n\n\n\nPredicted Probability for Population Density and Median Income Under Specific Conditions\n\n\nPopulation Density\nMedian Income\n.fitted\nPredicted Probability\n\n\n\n\n124\n46094\n0.97\n97%\n\n\n\n\n\n\n\n\n\n\nComprehensive Interaction Model Containing Binary Predictor Variable\n\nCoefficient and Odds Ratio Table\nTo interpret the model, we compute the odds ratios for each coefficient, providing insight into how each variable and its interactions affect wind plant activity.\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Population Density) * \\beta_2  (Median Income) * \\beta_3  (AntiWindOpinion) +\\varepsilon \\]\n\n\nCode\n# Fit the comprehensive model\ncomprehensive_model &lt;- glm(status ~ pop_den * med_inc * is_anti_wind,\n                           data = wind_us,\n                           family = 'binomial')\n\n#summary(comprehensive_model)\n\n# Compute odds ratios for the model coefficients\n#exp(coef(comprehensive_model))\n\n# Extract coefficients and compute odds ratios\ncoef_summary &lt;- data.frame(\n  Term = names(coef(comprehensive_model)),\n  Estimate = coef(comprehensive_model),\n  Odds_Ratio = exp(coef(comprehensive_model))\n)\n\n# Create and style the table\nkable(coef_summary, format = \"html\", digits = 4, caption = \"Table 10: Summary of Coefficients and Odds Ratios\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), position = \"center\", font_size = 12)\n\n\n\n\nTable 10: Summary of Coefficients and Odds Ratios\n\n\n\nTerm\nEstimate\nOdds_Ratio\n\n\n\n\n(Intercept)\n(Intercept)\n4.1017\n60.4441\n\n\npop_den\npop_den\n0.0011\n1.0011\n\n\nmed_inc\nmed_inc\n0.0000\n1.0000\n\n\nis_anti_wind\nis_anti_wind\n-0.7008\n0.4962\n\n\npop_den:med_inc\npop_den:med_inc\n0.0000\n1.0000\n\n\npop_den:is_anti_wind\npop_den:is_anti_wind\n-0.0016\n0.9984\n\n\nmed_inc:is_anti_wind\nmed_inc:is_anti_wind\n0.0000\n1.0000\n\n\npop_den:med_inc:is_anti_wind\npop_den:med_inc:is_anti_wind\n0.0000\n1.0000\n\n\n\n\n\n\n\n\nThis table provides a comprehensive view of how each variable and their interactions contribute to the likelihood of wind plant activity, facilitating a better understanding of the model’s results.\n\nMain Effects: The odds ratios for pop_den, med_inc, and is_anti_wind indicate the individual effects of each variable on the likelihood of having an active wind plant.\nInteraction Effects: The interaction terms show how the relationship between each pair of variables influences the odds of wind plant activity. For instance, the interaction between pop_den and med_inc captures how the effect of population density on wind plant status changes with median income.\n\n\n\n\nOur Findings\nOur analysis of wind plant activity using logistic regression models has provided valuable insights into the factors influencing the presence of operational wind plants.\n\nMain Takeaways\nPopulation Density: Effect: A unit increase in population density is associated with a slight increase in the odds of having an operational wind plant. This suggests that areas with higher population densities are marginally more likely to host wind plants.\nMedian Income: Effect: An increase in median income is linked to a decrease in the odds of having an operational wind plant. Higher income areas show a lower likelihood of wind plant activity, potentially due to different local priorities or economic factors.\nAnti-Wind Infrastructure Opinion: Effect: Areas with higher opposition to wind infrastructure are less likely to have operational wind plants. This aligns with expectations that local opposition impacts the establishment of wind plants.\nModel Performance:\nP-Values: The p-values for the majority of the coefficients in our models were above the significance level of 0.05. This indicates that while our models show some trends, the results are not statistically significant enough to make strong conclusions. The lack of significance suggests that there may be insufficient evidence to definitively assess the impact of these variables on wind plant activity.However, the intercept provided a baseline log-odds of having an operational wind plant fell within the range of approximately 60% for observations in this data set. The gaps within the data for areas without operating wind plants result in heavy bias towards operating wind plants.\n\n\nFuture Works Considerations\nIf given the opportunity, I would expand the dataset to include any more possible non-operational wind plants and explore in greater detail how exogenous our variables are and determine which values are likely interacting, to produce the best model fit. By expanding the dataset and refining the model, we can better understand the dynamics influencing the operational status of wind plants and make more informed conclusions. Additionally, I would bolster this analysis by identifying and including relevant variables to address the omitted variables bias present in this statistical exploration.\n\n\n\nCitations\n\nStokes, Leah C., et al. “Prevalence and Predictors of Wind Energy Opposition in North America.” Proceedings of the National Academy of Sciences, vol. 120, no. 40, Sept. 2023, doi:10.1073/pnas.2302313120.\nStoke, L. C. “Front Matter.” American Journal of Political Science, vol. 60, no. 4, 2016. JSTOR, http://www.jstor.org/stable/24877456. Accessed 26 October. 2023\nKahn, M. E. (2007). Green Cities: Urban Growth and the Environment. Brookings Institution Press.\nMcCright, A. M., & Dunlap, R. E. (2011). The Politicization of Climate Change and Polarization in the American Public’s Views of Global Warming. Society & Natural Resources, 24(5), 398-413.\nVerba, S., Schlozman, K. L., & Brady, H. E. (1995). Voice and Equality: Civic Voluntarism in American Politics. Harvard University Press.\nHertel, T. W., & Tsigas, M. E. (2002). The Role of Political Lobbying and Financial Contributions in Policy Making. In Public Policy Analysis (pp. 67-90). Routledge.\nKirschen, D. S., & Strbac, G. (2004). Fundamentals of Power System Economics. Wiley.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ingersoll2023,\n  author = {Ingersoll, Sofia},\n  title = {Spatially {Distorted} {Signalling:} {How} {Opinions}\n    {Against} {Wind} {Infrastructure} {Delay} {Our} {Transition} to\n    {Renewable} {Energy}},\n  date = {2023-12-14},\n  url = {https://saingersoll.github.io/posts/SDS_Wind_Infrastructure},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIngersoll, Sofia. 2023. “Spatially Distorted Signalling: How\nOpinions Against Wind Infrastructure Delay Our Transition to Renewable\nEnergy.” December 14, 2023. https://saingersoll.github.io/posts/SDS_Wind_Infrastructure."
  },
  {
    "objectID": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html",
    "href": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nIn AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. The background of the plot is color categorized according to the AQI’s six categories of concern for ppm levels. A relative trend of moderate is seen throughout the year. However,a spike in air pollutants between the months of December 2017 and January 2018 is clearly observed. The sharp initial peak observed the day the Thomas Fire began (Dec. 4, 2017) caused an inital AQI response within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\nLet’s get a full understanding of exactly how much local area was affected by the Thomas Fire by importing the fire perimeter boundary information & some super cool Landsat 2\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "href": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html#investigation-on-the-thomas-fire-impacts-in-santa-barbara-county-ca-2017---2018",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "",
    "text": "Google earth V 6.2.2.6613. (December 13, 2017). Santa Barbara, United States. 34.6099° N, 120.0665° W, Eye alt 13.72 feet. DigitalGlobe 2020. http://www.earth.google.com [December 12, 2023].\n\n\n\nOn December 4, 2017, the Thomas Fire in Santa Barbara County. began to sweep throughout Ventura and Santa Barbara County, CA. For 40 days, a devistating total of 281,893 acres were consumed; destroying 1,063 structures and claiming two casualties (one civilian and one firefighter). Investigations have found that this wildfire was the result of a “line slap,” shared between Southern California Edison (“SCE”) powerlines during a high wind event that sparked hot materials to ignite a nearby fuel bed (Ventura County Fire Department). As of 2019, SCE agreed to a $360 million settlement to address the conglomorate negative impacts caused by the Thomas Fire, Woolsey Fire, and Koeningstein Fire. As well as, the ripple effect of the Thomas Fire, which was especially felt by community members in January of 2018 when 23 lives were claimed from a debris flow in Montecito (Wildfire Today, California Govenor’s Office of Emergency Services).\n\n\n\nTo get a better understanding of the initial environmental and public health impacts caused by the Thomas Fire, together, we will explore the Air Quality Index (AQI) of SB County between 2017/01 - 2018/10. We’ll quanitfy and visualize the amount of particulate matter seen in the image abouve using both the Daily AQI and the average AQI over a 5 day rolling window in units of ppm. In addition, we will gain insight into what parts of Santa Barbara County were exposed to the Thomas Fire, through the examination of burn scars using false-color imaging on Landsat 8 satellite data from the Microsoft Planetary Computer (“MPC”). We will use a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data.\n\n\n\nIn AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\n\nAQI_Assessment.png\n\n\n\n\n\n\nDirectly accessing & processing MPC STAC data\nRaster analysis applying false color imagery\nTime series analysis\n\n\n\n\n\n\nThe Daily Air Quality Index (AQI) data to quantify the particulate matter released into Santa Barbara County from the fire was collected here from the US Environmental Protection Agency to visualize the rolling AQI averages between 2017 and 2018.\n\n\n\nFor our true and false color imagery, we are going to direct access Microsoft Planetary Computer Landsat Collection 2 Level-2 data. The STAC item utilized for this project is ****LE07_L2SP_042036_20171217_02_T1****. The raster data was collected on 2017-12-17.\nThis data should be used for visualization purposes only.\n\n\n\nThe shapefile of fire perimeters in California were provided by the California State Geoportal. The complete file can be accessed here.\n\n\n\n\nUS Environmental Protection Agency (2023). Daily AQI by County [Data File]. Available from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed October 25, 2023\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 [Dataset]. Available from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed November 28, 2023\nCalifornia Department of Forestry and Fire Protection (2023). California Fire Perimeters (all) [Data File]. Available from https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed November 28, 2023\n\n\n\n\n\n\n\nCode\n#------------------------------------\n# ----    Load the Essentials    ----\n#------------------------------------\n# Reading in libraries and functions\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport pystac\nimport planetary_computer\n\nimport rasterio\nimport xarray as xr\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib.patches as mpatches\n\n\n\n\n\nTo simplify our workflow, we’re going to combine the 2017 and 2018 data sets, and wrangle a single concatonated dataset.\nOnce we have one dataset, we will select our region of interest (ROI) and correct the Date dtype so it may be used as the index to calculate the average Air Quality Index over a 5 day rolling window.\n\n\nCode\n#------------------------------------\n# ----       Read & Wrangle      ----\n#------------------------------------\n# Reading in the data\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')                     \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') \n\n# glueing the datasets together\naqi = pd.concat([aqi_17, aqi_18])                                                                         \n#  .str.replace(' ','_') to replace the space for _\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')     \n\n# Subsetting using loc\n# selecting SB county\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara']   \n# isolating desired columns   \naqi_sb = aqi_sb.iloc[:, 4:]                               \n\n#  Datetime Indexing\n# converting the date type to datetimes64\naqi_sb.date = pd.to_datetime(aqi_sb.date)    \n # updating the index to the data column                       \naqi_sb = aqi_sb.set_index('date')                                    \n\n# Rolling Window Mean Calc\n# provides rolling window calculations of \n# the mean aqi over 5 day periods  \naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()         \n\n\n\n\nIs everything looking as we expect it to?\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# checking that dataframes joined properly and column names changed\nprint('The number of aqi observations in 2017 were:', len(aqi_17.Date))\nprint('The number of aqi observations in 2018 were:', len(aqi_18.Date))\nprint('The number of aqi observations between 2017-2018 were:', len(aqi.date))        \n\n\nThe number of aqi observations in 2017 were: 326801\nThe number of aqi observations in 2018 were: 327537\nThe number of aqi observations between 2017-2018 were: 654338\n\n\n\n\n\n\nThe visual below displays the mean AQI over a 5 day rolling window between January 2017 and October 2018. The background of the plot is color categorized according to the AQI’s six categories of concern for ppm levels. A relative trend of moderate is seen throughout the year. However,a spike in air pollutants between the months of December 2017 and January 2018 is clearly observed. The sharp initial peak observed the day the Thomas Fire began (Dec. 4, 2017) caused an inital AQI response within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax.fill_between(aqi_sb.index, lower, upper, color=color,\n                    alpha=0.2,\n                    label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax,\n            y=['aqi', 'five_day_average'],\n            color=colors,\n            xlabel='Date',                                                   \n            ylabel='AQI Values (ppm)',\n            ylim= (0,400),\n            legend= True\n            )\n\n# applying customizations\nax.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=18) \n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=4, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=4, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate',\n                                          'Unhealthy for Sensitive Groups',\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 12)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(0.866, 0.88),\n                            fontsize = 12)\n\n# Add both legends to the plot\nax.add_artist(background_legend_art)\nax.add_artist(line_legend_art)\n\n# Add annotation\nax.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.45, 0.5), # position\n            xycoords='figure fraction', \n            fontsize=12, \n            color='black') \n\n# Adjust subplot parameters to add margin space\nplt.subplots_adjust(top=0.85, bottom=0.15, left=0.1, right=0.9)\n\nplt.show()\n\n\n\n\n\n\n\n\nLet’s get a full understanding of exactly how much local area was affected by the Thomas Fire by importing the fire perimeter boundary information & some super cool Landsat 2\n\n\n\n\nCode\n#------------------------------------\n# ----        Read  & Inspect     ----\n#------------------------------------\n# Reading in the data for CA fire perimeters \nca_fire = gpd.read_file(os.path.join(os.getcwd(),'..','data','California_Fire_Perimeters_1379327890478655659','California_Fire_Perimeters_(all).shp'))\n\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning) \n\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# quick viuslaization of CA fire perimeters\n# yow-za! CA is so clearly shaped }:&lt;\nca_fire.plot(ax = ax[0],\n             color = '#AA4203')\nax[0].set_title('CA Fire Perimeters')\n\n\n# Subset for Thomas Fire boundary data for plotting\nthomas_fire = ca_fire.loc[(ca_fire['FIRE_NAME'] == 'THOMAS') & (ca_fire['YEAR_'] &gt;= 2017)]          \n\n\nthomas_fire.plot(ax = ax[1],\n                 color = '#AA4203')\nax[1].set_title('Thomas Fire Boundary')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nTogether, we’re going to load in Landsat data directly from the Microsoft Planetary Computer STAC and collect our desired bands (SWIR22, NIR08, Red) to create a landsat array subset.LE07_L2SP_042036_20171217_02_T1 was captured on 12/17/2017 18:36:51 UTC and will be leveraged for this project.\nIn order to create a false color image, we need to adjust the dimensions of our data to only consider x and y coordinates. Furthermore, we will need to create an array containing the false color bands we intend on utilizing for our ROI. We’ll also be correcting the CRS so we can overlay the two datasets.\n\n\nCode\n#--------------------------------------\n# ---- Pull directly from MPC STAC ----\n#--------------------------------------\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress FutureWarnings\n\n\n# Let's pull our data fresh from the MPC STAC\n# We're also going to assign the bands we're interested in\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2/items/LE07_L2SP_042036_20171217_02_T1\"\n\n# Load the individual item metadata and sign the assets\nitem = pystac.Item.from_file(item_url)\nsigned_item = planetary_computer.sign(item)\n\n#--------------------------------------\n# ----   Collect band information  ----\n#--------------------------------------\n# Open the desired data assets\n# Short Wave Infrared (SWIR22) band\nasset_href_swir22 = signed_item.assets[\"swir22\"].href\nlandsat_swir22 = rioxr.open_rasterio(asset_href_swir22)\n\n# Near Infrared (NIR08) band\nasset_href_nir08 = signed_item.assets[\"nir08\"].href\nlandsat_nir08 = rioxr.open_rasterio(asset_href_nir08)\n\n# Red band\nasset_href_red = signed_item.assets[\"red\"].href\nlandsat_red = rioxr.open_rasterio(asset_href_red)\n\n# Green band\nasset_href_green = signed_item.assets[\"green\"].href\nlandsat_green = rioxr.open_rasterio(asset_href_green)\n\n# Blue band\nasset_href_blue = signed_item.assets[\"blue\"].href\nlandsat_blue = rioxr.open_rasterio(asset_href_blue)\n\n#--------------------------------------\n#  ----  Combine band information  ---- \n#--------------------------------------\n#--------------------------------------\n#  ----         True Color         ---- \n#--------------------------------------\n# Stack bands into a single dataset\ntrue_color = xr.concat([landsat_red, landsat_green, landsat_blue], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\ntrue_color = true_color.squeeze()\n\n# remove coordinates associated to band\ntrue_color = true_color.drop('band')\n\n#--------------------------------------\n#  ----        False Color         ---- \n#--------------------------------------\n# Note: For false-color, typically, you might use \n# a different combination (like NIR, Red, Green).\n# Adjust this based on the specific visualization you want.\nfalse_color = xr.concat([landsat_swir22, landsat_nir08, landsat_red], dim='band')\n\n# Updating data for plotting\n# Original dimensions and coordinates show us that band is a dimension\n# Remove length 1 dimension (band)\nfalse_color = false_color.squeeze()\n\n# remove coordinates associated to band\nfalse_color = false_color.drop('band')\n\n#--------------------------------------\n#  ---- Visualize band information ---- \n#--------------------------------------\n# Create a figure with two subplots\nfig, ax = plt.subplots(1, 2, figsize=(14, 7))\n\n# Plot the true and false color images\n#--------------------------------------\n#  ----     True Color Image       ---- \n#--------------------------------------\n# Plot the false color image\ntrue_color.plot.imshow(ax=ax[0],\n                        robust=True) \nax[0].set_title('True Color Landsat Image (Red, Green, Blue)')\n\n\n#--------------------------------------\n#          False Color Image\n#--------------------------------------\nfalse_color.plot.imshow(ax=ax[1],\n                        robust=True)\nax[1].set_title('False Color Landsat Image (SWIR22, NIR08, Red)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nIt looks good, let’s take a quick peak at the geospatial attr\n\n\nCode\n#------------------------------------\n# ----        Check Point!       ----\n#------------------------------------\n# Check geospatial attributes\nprint('height: ', false_color.rio.height)\nprint('width: ', false_color.rio.width, '\\n')\nprint(false_color.rio.bounds(), '\\n')\n\n# Now to update the CRS to match and check\n# Convert DataFrame to GeoDataFrame\nthomas_fire  = gpd.GeoDataFrame(thomas_fire, geometry='geometry')\n\nthomas_fire = thomas_fire.to_crs(false_color.rio.crs)                                      \n\n# Print CRS to check alignment\nprint('Thomas Fire Boundary CRS: ', thomas_fire.crs)\nprint('False Color CRS: ', false_color.rio.crs)\n\n\nheight:  7271\nwidth:  8291 \n\n(106785.0, 3725685.0, 355515.0, 3943815.0) \n\nThomas Fire Boundary CRS:  PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nFalse Color CRS:  EPSG:32611"
  },
  {
    "objectID": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "href": "posts/2023-11-13-ThomasFire/AQI_False_Color_Img.html#inspecting-areas-burned-by-thomas-fire-in-santa-barbara-county-2017",
    "title": "Investigation on the Thomas Fire Impacts in Santa Barbara County, CA (2017 - 2018)",
    "section": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)",
    "text": "Inspecting Areas Burned by Thomas Fire in Santa Barbara County (2017)\nBelow, we’ve overlayed the fire perimeter boundaries for the Thomas Fire over it’s respective burn scar. Our false color image displays an outline of the fire scorn area of Santa Barbara County. Here, we can observe the severity of the damage by generally visualizing the area and the types of regions impacted by the wildfire.\n\n\nCode\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Adjust the figure size and DPI\nfig, ax = plt.subplots(figsize=(6, 6)) \n\n# Plot false color bands\nfalse_color.plot.imshow(ax=ax, robust=True)\n\n# Plot Thomas Fire burn area\nthomas_fire.plot(ax=ax, color='none', edgecolor='#AA4203')\n\n# Add a legend for the burned area\nfire_scar = mpatches.Patch(color='#AA4203', label='Burned Area')\nax.legend(handles=[fire_scar], loc='upper right', fontsize=12) \n\n# Add a title to the plot\nax.set_title('False Color Image (SWIR22, NIR08, Red)', fontsize=14)  \n\n# Remove the plot axes\nax.axis('off')\n\n# Save the figure with tight bounding box and high resolution\nplt.savefig('thomas_fire_plot.png')  \nplt.show()\n\n\n\n\n\n\nCombining the Visuals\nLet’s put the most informative visuals together into a single figure to gain a full sense of the extent of damage caused by the Thomas Fire. In AQI the figure on the left, there is a sharp peak observed the day the Thomas Fire began (Dec. 4, 2017) that fell within the Very Unhealthy category and over a 5 day period, averaged out into the Unhealthy category. This trend can be observed at the noted location in the figure below, indicating negative public health effects on air quality resulting from the Thomas Fire. The figure on the right, displays the large parcel of Santa Barbara County that was engulfed by the fire, explaining the uptake in particulate matter observed in December of 2017. There were significant direct impacts on public health in terms of air quality as a response to this fire and this can be understood when considering the size of the burn scar seen in the figure on the right.\n\n\nCode\n#------------------------------------\n#  ----     Customization        ----\n#------------------------------------\n# Define AQI categories and colors\naqi_categories = {\n    'Good': (0, 50, '#00E400'),\n    'Moderate': (51, 100, '#FFFF00'),\n    'Unhealthy for Sensitive Groups': (101, 150, '#FF9933'),\n    'Unhealthy': (151, 200, '#FF0000'),\n    'Very Unhealthy': (201, 300, '#8B0000'),\n    'Hazardous': (301, 500, '#800080')\n}\n\ncolors = {'aqi':'#f69517',\n          'five_day_average':'#360F39'}\n\n# Plot the images in the same figure side-by-side\nfig, (ax1, ax2) = plt.subplots(1, 2, \n                               figsize = (22,8)) \n\n\n#------------------------------------------------------------------------\n# Plotting the Daily & 5-Day Rolling Window AQI for Santa Barbara County \n#------------------------------------------------------------------------\n#------------------------------------\n#  ----     Visualizing AQI      ----\n#------------------------------------\n# Plot AQI categories as background colors for associated ranges\nfor category, (lower, upper, color) in aqi_categories.items():\n    ax1.fill_between(aqi_sb.index, lower, upper, color=color, alpha=0.2, label=f'{category}')\n\n# Plot the AQI and 5-Day Rolling Average\naqi_sb.plot(ax=ax1,\n            y=['aqi', 'five_day_average'],\n            color=colors,             \n            ylim= (0,400),\n            legend= True,\n            ylabel='AQI Values (ppm)'\n            )\n\n# applying customizations\nax1.set_title('Air Quality Index (AQI) Assessment of Santa Barbara County 2017-2018', \n             fontsize=21) \n\nax1.set_xlabel(xlabel='Date',\n               fontsize = 13)\n\nax2.set_ylabel(ylabel='AQI Values (ppm)',\n               fontsize = 10)\n\n# Add a legend for background colors\nbackground_legend = [Line2D([0], [0], color='#00E400', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FFFF00', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF9933', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#FF0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#8B0000', lw=6, alpha=0.3),\n                     Line2D([0], [0], color='#800080', lw=6, alpha=0.3)]\n\n# Add background color legend to the plot\nbackground_legend_art = ax1.legend(handles=background_legend,\n                                  labels=['Good', 'Moderate', \n                                          'Unhealthy for Sensitive Groups'\n                                          'Unhealthy', 'Very Unhealthy',\n                                          'Hazardous'],\n                                  loc='upper center',\n                                  bbox_to_anchor=(0.5, 1),\n                                  ncol=3,\n                                  fontsize = 18)\n# Line color legend\nline_legend = [Line2D([0], [0], color='#f69517', lw=2),\n               Line2D([0], [0], color='#360F39', lw=2)]\n\n# Add line color legend to the plot\nline_legend_art = ax1.legend(handles=line_legend,\n                            labels=['AQI', '5-Day Average'],\n                            loc='upper right',\n                            bbox_to_anchor=(1, 0.86),\n                            fontsize = 18)\n\n# Add both legends to the plot\nax1.add_artist(background_legend_art)\nax1.add_artist(line_legend_art)\n\n# Add annotation\nax1.annotate(\"* This peak is a result of the \\nThomas Fire in Santa Barbara.\", \n            xy=(0.28, 0.52), # position\n            xycoords='figure fraction', \n            fontsize=20, \n            color='black') \n#------------------------------------------------------------------------\n#        Plotting the Thomas Fire burned areas of Santa Barbara\n#------------------------------------------------------------------------\n # Remove plot axes\nplt.axis('off')                               \n# Plot false color bands\nfalse_color.plot.imshow(ax = ax2,              \n                        # Include colors\n                        robust = True)        \n\n# Plot thomas fire burn area\nthomas_fire.plot(ax = ax2,                     \n       # No color of burn area\n       color = 'none',                         \n       # Opacity of edgecolor\n       edgecolor = '#AA4203')     \n          \n# Add a figure legend\nfire_scar = mpatches.Patch(color = '#AA4203',\n                          label = 'Burned Area') \n\nax2.legend(handles=[fire_scar])\n\n#------------------------------------\n# ----   Plot the Fire Bound    -----\n#------------------------------------\nthomas_fire.plot(ax = ax2,\n                 color = '#AA4203',\n                 # make border around shapefile\n                edgecolor = '#AA4203', \n                 # make transparent\n                alpha = 0.5)\n\nfire_patch = mpatches.Patch(color = '#AA4203',\n                            label = \"Thomas Fire\")\n\n#------------------------------------\n# ----  Plot Landsat False Color ----\n#------------------------------------\n# Plot the false landsat\nfalse_color.plot.imshow(ax = ax2,\n                          robust = True) \n\n# Edit the Legend and Caption \n# Show lables for legend\nax2.legend(handles = [fire_patch], \n        # No border around legend\n          frameon = True,\n        # Where legend is located\n          bbox_to_anchor = (0.9, 0.8),\n          fontsize = 18) \n \n# Add title\nax2.set_title('Areas Burned by Thomas Fire in Santa Barbara, CA (2017)',\n               fontsize = 21) \n\n# Plot the whole figure  \n# space well\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\nCitations:\n\nWikipedia contributors. “Thomas Fire.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 16 Apr. 2024. Web. 2 Aug. 2024.\nVCFD determines cause of The thomas fire. Ventura County Fire Department. (2019, March 13). https://vcfd.org/news/vcfd-determines-cause-of-the-thomas-fire/\nWildfire Today. “Thomas Fire Archives - Wildfire Today.” Wildfire Today, 14 Nov. 2019, https://wildfiretoday.com/tag/thomas-fire.\nCalifornia, State Of. Montecito Mudslides Anniversary, Reflections Through Images | Cal OES News. https://news.caloes.ca.gov/montecito-mudslides-anniversary-reflections-through-images.\nAirNow.gov, U.S. EPA. (n.d.). Aqi Basics. AQI Basics | AirNow.gov. https://www.airnow.gov/aqi/aqi-basics/\nMicrosoft Planetary Computer. Planetary Computer. (n.d.). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nCalifornia fire perimeters (all). California State Geoportal. (n.d.). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about"
  },
  {
    "objectID": "posts/2023-10-08-AffordableHousing/AffordableHousing.html",
    "href": "posts/2023-10-08-AffordableHousing/AffordableHousing.html",
    "title": "Affordable Housing Project",
    "section": "",
    "text": "There is not enough affordable housing available to meet the growing demand Gentrification leads to rising costs, and the displacement of people from rising housing costs has ramifications for access to public services like quality education, food, and transportation\nIncreasing density in areas to raise the median income to generator money for public services\nBarriers to development in the private sector can be addressed with city regulations that require developers to build affordable housing for more square footage instead of paying for credits to develop inland affordable housing that never happens.\nA means to streamline the production of affordable housing is the inclusion of incentives such as: increased housing density, relaxed parking requirements, or other similar concessions influence the production of affordable housing.\nData science can be used to make the identification of potential land and use by communities and private developers easier.\nCities need to make the priority developing quality affordable housing that increases stability and social mobility that can grant people autonomy\nWorking with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nGoing beyond building homes, the public sector should collaborate with local industries and nonprofit organizations to create workshops/vocational schools and local job opportunities. Increasing the overall quality of life for residents through the implementation of a ground-up (community-based) rehabilitation process for unhoused people."
  },
  {
    "objectID": "posts/2023-10-08-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "href": "posts/2023-10-08-AffordableHousing/AffordableHousing.html#data-science-in-affordable-housing",
    "title": "Affordable Housing Project",
    "section": "Data Science in Affordable Housing",
    "text": "Data Science in Affordable Housing\n\nData in Development\nThere are many things that can make the building of affordable housing an easy option.\n\nMaking the information available for use and implementation\n\nCompile a list / map of available plots that the city owns and the zoning options for those plots\nFor each area of land available for development:\n\nPresent 2 options:\n\nNormal development: Present the proposed zoning and floor area (FAR)\nAffordable housing development: Present the proposed up-zoning and floor area (FAR)\n\nAllow developers to compare options and make it easy for those that are interested in developing quality affordable housing.\n\n\n\n\nDeveloping Research Tools\nQuestions to consider:\n\nWhat is the goal?\n\nWhat are you trying to reveal with your analysis? Who are you trying to help?\n\nWho is defining the goal?\n\nHave you talked to members of the community that are affected by this scrutiny/observation?\nHave you made their voices central to defining the problem and listened to proposed solutions?\n\nWhat assumptions are you making about the community?\nWhat factors are you considering to be important?\n\nWhat data are you using to determine location/accessibility?\nAre you selecting the right variables in your understanding?\nHow did you choose those variables/indicators?\n\n\n\n\n\nOrganizations conducting data collection and analysis on affordable housing\n\n\n\nHousing Assessment Resources Tools (HART)\n\nLearn more here\nHousing assessment tool: Measures core housing needs and affordable housing costs by income category, household size, and vulnerable populations.\nLand assessment tool: looks at the area in Toronto that could be developed and measures its proximity to important amenities. Makes recommendations for building affordable housing in those areas because they already have access to the amenities that they would need.\n\n\nProperty acquisition tool: acquisition of existing housing to maintain access to affordable housing supply over the long term, and develop resources to aid governments at all levels to implement effective acquisition strategies.\n\n\nOther and Belonging Institute\n\n\nLearn more here\nThis institute at Berkeley works on understanding marginalization and exclusion. They have published research on the benefits of community land trusts as stewards of public land. Provide a guide for how local governments can partner with community land trusts to achieve their goals.\n\n\nHow does a Community Land Trust work:\n\nUse ground lease that ensures permanent affordability and community control to provide lower income community members\nSupport residents with services that ensure their financial stability and ability to thrive\n\n\n\nCities can donate surplus land to CLT’s to support communities.\n\n\nParkdale Neighborhood land trust\n\n\nLearn more here\nNon-profit that owns and manages land in a community ownership model, and partners with housing partners who then provide high quality affordable housing, supportive housing, and community economic development programs.\n\n\nTapestry\n\nLearn more here\nInvestment marketplace that finances community projects as a form of impact investing. Help nonprofits and cooperatives raise funds for their projects through a network of investors and buying bonds.\n\n\nCommunity Ethics Research Workshop (CREW)\n\n\nLearn more here\n“We want to develop a community review model that empowers community members to review and offer suggestions on research proposals in collaboration with researchers. Ultimately, we want to work towards reducing the harms associated with research and making it easier for researchers to collaborate in our community in a positive, respectful way.”\n\n\nBalanced Supply of Housing Research Cluster (BSHRC)\nLearn morehere\nLooking at four major Canadian city regions to understand the attitudes on neighborhood densification for affordability, choice, and diversity."
  },
  {
    "objectID": "posts/2023-10-08-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "href": "posts/2023-10-08-AffordableHousing/AffordableHousing.html#providing-security-beyond-housing",
    "title": "Affordable Housing Project",
    "section": "Providing Security Beyond Housing",
    "text": "Providing Security Beyond Housing\nTo provide true housing security to individuals, we need policies to protect our people and work to create vocational schools, mentorship programs, and/or apprenticeship opportunities to help residents find career paths and gain access to high quality local jobs. This can be achieved by partnering with existing local job providers, educational institutions, and public agencies to train community members to enter the job market, obtain high-quality work, and earn a comfortable wage. In order to build a stronger economy, it is essential that the community has a wide range of opportunities and resources available. Strengthening Community Involvement\nCommunity members carry a wealth of insights to help solve local community issues like affordable housing. Working with a wide range of local individuals who represent community groups, industries, and institutions, to leverage the wisdom of active partners in our community will assist in the facilitation of access to fair housing, clean water, air, and green spaces.\nFurthermore, community members need to be notified far in advance when there is talk of a significant local project. The community members should not be forced to rely on a last minute notice as a means to voice their concerns. A trend within research findings demonstrates lower rates of residential displacement when early community collaboration is practiced in the planning process (National Low Income Housing Coalition). Incorporating the community in the planning process not only helps protect low-income or existing residents from displacement and aids in the removal of developer’s ambiguity as they navigate an uncertain planning process. Overall, fostering more support for projects. Ways to Include the Community Working with local residents to research and peer-review housing policies in other areas, and most importantly. Providing an opportunity for early feedback on planning issues that are occurring within the community. Community development staff attend community meetings frequently to allow for more collaborative solutions to issues such as: affordable housing, traffic, overlay zones, density, etc. Organizing with a local educational institution to offer architectural students an opportunity to design housing projects.\nReferences\n\n“2023 State of the Nation’s Housing Report: 4 Key Takeaways.” Cost of Home, www.habitat.org/costofhome/2023-state-nations-housing-report-lack-affordable-housing#:~:text=During%20the%20pandemic%2C%20the%20number,that%20exceeded%20half%20their%20income. Accessed 11 Dec. 2023.\n“Addressing America’s Affordable Housing Crisis.” Housing Matters, 12 Apr. 2023, housingmatters.urban.org/research-summary/addressing-americas-affordable-housing-crisis.\n“Gentrification and Neighborhood Revitalization: What’s the Difference?” National Low Income Housing Coalition, 5 Apr. 2019, nlihc.org/resource/gentrification-and-neighborhood-revitalization-whats-difference.\nHelen Eloyan, www.heleneloyan.com/. Accessed 11 Dec. 2023.\nRyan Jones June 24, 2022. ” Michigan Law Journal of Law and Mobility.” 24 June 2022, futurist.law.umich.edu/potential-solutions-to-the-first-mile-last-mile-problem/#:~:text=One%20of%20these%20challenges%2C%20known,transportation%20station%20to%20their%20destination."
  },
  {
    "objectID": "index.html#im-sofia-it-rhymes-with-papaya-sof-aya",
    "href": "index.html#im-sofia-it-rhymes-with-papaya-sof-aya",
    "title": "Sofia Ingersoll",
    "section": "I’m Sofia, it rhymes with papaya (Sof-aya)",
    "text": "I’m Sofia, it rhymes with papaya (Sof-aya)\nPeople know me as a carbon crusader, an environmental data scientist, and a certified chemist (in and out of the kitchen). I utilize data science to amplify the voices of vulnerable communities to drive sustainable climate-solutions paired with tangible policy suggestions.\nCurrently, I’m at The 2035 Initiative researching global climate adaptation and resiliency in disadvantaged communities and aiding in the development of an equitable clean energy grid model for CA. I employ techniques like geospatial analysis, and statistics to quantify and visualize diverse sets of empirical research and environmental data to support transformational policy change.\nI am a continuous learner. I actively seek to satiate my curiosity for emerging energy technologies, as well as, strategies to tackle GHG emissions and the carbon economy.\nFeel free to explore my website and learn more about me and a selection of my projects. If my content resonates with you, let’s connect and discuss how I can bring value to your team!"
  }
]